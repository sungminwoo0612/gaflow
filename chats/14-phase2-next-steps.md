# Chat 14: Phase 2Â·ë‹¤ìŒ ë‹¨ê³„

**ë‚ ì§œ**: 30 Jan

---

## Phase 2: ê¸°ì¡´ ì½”ë“œ ì—°ë™ + ì¶”ë¡ Â·ì–‘ìí™”

---

## ì‘ì—… ëª©ë¡

| ID | ì‘ì—… | ìš°ì„ ìˆœìœ„ | ì˜ˆìƒ ì‹œê°„ | ì˜ì¡´ì„± |
|----|------|----------|-----------|--------|
| 2.1 | ê¸°ì¡´ ì‹¤í—˜ ë””ë ‰í† ë¦¬ ì‹¤ì‚¬ | ğŸ”´ ë†’ìŒ | 2h | - |
| 2.2 | InferenceRunner êµ¬í˜„ | ğŸ”´ ë†’ìŒ | 4h | - |
| 2.3 | QuantizationRunner êµ¬í˜„ | ğŸ”´ ë†’ìŒ | 4h | - |
| 2.4 | ExperimentRegistry êµ¬í˜„ | ğŸŸ¡ ì¤‘ê°„ | 3h | 2.1 |
| 2.5 | GenericWrapper êµ¬í˜„ | ğŸŸ¡ ì¤‘ê°„ | 3h | 2.4 |
| 2.6 | CLI í™•ì¥ | ğŸŸ¢ ë‚®ìŒ | 2h | 2.2â€“2.5 |
| 2.7 | E2E í…ŒìŠ¤íŠ¸ | ğŸ”´ ë†’ìŒ | 2h | ì „ì²´ |

---

## 2.1 ê¸°ì¡´ ì‹¤í—˜ ë””ë ‰í† ë¦¬ ì‹¤ì‚¬

**ë¨¼ì € ì‹¤í–‰í•  ê²ƒ:**

```bash
ls -la ~/experiments/
find ~/experiments/donghwa-entec-ppe -type f -name "*.py" | head -20
find ~/experiments -name "*.py" | xargs basename -a | sort | uniq -c | sort -rn | head -20
find ~/experiments -name "*.pt" | head -20
find ~/experiments -name "*.yaml" | head -20
```

ê²°ê³¼ë¥¼ ê³µìœ í•˜ë©´ **ì‹¤ì œ êµ¬ì¡°ì— ë§ê²Œ ì½”ë“œ ì¡°ì •**.

---

## 2.2 InferenceRunner êµ¬í˜„ ìš”ì•½

- **ì…ë ¥**: experiment_name, model_path, data_yaml; BenchmarkConfig(batch_sizes, warmup_runs, benchmark_runs, conf/iou_threshold, imgsz, device).
- **ë¡œì§**: mlflow.start_run â†’ set_tag(experiment_type=infer), log_params â†’ YOLO(model_path) ë¡œë“œ â†’ _run_validation(data_yaml) â†’ detection ë©”íŠ¸ë¦­ log_metrics â†’ batch_sizeë³„ _run_benchmark (warmup í›„ latency ì¸¡ì •) â†’ latency_bs{N}_mean_ms, fps_bs{N}, gpu_memory_bs{N}_mb, best_fps/best_batch_size/best_latency_ms, model_size_mb.
- **ì¶œë ¥**: { run_id, status, detection, benchmark[], best }.
- **êµ¬í˜„**: guardianflow/inference.py â€” InferenceRunner.run(config, run_name, tags, source_run_id); _run_validation(model, config); _run_benchmark(model, config, batch_size) â†’ BenchmarkResult(dataclass).

---

## 2.3 QuantizationRunner êµ¬í˜„ ìš”ì•½

- **ì…ë ¥**: experiment_name, source_run_id, data_yaml(optional); QuantConfig(backend, precision, imgsz, calibration_data, calibration_size).
- **ë¡œì§**: mlflow.start_run â†’ set_tag(experiment_type=quant, source_run_id, quant_backend, quant_precision) â†’ _download_source_model() (mlflow.artifacts.download_artifacts) â†’ original_size_mb log_metric â†’ YOLO(original_path) ë¡œë“œ â†’ _benchmark_model(original) â†’ _export_model(model, config) (model.export(format=backend, half/int8, data=calibration)) â†’ quantized_size_mb â†’ _benchmark_model(quantized) â†’ comparison (accuracy_drop_mAP50, speedup_ratio, compression_ratio) log_metrics â†’ log_artifact(quantized_path, "weights") â†’ register(model_name={base}__{backend}_{precision}, @candidate).
- **ì¶œë ¥**: { run_id, status, original, quantized, comparison, model }.
- **êµ¬í˜„**: guardianflow/quantization.py â€” QuantizationRunner.run(config, run_name, tags, register); _download_source_model, _export_model, _benchmark_model, _get_model_name.

---

## ë‹¤ìŒ ë‹¨ê³„

- 2.1 ì‹¤ì‚¬ ê²°ê³¼ ë°˜ì˜ í›„ ExperimentRegistry YAML ìŠ¤í‚¤ë§ˆÂ·GenericWrapper ë™ì‘ ë°©ì‹ í™•ì •.
- 2.2, 2.3 ì™„ë£Œ í›„ CLI infer/quant ëª…ë ¹ ì—°ê²° ë° E2E í…ŒìŠ¤íŠ¸.
