
1000049203.jpg
Mlflow로 로직을 짜는데 tracker와 logger를 나눠야 할까? 이렇게 세가지 로깅을 따로 파일화?

30 Jan
핵심 요약
MLflow에서 Metrics/Parameters/Artifacts 로깅을 별도 파일로 분리할 필요 없음. 단일 mlflow 모듈이 이 세 가지를 통합 관리하도록 설계됨.

권장 구조
분리 기준: 기능별이 아닌 책임별

ml_tracking/
├── experiment.py    # MLflow run 컨텍스트 관리
├── callbacks.py     # 훈련 프레임워크 콜백 (PyTorch Lightning, YOLO 등)
└── config.py        # tracking URI, experiment name 설정
분리하지 않는 이유:

mlflow.log_metric(), log_param(), log_artifact()는 모두 동일한 run context 내에서 호출되어야 함
세 로깅 타입이 분리되면 run context 공유를 위한 불필요한 의존성 발생
MLflow 자체가 이미 내부적으로 분리 저장함 (metrics→time-series DB, artifacts→object storage)
최소 구현 예시
python
# experiment.py
import mlflow
from contextlib import contextmanager

@contextmanager
def tracked_run(experiment_name: str, run_name: str = None):
    mlflow.set_experiment(experiment_name)
    with mlflow.start_run(run_name=run_name) as run:
        yield run

def log_training_step(metrics: dict, params: dict = None, artifacts: list[str] = None):
    """단일 함수에서 세 타입 모두 처리 - run context 내에서 호출"""
    if params:
        mlflow.log_params(params)  # 1회성, 훈련 시작 시
    mlflow.log_metrics(metrics)    # 반복 호출 (epoch/step마다)
    if artifacts:
        for path in artifacts:
            mlflow.log_artifact(path)
python
# 사용
with tracked_run("guardianflow-ppe-detection", run_name="yolo-v8-exp-001"):
    mlflow.log_params({"lr": 0.001, "batch_size": 16, "epochs": 100})
    
    for epoch in range(100):
        # ... training ...
        mlflow.log_metrics({"loss": loss, "mAP50": map50}, step=epoch)
    
    mlflow.log_artifact("weights/best.pt")
    mlflow.log_artifact("configs/train_config.yaml")
Tracker vs Logger 분리가 의미 있는 경우
상황	분리 여부	이유
단일 프레임워크 (YOLO, Lightning)	❌	프레임워크 콜백이 이미 통합 처리
멀티 백엔드 (MLflow + W&B + 자체 DB)	✅	추상화 레이어 필요
분산 훈련 (multi-node)	✅	rank 0만 로깅하는 로직 분리
현재 GuardianFlow POC 단계에서는 분리 불필요. 복잡도만 증가함.

시나리오 = 어떤 데이터 셋 + 어떤 데이터 처리 + 어떤 모델 + 어떤 모델 처리 + 어떤 실험 설정 + 실험 과정 + 실험 결과

시나리오라는 컨테이너 단위를 만들어서 이를 파이프라인으로 운용하고 싶음

30 Jan
핵심 요약
"시나리오"는 실험의 재현 가능한 전체 명세. MLflow의 기존 구조(Experiment → Run)와 매핑하되, 시나리오를 선언적 config + 실행 파이프라인으로 분리하는 것이 핵심.

개념 매핑
시나리오 컴포넌트          MLflow 매핑              저장 방식
─────────────────────────────────────────────────────────────
데이터셋 정의            → Run Tag / Param        dataset_id, version
데이터 처리              → Param + Artifact       transform config, DVC ref
모델 정의                → Param                  model_name, architecture
모델 처리 (학습 설정)     → Param                  optimizer, scheduler
실험 설정                → Param                  seed, device, epochs
실험 과정                → Metrics (step)         loss, mAP per epoch
실험 결과                → Metrics + Artifacts    final metrics, weights
시나리오 스키마 설계
python
# scenario/schema.py
from dataclasses import dataclass, field
from typing import Literal

@dataclass
class DatasetConfig:
    name: str
    version: str
    dvc_path: str  # DVC 추적 경로
    split_ratio: tuple[float, float, float] = (0.8, 0.1, 0.1)

@dataclass
class PreprocessConfig:
    image_size: int = 640
    augmentations: list[str] = field(default_factory=lambda: ["hsv", "flip", "mosaic"])
    normalize: bool = True

@dataclass
class ModelConfig:
    architecture: Literal["yolov8n", "yolov8s", "yolov8m", "llava-next-video"]
    pretrained: str | None = None
    num_classes: int = 10

@dataclass
class TrainingConfig:
    epochs: int = 100
    batch_size: int = 16
    learning_rate: float = 0.001
    optimizer: Literal["adam", "adamw", "sgd"] = "adamw"
    scheduler: str | None = "cosine"
    seed: int = 42

@dataclass
class Scenario:
    """실험 시나리오 = 재현 가능한 전체 명세"""
    name: str
    dataset: DatasetConfig
    preprocess: PreprocessConfig
    model: ModelConfig
    training: TrainingConfig
    tags: dict[str, str] = field(default_factory=dict)
    
    def to_flat_params(self) -> dict:
        """MLflow log_params용 flat dict 변환"""
        from dataclasses import asdict
        params = {}
        for section_name in ["dataset", "preprocess", "model", "training"]:
            section = getattr(self, section_name)
            for k, v in asdict(section).items():
                params[f"{section_name}.{k}"] = str(v)
        return params
파이프라인 실행 구조
python
# scenario/runner.py
import mlflow
from abc import ABC, abstractmethod

class ScenarioRunner:
    """시나리오 → MLflow Run 실행기"""
    
    def __init__(self, scenario: Scenario, tracking_uri: str = None):
        self.scenario = scenario
        if tracking_uri:
            mlflow.set_tracking_uri(tracking_uri)
    
    def run(self) -> str:
        mlflow.set_experiment(f"guardianflow-{self.scenario.dataset.name}")
        
        with mlflow.start_run(run_name=self.scenario.name) as run:
            # 1. 전체 시나리오 파라미터 로깅
            mlflow.log_params(self.scenario.to_flat_params())
            mlflow.set_tags(self.scenario.tags)
            
            # 2. 시나리오 config 자체를 artifact로 저장 (재현성)
            self._log_scenario_artifact()
            
            # 3. 파이프라인 단계별 실행
            data = self._prepare_data()
            model = self._build_model()
            metrics = self._train(model, data)
            self._save_results(metrics)
            
            return run.info.run_id
    
    def _log_scenario_artifact(self):
        import yaml
        from dataclasses import asdict
        with open("/tmp/scenario.yaml", "w") as f:
            yaml.dump(asdict(self.scenario), f)
        mlflow.log_artifact("/tmp/scenario.yaml")
    
    def _prepare_data(self):
        # DVC pull + transform
        ...
    
    def _build_model(self):
        # 모델 초기화
        ...
    
    def _train(self, model, data) -> dict:
        # 학습 루프 + mlflow.log_metrics(step=epoch)
        ...
    
    def _save_results(self, metrics: dict):
        mlflow.log_metrics(metrics)
        mlflow.log_artifact("weights/best.pt")
사용 예시
python
# experiments/ppe_baseline.py
scenario = Scenario(
    name="ppe-yolov8s-baseline-v1",
    dataset=DatasetConfig(
        name="ppe-detection",
        version="v2.1",
        dvc_path="data/ppe-detection.dvc"
    ),
    preprocess=PreprocessConfig(image_size=640),
    model=ModelConfig(architecture="yolov8s", num_classes=5),
    training=TrainingConfig(epochs=100, batch_size=16, learning_rate=0.01),
    tags={"customer": "site-A", "purpose": "baseline"}
)

runner = ScenarioRunner(scenario, tracking_uri="http://mlflow:5000")
run_id = runner.run()
```

---

## 디렉토리 구조
```
guardianflow/
├── scenario/
│   ├── schema.py      # Scenario, DatasetConfig 등 dataclass
│   ├── runner.py      # ScenarioRunner
│   └── registry.py    # 시나리오 버전 관리 (optional)
├── pipelines/
│   ├── data.py        # DVC 연동, 전처리
│   ├── train.py       # 학습 루프
│   └── evaluate.py    # 평가 로직
├── experiments/
│   ├── ppe_baseline.py
│   └── emergency_detection.py
└── configs/
    └── scenarios/     # YAML로 시나리오 정의 (코드 대신 선언적 관리)
        ├── ppe_v1.yaml
        └── ppe_v2.yaml
Trade-off 정리
접근 방식	장점	단점
Python dataclass (현재 제안)	타입 체크, IDE 지원, 코드 재사용	비개발자 접근 어려움
YAML config	선언적, 버전 관리 용이	스키마 검증 별도 필요 (Pydantic/Hydra)
Hydra	계층적 config 오버라이드	러닝 커브, 디버깅 복잡
POC 단계 권장: dataclass로 시작 → 시나리오 많아지면 YAML + Pydantic 전환

실험 타입이 학습, 추론벤치, 양자화 세가지야. 이를 반영한다면

30 Jan
핵심 요약
실험 타입별로 공통 기반 스키마 + 타입별 확장으로 설계. 각 타입이 요구하는 파라미터와 결과물이 다르므로 상속/합성 구조 적용.

실험 타입별 특성
타입	입력	주요 파라미터	결과물
Training	데이터셋	epochs, lr, optimizer	weights, metrics
Inference Benchmark	모델 + 테스트셋	batch_size, device, precision	latency, throughput, mAP
Quantization	학습된 모델	quant_method, calibration_size	경량 모델, 정확도 손실
스키마 설계
python
# scenario/schema.py
from dataclasses import dataclass, field
from typing import Literal
from abc import ABC

# ─────────────────────────────────────────────
# 공통 컴포넌트
# ─────────────────────────────────────────────

@dataclass
class DatasetConfig:
    name: str
    version: str
    dvc_path: str
    split: Literal["train", "val", "test"] = "train"

@dataclass
class ModelConfig:
    architecture: str
    weights_path: str | None = None  # 사전학습/기존 모델 경로
    num_classes: int = 10

@dataclass
class EnvironmentConfig:
    device: Literal["cuda", "cpu", "tensorrt"] = "cuda"
    precision: Literal["fp32", "fp16", "int8"] = "fp32"
    seed: int = 42

# ─────────────────────────────────────────────
# 실험 타입별 설정
# ─────────────────────────────────────────────

@dataclass
class TrainingParams:
    epochs: int = 100
    batch_size: int = 16
    learning_rate: float = 0.001
    optimizer: Literal["adam", "adamw", "sgd"] = "adamw"
    scheduler: str | None = "cosine"
    early_stopping_patience: int | None = 10

@dataclass
class InferenceBenchmarkParams:
    batch_sizes: list[int] = field(default_factory=lambda: [1, 8, 16, 32])
    warmup_runs: int = 10
    benchmark_runs: int = 100
    input_size: tuple[int, int] = (640, 640)

@dataclass
class QuantizationParams:
    method: Literal["dynamic", "static", "qat"] = "static"
    backend: Literal["tensorrt", "onnx", "openvino"] = "tensorrt"
    calibration_size: int = 500
    calibration_method: Literal["entropy", "minmax", "percentile"] = "entropy"

# ─────────────────────────────────────────────
# 시나리오 (실험 타입별)
# ─────────────────────────────────────────────

@dataclass
class BaseScenario(ABC):
    """모든 시나리오 공통"""
    name: str
    experiment_type: str
    model: ModelConfig
    environment: EnvironmentConfig
    tags: dict[str, str] = field(default_factory=dict)
    
    def to_flat_params(self) -> dict:
        from dataclasses import asdict, fields
        params = {"experiment_type": self.experiment_type}
        for f in fields(self):
            val = getattr(self, f.name)
            if hasattr(val, "__dataclass_fields__"):
                for k, v in asdict(val).items():
                    params[f"{f.name}.{k}"] = str(v)
        return params

@dataclass
class TrainingScenario(BaseScenario):
    experiment_type: str = field(default="training", init=False)
    dataset: DatasetConfig = None
    preprocess: PreprocessConfig = None
    training: TrainingParams = None

@dataclass
class InferenceBenchmarkScenario(BaseScenario):
    experiment_type: str = field(default="inference_benchmark", init=False)
    dataset: DatasetConfig = None  # 테스트셋
    benchmark: InferenceBenchmarkParams = None

@dataclass
class QuantizationScenario(BaseScenario):
    experiment_type: str = field(default="quantization", init=False)
    dataset: DatasetConfig = None  # calibration용
    quantization: QuantizationParams = None
    source_run_id: str = None  # 원본 학습 run 참조
타입별 Runner
python
# scenario/runner.py
import mlflow
from abc import ABC, abstractmethod

class BaseRunner(ABC):
    def __init__(self, scenario: BaseScenario, tracking_uri: str = None):
        self.scenario = scenario
        if tracking_uri:
            mlflow.set_tracking_uri(tracking_uri)
    
    def run(self) -> str:
        experiment_name = f"guardianflow-{self.scenario.experiment_type}"
        mlflow.set_experiment(experiment_name)
        
        with mlflow.start_run(run_name=self.scenario.name) as run:
            mlflow.log_params(self.scenario.to_flat_params())
            mlflow.set_tags(self.scenario.tags)
            self._log_scenario_artifact()
            
            self._execute()  # 타입별 실행 로직
            
            return run.info.run_id
    
    @abstractmethod
    def _execute(self):
        pass
    
    def _log_scenario_artifact(self):
        import yaml
        from dataclasses import asdict
        with open("/tmp/scenario.yaml", "w") as f:
            yaml.dump(asdict(self.scenario), f)
        mlflow.log_artifact("/tmp/scenario.yaml")


class TrainingRunner(BaseRunner):
    def _execute(self):
        s = self.scenario
        # 1. 데이터 로드 (DVC)
        # 2. 모델 초기화
        # 3. 학습 루프
        for epoch in range(s.training.epochs):
            # train_one_epoch()
            mlflow.log_metrics({"train_loss": ..., "val_mAP": ...}, step=epoch)
        
        # 4. 최종 결과
        mlflow.log_artifact("weights/best.pt")
        mlflow.log_metrics({"final_mAP50": ..., "final_mAP50-95": ...})


class InferenceBenchmarkRunner(BaseRunner):
    def _execute(self):
        s = self.scenario
        results = {}
        
        for batch_size in s.benchmark.batch_sizes:
            # warmup
            # benchmark
            latency_ms = ...
            throughput_fps = ...
            
            results[f"latency_bs{batch_size}"] = latency_ms
            results[f"throughput_bs{batch_size}"] = throughput_fps
        
        mlflow.log_metrics(results)
        # 상세 결과 JSON artifact
        mlflow.log_artifact("benchmark_results.json")


class QuantizationRunner(BaseRunner):
    def _execute(self):
        s = self.scenario
        
        # 1. 원본 모델 로드 (source_run_id에서)
        if s.source_run_id:
            mlflow.set_tag("source_run_id", s.source_run_id)
            original_model_path = mlflow.artifacts.download_artifacts(
                run_id=s.source_run_id, artifact_path="weights/best.pt"
            )
        
        # 2. Calibration 데이터 준비
        # 3. 양자화 실행
        # 4. 정확도 검증
        
        mlflow.log_metrics({
            "original_mAP50": ...,
            "quantized_mAP50": ...,
            "accuracy_drop": ...,
            "model_size_mb": ...,
            "compression_ratio": ...
        })
        mlflow.log_artifact("model_quantized.engine")  # TensorRT
팩토리 패턴으로 통합
python
# scenario/factory.py
from typing import Union

ScenarioType = Union[TrainingScenario, InferenceBenchmarkScenario, QuantizationScenario]

def create_runner(scenario: ScenarioType, tracking_uri: str = None) -> BaseRunner:
    runners = {
        "training": TrainingRunner,
        "inference_benchmark": InferenceBenchmarkRunner,
        "quantization": QuantizationRunner,
    }
    runner_cls = runners.get(scenario.experiment_type)
    if not runner_cls:
        raise ValueError(f"Unknown experiment type: {scenario.experiment_type}")
    return runner_cls(scenario, tracking_uri)
사용 예시
python
# 1. 학습
train_scenario = TrainingScenario(
    name="ppe-yolov8s-v1",
    model=ModelConfig(architecture="yolov8s", num_classes=5),
    environment=EnvironmentConfig(device="cuda", precision="fp16"),
    dataset=DatasetConfig(name="ppe", version="v2.1", dvc_path="data/ppe.dvc"),
    training=TrainingParams(epochs=100, batch_size=16)
)
train_run_id = create_runner(train_scenario, "http://mlflow:5000").run()

# 2. 양자화 (학습 결과 참조)
quant_scenario = QuantizationScenario(
    name="ppe-yolov8s-v1-int8",
    model=ModelConfig(architecture="yolov8s", num_classes=5),
    environment=EnvironmentConfig(device="tensorrt", precision="int8"),
    dataset=DatasetConfig(name="ppe", version="v2.1", dvc_path="data/ppe.dvc", split="val"),
    quantization=QuantizationParams(method="static", backend="tensorrt"),
    source_run_id=train_run_id  # 학습 run 연결
)
quant_run_id = create_runner(quant_scenario).run()

# 3. 추론 벤치마크 (양자화 모델)
bench_scenario = InferenceBenchmarkScenario(
    name="ppe-yolov8s-v1-int8-bench",
    model=ModelConfig(architecture="yolov8s", weights_path=f"runs:/{quant_run_id}/model_quantized.engine"),
    environment=EnvironmentConfig(device="tensorrt", precision="int8"),
    benchmark=InferenceBenchmarkParams(batch_sizes=[1, 4, 8])
)
create_runner(bench_scenario).run()
```

---

## MLflow 실험 구조
```
MLflow Experiments
├── guardianflow-training/
│   ├── ppe-yolov8s-v1 (run)
│   └── emergency-yolov8m-v1 (run)
├── guardianflow-quantization/
│   ├── ppe-yolov8s-v1-int8 (run) → links to training run
│   └── ...
└── guardianflow-inference_benchmark/
    ├── ppe-yolov8s-v1-int8-bench (run)
    └── ...
```

---

## 디렉토리 구조 (최종)
```
guardianflow/
├── scenario/
│   ├── schema.py       # 모든 dataclass 정의
│   ├── runner.py       # BaseRunner + 타입별 Runner
│   └── factory.py      # create_runner
├── pipelines/
│   ├── data.py         # DVC 연동
│   ├── train.py        # 학습 로직
│   ├── benchmark.py    # 추론 벤치마크 로직
│   └── quantize.py     # 양자화 로직
└── experiments/
    ├── ppe_training.py
    ├── ppe_quantization.py
    └── ppe_benchmark.py

ExportBlock-9d70385d-e460-40da-9047-cf3266af3385.zip
zip

Model, Experiment 관리 전략은 아래와 같음

30 Jan

명령 2개 실행함, 파일 조회함
문서 전체를 확인했습니다. 기존 컨벤션 문서와 시나리오 스키마 설계를 통합 분석해보겠습니다.

핵심 분석
기존 문서의 구조:

Experiment = 현장/고객사/구역 (site context)
Run = 실험 타입(train/infer/quant) + 버전
Registered Model = 태스크별 모델 + 양자화 variant
이 구조와 시나리오 스키마를 정합시켜야 함.

통합 스키마 설계
python
# scenario/schema.py
from dataclasses import dataclass, field
from typing import Literal
from abc import ABC

# ─────────────────────────────────────────────
# 현장 컨텍스트 (Experiment 레벨)
# ─────────────────────────────────────────────

@dataclass
class SiteConfig:
    """MLflow Experiment 이름 생성 기준"""
    customer: str              # donghwa-entec, sampyo, duckwon
    region: str                # busan, seoul, gimhae
    plant: str = "plant1"      # plant1, plant2
    zone: str = "full"         # welding, brazing, bct, packer, packaging, full
    
    def to_experiment_name(self) -> str:
        """MLflow Experiment 네이밍: customer/region-plant/zone"""
        return f"{self.customer}/{self.region}-{self.plant}/{self.zone}"
    
    def to_tags(self) -> dict:
        return {
            "site.customer": self.customer,
            "site.region": self.region,
            "site.plant": self.plant,
            "site.zone": self.zone,
        }

# ─────────────────────────────────────────────
# 태스크 정의
# ─────────────────────────────────────────────

@dataclass
class TaskConfig:
    """탐지 태스크 정의"""
    type: Literal["ppe", "fall", "fire", "collision", "stuck"]
    classes: list[str] = field(default_factory=list)  # headgear, harness, hook 등
    
    def to_tags(self) -> dict:
        return {
            "task.type": self.type,
            "task.classes": ",".join(self.classes),
        }

# ─────────────────────────────────────────────
# 공통 컴포넌트
# ─────────────────────────────────────────────

@dataclass
class DatasetConfig:
    version: str                    # v1.2
    dvc_hash: str | None = None     # DVC 연동 시
    path: str | None = None         # s3://bucket/datasets/...
    image_count: int | None = None
    
    def to_tags(self) -> dict:
        tags = {"data.version": self.version}
        if self.dvc_hash:
            tags["data.dvc_hash"] = self.dvc_hash
        if self.path:
            tags["data.path"] = self.path
        return tags

@dataclass
class ModelConfig:
    architecture: str              # yolov11n, yolov11s, yolov11l
    input_size: int = 640          # 640, 1280
    weights_path: str | None = None
    num_classes: int = 10
    
    def to_params(self) -> dict:
        return {
            "model.architecture": self.architecture,
            "model.input_size": self.input_size,
            "model.num_classes": self.num_classes,
        }

@dataclass 
class EnvironmentConfig:
    device: Literal["cuda", "cpu", "tensorrt", "openvino"] = "cuda"
    precision: Literal["fp32", "fp16", "int8"] = "fp32"
    seed: int = 42

# ─────────────────────────────────────────────
# 실험 타입별 파라미터
# ─────────────────────────────────────────────

@dataclass
class TrainingParams:
    epochs: int = 100
    batch_size: int = 16
    learning_rate: float = 0.001
    optimizer: Literal["adam", "adamw", "sgd"] = "adamw"
    scheduler: str | None = "cosine"
    early_stopping: int | None = 10
    
@dataclass
class InferenceBenchmarkParams:
    batch_sizes: list[int] = field(default_factory=lambda: [1, 8, 16, 32])
    warmup_runs: int = 10
    benchmark_runs: int = 100
    conf_threshold: float = 0.25
    iou_threshold: float = 0.45

@dataclass
class QuantizationParams:
    method: Literal["dynamic", "static", "qat"] = "static"
    backend: Literal["tensorrt", "openvino", "onnx"] = "openvino"
    calibration_size: int = 500
    calibration_method: Literal["entropy", "minmax"] = "entropy"

# ─────────────────────────────────────────────
# 시나리오 (Base + 타입별)
# ─────────────────────────────────────────────

@dataclass
class BaseScenario(ABC):
    """모든 시나리오 공통 - MLflow Run 생성 단위"""
    name: str                       # run name
    version: str                    # v1, v2.1
    experiment_type: str            # train, infer, quant
    
    site: SiteConfig
    task: TaskConfig
    model: ModelConfig
    environment: EnvironmentConfig
    dataset: DatasetConfig | None = None
    
    def get_experiment_name(self) -> str:
        return self.site.to_experiment_name()
    
    def get_run_name(self) -> str:
        """Run 네이밍: {task}-{architecture}-{exp_type}-{version}"""
        return f"{self.task.type}-{self.model.architecture}-{self.experiment_type}-{self.version}"
    
    def get_registered_model_name(self, variant: str = "fp32") -> str:
        """Registered Model 네이밍"""
        base = f"{self.site.to_experiment_name()}/{self.task.type}"
        if variant != "fp32":
            return f"{base}__{variant}"
        return base
    
    def to_all_tags(self) -> dict:
        tags = {
            "exp.type": self.experiment_type,
            "exp.version": self.version,
        }
        tags.update(self.site.to_tags())
        tags.update(self.task.to_tags())
        if self.dataset:
            tags.update(self.dataset.to_tags())
        return tags
    
    def to_all_params(self) -> dict:
        params = self.model.to_params()
        params.update({
            "env.device": self.environment.device,
            "env.precision": self.environment.precision,
            "env.seed": self.environment.seed,
        })
        return params


@dataclass
class TrainingScenario(BaseScenario):
    experiment_type: str = field(default="train", init=False)
    training: TrainingParams = None
    
    def to_all_params(self) -> dict:
        params = super().to_all_params()
        if self.training:
            params.update({
                "train.epochs": self.training.epochs,
                "train.batch_size": self.training.batch_size,
                "train.learning_rate": self.training.learning_rate,
                "train.optimizer": self.training.optimizer,
            })
        return params


@dataclass
class InferenceBenchmarkScenario(BaseScenario):
    experiment_type: str = field(default="infer", init=False)
    benchmark: InferenceBenchmarkParams = None
    source_run_id: str | None = None  # 원본 학습 run
    
    def to_all_params(self) -> dict:
        params = super().to_all_params()
        if self.benchmark:
            params.update({
                "bench.batch_sizes": str(self.benchmark.batch_sizes),
                "bench.warmup_runs": self.benchmark.warmup_runs,
                "bench.conf_threshold": self.benchmark.conf_threshold,
            })
        return params
    
    def to_all_tags(self) -> dict:
        tags = super().to_all_tags()
        if self.source_run_id:
            tags["source.run_id"] = self.source_run_id
        return tags


@dataclass
class QuantizationScenario(BaseScenario):
    experiment_type: str = field(default="quant", init=False)
    quantization: QuantizationParams = None
    source_run_id: str = None  # 필수: 원본 학습 run
    
    def to_all_params(self) -> dict:
        params = super().to_all_params()
        if self.quantization:
            params.update({
                "quant.method": self.quantization.method,
                "quant.backend": self.quantization.backend,
                "quant.calibration_size": self.quantization.calibration_size,
            })
        return params
    
    def get_registered_model_name(self, variant: str = None) -> str:
        """양자화 모델은 자동으로 variant 포함"""
        if variant is None:
            variant = f"{self.quantization.backend}_{self.environment.precision}"
        return super().get_registered_model_name(variant)
사용 예시 (동화엔텍 시나리오)
python
# experiments/donghwa_entec_welding.py

# 1. 학습 시나리오
train_scenario = TrainingScenario(
    name="ppe-welding-baseline",
    version="v2",
    site=SiteConfig(
        customer="donghwa-entec",
        region="busan", 
        plant="plant1",
        zone="welding"
    ),
    task=TaskConfig(type="ppe", classes=["headgear", "harness", "hook"]),
    model=ModelConfig(architecture="yolov11l", input_size=1280, num_classes=3),
    environment=EnvironmentConfig(device="cuda", precision="fp16"),
    dataset=DatasetConfig(version="v1.2", path="s3://bucket/donghwa-ppe-v1.2"),
    training=TrainingParams(epochs=100, batch_size=16, learning_rate=0.01)
)

print(train_scenario.get_experiment_name())
# → donghwa-entec/busan-plant1/welding

print(train_scenario.get_run_name())
# → ppe-yolov11l-train-v2

print(train_scenario.get_registered_model_name())
# → donghwa-entec/busan-plant1/welding/ppe

# 2. 양자화 시나리오
quant_scenario = QuantizationScenario(
    name="ppe-welding-openvino",
    version="v2",
    site=train_scenario.site,
    task=train_scenario.task,
    model=train_scenario.model,
    environment=EnvironmentConfig(device="openvino", precision="int8"),
    quantization=QuantizationParams(backend="openvino", calibration_size=500),
    source_run_id="abc123"
)

print(quant_scenario.get_registered_model_name())
# → donghwa-entec/busan-plant1/welding/ppe__openvino_int8
```

---

## MLflow 구조 매핑 (최종)
```
MLflow
├── Experiments (현장 컨텍스트)
│   ├── donghwa-entec/busan-plant1/welding
│   │   ├── Run: ppe-yolov11l-train-v1
│   │   ├── Run: ppe-yolov11l-train-v2
│   │   ├── Run: ppe-yolov11l-infer-v2
│   │   └── Run: ppe-yolov11l-quant-v2
│   │
│   └── sampyo/gimhae-plant1/bct
│       ├── Run: ppe-yolov11s-train-v1
│       └── ...
│
└── Registered Models (배포 단위)
    ├── donghwa-entec/busan-plant1/welding/ppe
    │   ├── v1 @rollback
    │   └── v2 @champion
    │
    ├── donghwa-entec/busan-plant1/welding/ppe__openvino_int8
    │   └── v2 @champion
    │
    └── sampyo/gimhae-plant1/bct/ppe
        └── v1 @candidate
```

---

## 디렉토리 구조
```
guardianflow/
├── scenario/
│   ├── schema.py           # 위 dataclass 전체
│   ├── runner.py           # 타입별 Runner
│   └── factory.py          # create_runner
├── pipelines/
│   ├── train.py
│   ├── benchmark.py
│   └── quantize.py
├── experiments/
│   ├── donghwa_entec/
│   │   ├── welding_ppe.py
│   │   └── brazing_ppe.py
│   └── sampyo/
│       ├── gimhae_bct.py
│       └── sejong_packer.py
└── configs/
    └── sites/              # YAML로 사이트 정보 관리 (optional)
        ├── donghwa_entec.yaml
        └── sampyo.yaml
Yolo 기본 및 mot 메트릭을 기록해야 함. 시스템 메트릭도 기록해야 함.

30 Jan
핵심 요약
메트릭을 3개 카테고리로 분리: Detection Metrics, MOT Metrics, System Metrics. 각 실험 타입별로 기록 대상이 다름.

메트릭 카테고리별 정의
카테고리	실험 타입	기록 시점
Detection (YOLO)	train, infer	epoch별 / 최종
MOT (Tracking)	infer	최종
System	train, infer, quant	step별 / 최종
스키마 확장
python
# scenario/metrics.py
from dataclasses import dataclass, field, asdict
from typing import Literal

# ─────────────────────────────────────────────
# Detection Metrics (YOLO 기본)
# ─────────────────────────────────────────────

@dataclass
class DetectionMetrics:
    """YOLO 학습/추론 메트릭"""
    # Box metrics
    mAP50: float = None
    mAP50_95: float = None
    precision: float = None
    recall: float = None
    f1: float = None
    
    # Loss (train only)
    box_loss: float = None
    cls_loss: float = None
    dfl_loss: float = None
    
    # Per-class (optional)
    per_class_ap: dict[str, float] = field(default_factory=dict)
    
    def to_mlflow_dict(self, prefix: str = "det") -> dict:
        result = {}
        for k, v in asdict(self).items():
            if v is not None and k != "per_class_ap":
                result[f"{prefix}.{k}"] = v
        for cls_name, ap in self.per_class_ap.items():
            result[f"{prefix}.ap.{cls_name}"] = ap
        return result


# ─────────────────────────────────────────────
# MOT Metrics (Multi-Object Tracking)
# ─────────────────────────────────────────────

@dataclass
class MOTMetrics:
    """CLEAR MOT + HOTA 메트릭"""
    # CLEAR MOT
    mota: float = None          # Multi-Object Tracking Accuracy
    motp: float = None          # Multi-Object Tracking Precision
    idf1: float = None          # ID F1 Score
    
    # Identity
    id_switches: int = None     # ID 전환 횟수
    id_precision: float = None
    id_recall: float = None
    
    # Fragmentation
    frag: int = None            # Fragmentation count
    mt: int = None              # Mostly Tracked
    ml: int = None              # Mostly Lost
    pt: int = None              # Partially Tracked
    
    # HOTA (Higher Order Tracking Accuracy)
    hota: float = None
    deta: float = None          # Detection Accuracy
    assa: float = None          # Association Accuracy
    loca: float = None          # Localization Accuracy
    
    # Counts
    gt_tracks: int = None
    pred_tracks: int = None
    fp: int = None              # False Positives
    fn: int = None              # False Negatives
    
    def to_mlflow_dict(self, prefix: str = "mot") -> dict:
        return {
            f"{prefix}.{k}": v 
            for k, v in asdict(self).items() 
            if v is not None
        }


# ─────────────────────────────────────────────
# System Metrics
# ─────────────────────────────────────────────

@dataclass
class SystemMetrics:
    """하드웨어 및 성능 메트릭"""
    # Latency
    inference_time_ms: float = None      # 평균 추론 시간
    inference_time_p50_ms: float = None
    inference_time_p95_ms: float = None
    inference_time_p99_ms: float = None
    preprocessing_time_ms: float = None
    postprocessing_time_ms: float = None
    
    # Throughput
    fps: float = None
    throughput_imgs_per_sec: float = None
    
    # GPU
    gpu_memory_used_mb: float = None
    gpu_memory_peak_mb: float = None
    gpu_utilization_pct: float = None
    gpu_power_watts: float = None
    gpu_temperature_c: float = None
    
    # CPU
    cpu_utilization_pct: float = None
    cpu_memory_used_mb: float = None
    
    # Model size
    model_size_mb: float = None
    model_params_m: float = None          # 파라미터 수 (millions)
    model_flops_g: float = None           # GFLOPs
    
    def to_mlflow_dict(self, prefix: str = "sys") -> dict:
        return {
            f"{prefix}.{k}": v 
            for k, v in asdict(self).items() 
            if v is not None
        }


# ─────────────────────────────────────────────
# 실험 타입별 메트릭 컨테이너
# ─────────────────────────────────────────────

@dataclass
class TrainingMetricsSnapshot:
    """학습 중 epoch별 스냅샷"""
    epoch: int
    detection: DetectionMetrics
    system: SystemMetrics = None
    
    def to_mlflow_dict(self) -> dict:
        result = self.detection.to_mlflow_dict()
        if self.system:
            result.update(self.system.to_mlflow_dict())
        return result


@dataclass
class InferenceMetricsResult:
    """추론 벤치마크 최종 결과"""
    detection: DetectionMetrics
    mot: MOTMetrics = None          # tracking 사용 시
    system: SystemMetrics = None
    
    def to_mlflow_dict(self) -> dict:
        result = self.detection.to_mlflow_dict()
        if self.mot:
            result.update(self.mot.to_mlflow_dict())
        if self.system:
            result.update(self.system.to_mlflow_dict())
        return result


@dataclass
class QuantizationMetricsResult:
    """양자화 결과 비교"""
    original: DetectionMetrics
    quantized: DetectionMetrics
    system_original: SystemMetrics
    system_quantized: SystemMetrics
    
    # 비교 메트릭
    accuracy_drop_mAP50: float = None
    speedup_ratio: float = None
    compression_ratio: float = None
    
    def to_mlflow_dict(self) -> dict:
        result = {}
        # Original
        for k, v in self.original.to_mlflow_dict("det.original").items():
            result[k] = v
        for k, v in self.system_original.to_mlflow_dict("sys.original").items():
            result[k] = v
        # Quantized
        for k, v in self.quantized.to_mlflow_dict("det.quantized").items():
            result[k] = v
        for k, v in self.system_quantized.to_mlflow_dict("sys.quantized").items():
            result[k] = v
        # Comparison
        if self.accuracy_drop_mAP50 is not None:
            result["comparison.accuracy_drop_mAP50"] = self.accuracy_drop_mAP50
        if self.speedup_ratio is not None:
            result["comparison.speedup_ratio"] = self.speedup_ratio
        if self.compression_ratio is not None:
            result["comparison.compression_ratio"] = self.compression_ratio
        return result
메트릭 수집 유틸리티
python
# scenario/metrics_collector.py
import time
from contextlib import contextmanager
from dataclasses import dataclass
import torch

try:
    import pynvml
    NVML_AVAILABLE = True
except ImportError:
    NVML_AVAILABLE = False


class SystemMetricsCollector:
    """실시간 시스템 메트릭 수집"""
    
    def __init__(self, device_id: int = 0):
        self.device_id = device_id
        self._init_nvml()
        self._latencies = []
    
    def _init_nvml(self):
        if NVML_AVAILABLE and torch.cuda.is_available():
            pynvml.nvmlInit()
            self._handle = pynvml.nvmlDeviceGetHandleByIndex(self.device_id)
        else:
            self._handle = None
    
    @contextmanager
    def measure_latency(self):
        """추론 시간 측정 컨텍스트"""
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        start = time.perf_counter()
        yield
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        elapsed_ms = (time.perf_counter() - start) * 1000
        self._latencies.append(elapsed_ms)
    
    def get_gpu_metrics(self) -> dict:
        if not self._handle:
            return {}
        
        mem_info = pynvml.nvmlDeviceGetMemoryInfo(self._handle)
        util = pynvml.nvmlDeviceGetUtilizationRates(self._handle)
        power = pynvml.nvmlDeviceGetPowerUsage(self._handle) / 1000  # mW → W
        temp = pynvml.nvmlDeviceGetTemperature(self._handle, pynvml.NVML_TEMPERATURE_GPU)
        
        return {
            "gpu_memory_used_mb": mem_info.used / 1024**2,
            "gpu_utilization_pct": util.gpu,
            "gpu_power_watts": power,
            "gpu_temperature_c": temp,
        }
    
    def get_latency_stats(self) -> dict:
        if not self._latencies:
            return {}
        
        import numpy as np
        arr = np.array(self._latencies)
        return {
            "inference_time_ms": float(np.mean(arr)),
            "inference_time_p50_ms": float(np.percentile(arr, 50)),
            "inference_time_p95_ms": float(np.percentile(arr, 95)),
            "inference_time_p99_ms": float(np.percentile(arr, 99)),
            "fps": 1000.0 / float(np.mean(arr)),
        }
    
    def to_system_metrics(self) -> SystemMetrics:
        data = {}
        data.update(self.get_latency_stats())
        data.update(self.get_gpu_metrics())
        return SystemMetrics(**data)
    
    def reset(self):
        self._latencies = []


def get_model_stats(model: torch.nn.Module, input_size: tuple = (1, 3, 640, 640)) -> dict:
    """모델 파라미터 및 FLOPs 계산"""
    params = sum(p.numel() for p in model.parameters()) / 1e6  # millions
    
    try:
        from thop import profile
        dummy = torch.zeros(input_size).to(next(model.parameters()).device)
        flops, _ = profile(model, inputs=(dummy,), verbose=False)
        flops_g = flops / 1e9
    except:
        flops_g = None
    
    return {
        "model_params_m": params,
        "model_flops_g": flops_g,
    }
Runner 통합
python
# scenario/runner.py (메트릭 통합 버전)
import mlflow
from .metrics import *
from .metrics_collector import SystemMetricsCollector, get_model_stats


class TrainingRunner(BaseRunner):
    def _execute(self):
        s = self.scenario
        collector = SystemMetricsCollector()
        
        # 모델 통계
        model = self._build_model()
        model_stats = get_model_stats(model, (1, 3, s.model.input_size, s.model.input_size))
        mlflow.log_params(model_stats)
        
        for epoch in range(s.training.epochs):
            # 학습 1 epoch
            train_loss = self._train_one_epoch(model, epoch)
            val_metrics = self._validate(model)
            
            # 메트릭 스냅샷
            snapshot = TrainingMetricsSnapshot(
                epoch=epoch,
                detection=DetectionMetrics(
                    mAP50=val_metrics["mAP50"],
                    mAP50_95=val_metrics["mAP50-95"],
                    precision=val_metrics["precision"],
                    recall=val_metrics["recall"],
                    box_loss=train_loss["box"],
                    cls_loss=train_loss["cls"],
                    dfl_loss=train_loss["dfl"],
                ),
                system=collector.to_system_metrics()
            )
            
            mlflow.log_metrics(snapshot.to_mlflow_dict(), step=epoch)
            collector.reset()
        
        # 최종 모델 저장
        mlflow.log_artifact("weights/best.pt")


class InferenceBenchmarkRunner(BaseRunner):
    def _execute(self):
        s = self.scenario
        collector = SystemMetricsCollector()
        
        model = self._load_model()
        dataloader = self._get_test_dataloader()
        
        all_preds, all_gts = [], []
        
        # Warmup
        for _ in range(s.benchmark.warmup_runs):
            self._inference_one(model, next(iter(dataloader)))
        
        # Benchmark
        for batch in dataloader:
            with collector.measure_latency():
                preds = self._inference_one(model, batch)
            all_preds.extend(preds)
            all_gts.extend(batch["labels"])
        
        # Detection 메트릭 계산
        det_metrics = self._compute_detection_metrics(all_preds, all_gts)
        
        # MOT 메트릭 (tracking 사용 시)
        mot_metrics = None
        if hasattr(s, "use_tracking") and s.use_tracking:
            mot_metrics = self._compute_mot_metrics(all_preds, all_gts)
        
        # 결과 집계
        result = InferenceMetricsResult(
            detection=det_metrics,
            mot=mot_metrics,
            system=collector.to_system_metrics()
        )
        
        mlflow.log_metrics(result.to_mlflow_dict())


class QuantizationRunner(BaseRunner):
    def _execute(self):
        s = self.scenario
        
        # 원본 모델 로드 및 벤치마크
        original_model = self._load_original_model()
        original_det, original_sys = self._benchmark_model(original_model)
        
        # 양자화 실행
        quantized_model = self._quantize(original_model)
        
        # 양자화 모델 벤치마크
        quantized_det, quantized_sys = self._benchmark_model(quantized_model)
        
        # 결과 집계
        result = QuantizationMetricsResult(
            original=original_det,
            quantized=quantized_det,
            system_original=original_sys,
            system_quantized=quantized_sys,
            accuracy_drop_mAP50=original_det.mAP50 - quantized_det.mAP50,
            speedup_ratio=original_sys.inference_time_ms / quantized_sys.inference_time_ms,
            compression_ratio=original_sys.model_size_mb / quantized_sys.model_size_mb,
        )
        
        mlflow.log_metrics(result.to_mlflow_dict())
        mlflow.log_artifact(f"model_quantized.{s.quantization.backend}")
```

---

## MLflow 메트릭 구조 (최종)
```
Run Metrics
├── det.*                    # Detection
│   ├── mAP50, mAP50_95
│   ├── precision, recall, f1
│   ├── box_loss, cls_loss, dfl_loss
│   └── ap.{class_name}      # per-class
│
├── mot.*                    # MOT (infer only)
│   ├── mota, motp, idf1
│   ├── hota, deta, assa
│   ├── id_switches, frag
│   └── mt, ml, pt
│
├── sys.*                    # System
│   ├── inference_time_ms, fps
│   ├── inference_time_p50/p95/p99_ms
│   ├── gpu_memory_used_mb, gpu_utilization_pct
│   └── model_params_m, model_flops_g
│
└── comparison.*             # Quantization only
    ├── accuracy_drop_mAP50
    ├── speedup_ratio
    └── compression_ratio
실험 타입별 메트릭 매트릭스
메트릭	train	infer	quant
det.mAP50	✅ (step)	✅	✅ (orig/quant)
det.loss	✅ (step)	-	-
mot.*	-	✅	-
sys.inference_time	-	✅	✅
sys.gpu_*	✅	✅	✅
comparison.*	-	-	✅
실험 코드는 전부 해당 디렉터리 밖에 존재함 이를 MLproject 단위로 관리가 가능한지

30 Jan
핵심 요약
MLproject는 MLflow의 패키징 단위로, 외부 실험 코드를 독립적인 실행 단위로 관리 가능. 각 실험 타입(train/infer/quant)을 별도 MLproject로 구성하거나, 단일 MLproject에서 entry point로 분기.

구조 옵션
Option A: 단일 MLproject + 다중 Entry Points (권장)
guardianflow/                    # MLOps 코어
├── MLproject                    # 진입점 정의
├── conda.yaml                   # 의존성
├── scenario/
│   ├── schema.py
│   ├── runner.py
│   └── metrics.py
└── pipelines/
    ├── train.py
    ├── infer.py
    └── quant.py

experiments/                     # 외부 실험 코드 (별도 repo 가능)
├── yolo_trainer/
│   ├── train.py
│   └── utils/
├── mot_benchmark/
│   └── evaluate.py
└── quantization/
    ├── openvino_export.py
    └── tensorrt_export.py
Option B: 실험 타입별 MLproject
projects/
├── training/
│   ├── MLproject
│   └── conda.yaml
├── inference/
│   ├── MLproject
│   └── conda.yaml
└── quantization/
    ├── MLproject
    └── conda.yaml
권장: Option A - 공통 의존성 관리 용이, 시나리오 스키마 공유

MLproject 정의
yaml
# guardianflow/MLproject
name: guardianflow

conda_env: conda.yaml

entry_points:
  # ─────────────────────────────────────────────
  # Training
  # ─────────────────────────────────────────────
  train:
    parameters:
      scenario_path: {type: str}
      experiment_code_path: {type: str, default: "../experiments/yolo_trainer"}
    command: |
      python -m pipelines.train \
        --scenario {scenario_path} \
        --code-path {experiment_code_path}

  # ─────────────────────────────────────────────
  # Inference Benchmark
  # ─────────────────────────────────────────────
  infer:
    parameters:
      scenario_path: {type: str}
      experiment_code_path: {type: str, default: "../experiments/mot_benchmark"}
      source_run_id: {type: str, default: ""}
    command: |
      python -m pipelines.infer \
        --scenario {scenario_path} \
        --code-path {experiment_code_path} \
        --source-run-id {source_run_id}

  # ─────────────────────────────────────────────
  # Quantization
  # ─────────────────────────────────────────────
  quant:
    parameters:
      scenario_path: {type: str}
      experiment_code_path: {type: str, default: "../experiments/quantization"}
      source_run_id: {type: str}
      backend: {type: str, default: "openvino"}
    command: |
      python -m pipelines.quant \
        --scenario {scenario_path} \
        --code-path {experiment_code_path} \
        --source-run-id {source_run_id} \
        --backend {backend}

  # ─────────────────────────────────────────────
  # Full Pipeline (train → quant → infer)
  # ─────────────────────────────────────────────
  full_pipeline:
    parameters:
      scenario_path: {type: str}
    command: |
      python -m pipelines.orchestrator --scenario {scenario_path}
yaml
# guardianflow/conda.yaml
name: guardianflow
channels:
  - conda-forge
  - pytorch
dependencies:
  - python=3.10
  - pytorch>=2.0
  - torchvision
  - pip
  - pip:
    - mlflow>=2.10
    - ultralytics>=8.0
    - pynvml
    - thop
    - pyyaml
    - trackeval  # MOT 메트릭
파이프라인 구현 (외부 코드 연동)
python
# guardianflow/pipelines/train.py
import argparse
import importlib.util
import sys
from pathlib import Path

import mlflow
import yaml

from scenario.schema import TrainingScenario
from scenario.metrics import TrainingMetricsSnapshot, DetectionMetrics
from scenario.metrics_collector import SystemMetricsCollector


def load_external_trainer(code_path: str):
    """외부 실험 코드 동적 로드"""
    code_path = Path(code_path)
    
    # 외부 코드 경로를 sys.path에 추가
    sys.path.insert(0, str(code_path))
    
    # trainer 모듈 로드 (규약: train.py에 Trainer 클래스 존재)
    spec = importlib.util.spec_from_file_location("trainer", code_path / "train.py")
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    
    return module.Trainer


def load_scenario(scenario_path: str) -> TrainingScenario:
    with open(scenario_path) as f:
        data = yaml.safe_load(f)
    # YAML → dataclass 변환 로직
    return TrainingScenario(**data)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--scenario", required=True)
    parser.add_argument("--code-path", required=True)
    args = parser.parse_args()
    
    scenario = load_scenario(args.scenario)
    TrainerClass = load_external_trainer(args.code_path)
    
    # MLflow 설정
    mlflow.set_experiment(scenario.get_experiment_name())
    
    with mlflow.start_run(run_name=scenario.get_run_name()):
        # 파라미터 및 태그 기록
        mlflow.log_params(scenario.to_all_params())
        mlflow.set_tags(scenario.to_all_tags())
        
        # 시나리오 config artifact 저장
        mlflow.log_artifact(args.scenario, "config")
        
        # 외부 Trainer 초기화 및 실행
        trainer = TrainerClass(
            model_config=scenario.model,
            training_config=scenario.training,
            dataset_config=scenario.dataset,
        )
        
        collector = SystemMetricsCollector()
        
        # 콜백 주입: epoch 종료 시 메트릭 기록
        def on_epoch_end(epoch: int, metrics: dict):
            snapshot = TrainingMetricsSnapshot(
                epoch=epoch,
                detection=DetectionMetrics(**metrics),
                system=collector.to_system_metrics()
            )
            mlflow.log_metrics(snapshot.to_mlflow_dict(), step=epoch)
            collector.reset()
        
        trainer.on_epoch_end = on_epoch_end
        
        # 학습 실행
        trainer.train()
        
        # 최종 모델 등록
        best_model_path = trainer.get_best_model_path()
        mlflow.log_artifact(best_model_path, "weights")
        
        # Model Registry 등록
        model_uri = f"runs:/{mlflow.active_run().info.run_id}/weights"
        mlflow.register_model(model_uri, scenario.get_registered_model_name())


if __name__ == "__main__":
    main()
외부 실험 코드 인터페이스 규약
python
# experiments/yolo_trainer/train.py
"""
외부 Trainer는 이 인터페이스를 구현해야 함
"""
from abc import ABC, abstractmethod
from typing import Callable


class BaseTrainer(ABC):
    """GuardianFlow 연동을 위한 Trainer 인터페이스"""
    
    # 콜백 (Runner에서 주입)
    on_epoch_end: Callable[[int, dict], None] = None
    on_batch_end: Callable[[int, dict], None] = None
    
    @abstractmethod
    def train(self) -> None:
        """학습 실행"""
        pass
    
    @abstractmethod
    def get_best_model_path(self) -> str:
        """최적 모델 경로 반환"""
        pass


class Trainer(BaseTrainer):
    """YOLO Trainer 구현"""
    
    def __init__(self, model_config, training_config, dataset_config):
        self.model_config = model_config
        self.training_config = training_config
        self.dataset_config = dataset_config
        
        # YOLO 모델 초기화
        from ultralytics import YOLO
        self.model = YOLO(model_config.architecture)
    
    def train(self):
        # Ultralytics 학습
        results = self.model.train(
            data=self.dataset_config.path,
            epochs=self.training_config.epochs,
            batch=self.training_config.batch_size,
            imgsz=self.model_config.input_size,
            lr0=self.training_config.learning_rate,
            optimizer=self.training_config.optimizer,
        )
        
        # 콜백 호출 (Ultralytics 내부 콜백 연동)
        # Note: 실제로는 Ultralytics callback 시스템 활용
        for epoch, metrics in enumerate(results.metrics_per_epoch):
            if self.on_epoch_end:
                self.on_epoch_end(epoch, {
                    "mAP50": metrics["mAP50"],
                    "mAP50_95": metrics["mAP50-95"],
                    "precision": metrics["precision"],
                    "recall": metrics["recall"],
                    "box_loss": metrics["box_loss"],
                    "cls_loss": metrics["cls_loss"],
                })
    
    def get_best_model_path(self) -> str:
        return str(self.model.trainer.best)
실행 방법
bash
# 1. 단일 학습 실행
mlflow run guardianflow/ \
    -e train \
    -P scenario_path=configs/scenarios/donghwa_welding_v1.yaml \
    -P experiment_code_path=../experiments/yolo_trainer

# 2. 양자화 실행 (학습 run 참조)
mlflow run guardianflow/ \
    -e quant \
    -P scenario_path=configs/scenarios/donghwa_welding_quant_v1.yaml \
    -P source_run_id=abc123def456 \
    -P backend=openvino

# 3. 전체 파이프라인
mlflow run guardianflow/ \
    -e full_pipeline \
    -P scenario_path=configs/scenarios/donghwa_welding_v1.yaml

# 4. 원격 Git repo에서 실행
mlflow run git@github.com:company/guardianflow.git \
    -e train \
    -P scenario_path=configs/scenarios/donghwa_welding_v1.yaml
```

---

## 디렉토리 구조 (최종)
```
/workspace
├── guardianflow/                    # MLOps 코어 (MLproject)
│   ├── MLproject
│   ├── conda.yaml
│   ├── scenario/
│   │   ├── schema.py               # Scenario dataclasses
│   │   ├── metrics.py              # 메트릭 정의
│   │   └── metrics_collector.py    # 시스템 메트릭 수집
│   ├── pipelines/
│   │   ├── train.py                # Entry point 구현
│   │   ├── infer.py
│   │   ├── quant.py
│   │   └── orchestrator.py         # 전체 파이프라인
│   └── configs/
│       └── scenarios/              # 시나리오 YAML
│           ├── donghwa_welding_v1.yaml
│           └── sampyo_bct_v1.yaml
│
├── experiments/                     # 외부 실험 코드 (별도 관리)
│   ├── yolo_trainer/
│   │   ├── train.py                # Trainer 구현
│   │   └── callbacks.py
│   ├── mot_benchmark/
│   │   ├── evaluate.py
│   │   └── trackeval_wrapper.py
│   └── quantization/
│       ├── openvino_export.py
│       └── tensorrt_export.py
│
└── mlruns/                          # MLflow 저장소 (또는 원격 서버)
Trade-off 정리
방식	장점	단점
MLproject + 외부 코드 경로	기존 코드 수정 최소화, 유연함	인터페이스 규약 필요
MLproject 내부에 코드 포함	완전한 재현성	코드 중복, 유지보수 어려움
Git submodule	버전 관리 명확	submodule 관리 복잡도
권장: MLproject + 외부 코드 경로 + 인터페이스 규약. POC 이후 안정화되면 Git submodule 고려.

예를 들어 ~/experiments/ 아래 donghwa-entec-ppe, donghwa-entec-fall, sampyo-gimhae-bct

이런식으호 30개 넘는 디렉터리가 있고, 각각 아래에 학습, 추론, 양자화 등이 섞여있음

30 Jan
핵심 분석
현재 상태: 고객/태스크별 디렉토리에 실험 코드가 비정형으로 혼재. 이를 MLproject로 관리하려면 기존 구조 유지 + 메타데이터 레이어 추가 방식이 현실적.

현재 구조 추정
~/experiments/
├── donghwa-entec-ppe/
│   ├── train_yolov11_v1.py
│   ├── train_yolov11_v2.py
│   ├── infer_benchmark.py
│   ├── export_openvino.py
│   ├── weights/
│   └── data/
├── donghwa-entec-fall/
│   ├── train.py
│   ├── quant_tensorrt.py
│   └── ...
├── sampyo-gimhae-bct/
│   ├── yolo_train.py
│   ├── yolo_infer.py
│   └── ...
└── ... (30+ 디렉토리)
문제점:

파일명/구조 비일관
실험 타입 구분 어려움
메타데이터(시나리오) 부재
재현성 확보 불가
전략: 비침습적 래퍼 + 레지스트리
기존 코드 수정 없이 외부에서 감싸는 방식

guardianflow/                        # MLOps 코어
├── MLproject
├── scenario/
├── pipelines/
└── registry/
    └── experiments.yaml             # 실험 디렉토리 매핑 정보

~/experiments/                       # 기존 코드 (수정 없음)
├── donghwa-entec-ppe/
├── donghwa-entec-fall/
└── ...
실험 레지스트리 정의
yaml
# guardianflow/registry/experiments.yaml
# 기존 실험 디렉토리 → 시나리오 매핑

experiments:
  # ─────────────────────────────────────────────
  # 동화엔텍
  # ─────────────────────────────────────────────
  donghwa-entec-ppe:
    path: ~/experiments/donghwa-entec-ppe
    site:
      customer: donghwa-entec
      region: busan
      plant: plant1
      zone: welding
    task:
      type: ppe
      classes: [headgear, harness, hook]
    
    # 파일 패턴 → 실험 타입 매핑
    entry_points:
      train:
        patterns:
          - "train*.py"
          - "*_train.py"
        default: train_yolov11_v2.py
        args_template: "--epochs {epochs} --batch {batch_size} --imgsz {input_size}"
      
      infer:
        patterns:
          - "infer*.py"
          - "*_benchmark.py"
          - "eval*.py"
        default: infer_benchmark.py
        args_template: "--weights {weights_path} --source {data_path}"
      
      quant:
        patterns:
          - "export*.py"
          - "quant*.py"
          - "*_openvino.py"
          - "*_tensorrt.py"
        default: export_openvino.py
        args_template: "--weights {weights_path} --format {backend}"

  donghwa-entec-fall:
    path: ~/experiments/donghwa-entec-fall
    site:
      customer: donghwa-entec
      region: busan
      plant: plant1
      zone: full
    task:
      type: fall
      classes: [person_fallen, person_standing]
    entry_points:
      train:
        default: train.py
      quant:
        default: quant_tensorrt.py

  # ─────────────────────────────────────────────
  # 삼표
  # ─────────────────────────────────────────────
  sampyo-gimhae-bct:
    path: ~/experiments/sampyo-gimhae-bct
    site:
      customer: sampyo
      region: gimhae
      plant: plant1
      zone: bct
    task:
      type: ppe
      classes: [helmet, vest]
    entry_points:
      train:
        default: yolo_train.py
      infer:
        default: yolo_infer.py
범용 실행기 구현
python
# guardianflow/pipelines/generic_runner.py
"""
기존 비정형 실험 코드를 MLflow로 래핑하는 범용 실행기
"""
import subprocess
import sys
from pathlib import Path
from dataclasses import dataclass
import yaml
import mlflow

from scenario.schema import SiteConfig, TaskConfig, BaseScenario
from scenario.metrics_collector import SystemMetricsCollector


@dataclass
class ExperimentEntry:
    """레지스트리에서 로드된 실험 정보"""
    name: str
    path: Path
    site: SiteConfig
    task: TaskConfig
    entry_points: dict


class ExperimentRegistry:
    """실험 디렉토리 레지스트리"""
    
    def __init__(self, registry_path: str = "registry/experiments.yaml"):
        with open(registry_path) as f:
            self._data = yaml.safe_load(f)["experiments"]
    
    def get(self, name: str) -> ExperimentEntry:
        if name not in self._data:
            raise KeyError(f"Unknown experiment: {name}")
        
        entry = self._data[name]
        return ExperimentEntry(
            name=name,
            path=Path(entry["path"]).expanduser(),
            site=SiteConfig(**entry["site"]),
            task=TaskConfig(**entry["task"]),
            entry_points=entry.get("entry_points", {}),
        )
    
    def list_all(self) -> list[str]:
        return list(self._data.keys())
    
    def find_by_customer(self, customer: str) -> list[str]:
        return [k for k, v in self._data.items() 
                if v["site"]["customer"] == customer]


class GenericRunner:
    """비정형 실험 코드 실행기"""
    
    def __init__(self, experiment_name: str, exp_type: str, version: str = "v1"):
        self.registry = ExperimentRegistry()
        self.entry = self.registry.get(experiment_name)
        self.exp_type = exp_type
        self.version = version
    
    def _resolve_script(self, override: str = None) -> Path:
        """실행할 스크립트 결정"""
        if override:
            return self.entry.path / override
        
        ep = self.entry.entry_points.get(self.exp_type, {})
        
        # 기본 스크립트
        if "default" in ep:
            return self.entry.path / ep["default"]
        
        # 패턴 매칭으로 자동 탐색
        patterns = ep.get("patterns", [])
        for pattern in patterns:
            matches = list(self.entry.path.glob(pattern))
            if matches:
                return matches[0]
        
        raise FileNotFoundError(
            f"No script found for {self.exp_type} in {self.entry.path}"
        )
    
    def _build_args(self, script_path: Path, params: dict) -> list[str]:
        """실행 인자 구성"""
        ep = self.entry.entry_points.get(self.exp_type, {})
        template = ep.get("args_template", "")
        
        if template:
            # 템플릿 기반 인자 생성
            args_str = template.format(**params)
            return args_str.split()
        else:
            # 기본: key=value 형식
            args = []
            for k, v in params.items():
                args.extend([f"--{k.replace('_', '-')}", str(v)])
            return args
    
    def run(
        self, 
        params: dict = None,
        script_override: str = None,
        capture_metrics: bool = True,
    ) -> str:
        """실험 실행 및 MLflow 기록"""
        
        params = params or {}
        script_path = self._resolve_script(script_override)
        
        # MLflow 설정
        experiment_name = f"{self.entry.site.customer}/{self.entry.site.region}-{self.entry.site.plant}/{self.entry.site.zone}"
        run_name = f"{self.entry.task.type}-{self.exp_type}-{self.version}"
        
        mlflow.set_experiment(experiment_name)
        
        with mlflow.start_run(run_name=run_name) as run:
            # 메타데이터 기록
            mlflow.set_tags({
                **self.entry.site.to_tags(),
                **self.entry.task.to_tags(),
                "exp.type": self.exp_type,
                "exp.version": self.version,
                "exp.script": str(script_path.name),
                "exp.source_dir": str(self.entry.path),
            })
            
            mlflow.log_params(params)
            
            # 스크립트 실행
            args = self._build_args(script_path, params)
            cmd = [sys.executable, str(script_path)] + args
            
            print(f"Executing: {' '.join(cmd)}")
            print(f"Working dir: {self.entry.path}")
            
            collector = SystemMetricsCollector() if capture_metrics else None
            
            result = subprocess.run(
                cmd,
                cwd=self.entry.path,
                capture_output=True,
                text=True,
            )
            
            # stdout/stderr 기록
            mlflow.log_text(result.stdout, "stdout.txt")
            mlflow.log_text(result.stderr, "stderr.txt")
            
            if result.returncode != 0:
                mlflow.set_tag("status", "FAILED")
                raise RuntimeError(f"Script failed: {result.stderr}")
            
            # 결과물 자동 수집
            self._collect_artifacts()
            
            # 시스템 메트릭
            if collector:
                mlflow.log_metrics(collector.to_system_metrics().to_mlflow_dict())
            
            return run.info.run_id
    
    def _collect_artifacts(self):
        """실험 디렉토리에서 결과물 자동 수집"""
        artifact_patterns = {
            "weights": ["*.pt", "*.pth", "*.onnx", "*.engine", "best.pt"],
            "results": ["results*.csv", "metrics.json", "*.png"],
            "configs": ["*.yaml", "*.yml", "config.json"],
        }
        
        for category, patterns in artifact_patterns.items():
            for pattern in patterns:
                for f in self.entry.path.rglob(pattern):
                    # 최근 수정된 파일만 (1시간 이내)
                    if self._is_recent(f):
                        mlflow.log_artifact(str(f), category)
    
    def _is_recent(self, path: Path, hours: float = 1.0) -> bool:
        import time
        mtime = path.stat().st_mtime
        return (time.time() - mtime) < hours * 3600
CLI 인터페이스
python
# guardianflow/cli.py
import argparse
from pipelines.generic_runner import GenericRunner, ExperimentRegistry


def main():
    parser = argparse.ArgumentParser(description="GuardianFlow MLOps CLI")
    subparsers = parser.add_subparsers(dest="command")
    
    # ─────────────────────────────────────────────
    # list: 등록된 실험 목록
    # ─────────────────────────────────────────────
    list_parser = subparsers.add_parser("list", help="List registered experiments")
    list_parser.add_argument("--customer", help="Filter by customer")
    
    # ─────────────────────────────────────────────
    # run: 실험 실행
    # ─────────────────────────────────────────────
    run_parser = subparsers.add_parser("run", help="Run experiment")
    run_parser.add_argument("experiment", help="Experiment name (e.g., donghwa-entec-ppe)")
    run_parser.add_argument("--type", "-t", required=True, choices=["train", "infer", "quant"])
    run_parser.add_argument("--version", "-v", default="v1")
    run_parser.add_argument("--script", "-s", help="Override script path")
    run_parser.add_argument("--param", "-p", action="append", help="key=value params")
    
    # ─────────────────────────────────────────────
    # scan: 디렉토리 스캔하여 레지스트리 생성
    # ─────────────────────────────────────────────
    scan_parser = subparsers.add_parser("scan", help="Scan experiment directories")
    scan_parser.add_argument("base_path", help="Base experiments directory")
    scan_parser.add_argument("--output", "-o", default="registry/experiments.yaml")
    
    args = parser.parse_args()
    
    if args.command == "list":
        registry = ExperimentRegistry()
        experiments = (registry.find_by_customer(args.customer) 
                      if args.customer else registry.list_all())
        for exp in experiments:
            print(exp)
    
    elif args.command == "run":
        params = {}
        if args.param:
            for p in args.param:
                k, v = p.split("=", 1)
                params[k] = v
        
        runner = GenericRunner(args.experiment, args.type, args.version)
        run_id = runner.run(params=params, script_override=args.script)
        print(f"Run completed: {run_id}")
    
    elif args.command == "scan":
        scan_and_generate_registry(args.base_path, args.output)


def scan_and_generate_registry(base_path: str, output_path: str):
    """디렉토리 스캔하여 레지스트리 초안 생성"""
    from pathlib import Path
    import yaml
    import re
    
    base = Path(base_path).expanduser()
    experiments = {}
    
    for exp_dir in sorted(base.iterdir()):
        if not exp_dir.is_dir():
            continue
        
        name = exp_dir.name
        
        # 디렉토리명 파싱 (예: donghwa-entec-ppe → customer=donghwa-entec, task=ppe)
        parts = name.rsplit("-", 1)
        if len(parts) == 2:
            customer_region, task_type = parts
        else:
            customer_region, task_type = name, "unknown"
        
        # 파일 패턴 탐지
        py_files = list(exp_dir.glob("*.py"))
        entry_points = {}
        
        for py_file in py_files:
            fname = py_file.name.lower()
            if any(k in fname for k in ["train", "학습"]):
                entry_points.setdefault("train", {})["default"] = py_file.name
            elif any(k in fname for k in ["infer", "eval", "bench", "test"]):
                entry_points.setdefault("infer", {})["default"] = py_file.name
            elif any(k in fname for k in ["quant", "export", "openvino", "tensorrt"]):
                entry_points.setdefault("quant", {})["default"] = py_file.name
        
        experiments[name] = {
            "path": str(exp_dir),
            "site": {
                "customer": customer_region,
                "region": "unknown",  # 수동 입력 필요
                "plant": "plant1",
                "zone": "full",
            },
            "task": {
                "type": task_type,
                "classes": [],  # 수동 입력 필요
            },
            "entry_points": entry_points,
        }
    
    with open(output_path, "w") as f:
        yaml.dump({"experiments": experiments}, f, allow_unicode=True, sort_keys=False)
    
    print(f"Generated registry with {len(experiments)} experiments: {output_path}")
    print("⚠️  Please review and fill in 'region', 'zone', 'classes' manually")


if __name__ == "__main__":
    main()
사용 예시
bash
# 1. 기존 디렉토리 스캔하여 레지스트리 초안 생성
python -m guardianflow.cli scan ~/experiments/ -o registry/experiments.yaml

# 2. 등록된 실험 목록 확인
python -m guardianflow.cli list
python -m guardianflow.cli list --customer donghwa-entec

# 3. 학습 실행
python -m guardianflow.cli run donghwa-entec-ppe \
    --type train \
    --version v3 \
    -p epochs=100 \
    -p batch_size=16 \
    -p imgsz=1280

# 4. 특정 스크립트로 실행 (기본값 오버라이드)
python -m guardianflow.cli run donghwa-entec-ppe \
    --type train \
    --version v3 \
    --script train_yolov11_custom.py

# 5. 양자화 실행
python -m guardianflow.cli run donghwa-entec-ppe \
    --type quant \
    --version v3 \
    -p weights_path=weights/best.pt \
    -p backend=openvino
점진적 마이그레이션 계획
단계	작업	기존 코드 영향
1. 스캔	cli scan으로 레지스트리 초안 생성	없음
2. 메타데이터	experiments.yaml 수동 보완	없음
3. 래핑 실행	cli run으로 MLflow 기록 시작	없음
4. 인터페이스 표준화	점진적으로 BaseTrainer 상속 구조 적용	최소 수정
5. 통합	검증된 실험만 GuardianFlow 코어로 이동	선택적
핵심: 30개+ 기존 디렉토리를 즉시 수정하지 않고, 외부에서 메타데이터만 추가하여 MLflow 추적 시작.

학습, 양자화 수행 이후 모델이 mlflow model registry에 등록되는 로직이 현재 있는지

30 Jan
현재 상태 분석
현재까지 설계한 코드에서 Model Registry 등록 로직이 부분적으로만 존재.

구성요소	상태	위치
시나리오 → 모델명 생성	✅ 있음	BaseScenario.get_registered_model_name()
Alias 전략 정의	✅ 있음	Notion 문서 (candidate/champion/rollback)
실제 등록 코드	❌ 없음	Runner에 미구현
Alias 관리 코드	❌ 없음	미구현
Model Registry 등록 로직 추가
python
# scenario/model_registry.py
"""
MLflow Model Registry 등록 및 Alias 관리
"""
import mlflow
from mlflow.tracking import MlflowClient
from dataclasses import dataclass
from typing import Literal
from enum import Enum


class ModelAlias(str, Enum):
    CANDIDATE = "candidate"
    CHAMPION = "champion"
    ROLLBACK = "rollback"


@dataclass
class RegisteredModelInfo:
    name: str
    version: str
    run_id: str
    aliases: list[str]


class ModelRegistryManager:
    """MLflow Model Registry 관리"""
    
    def __init__(self, tracking_uri: str = None):
        if tracking_uri:
            mlflow.set_tracking_uri(tracking_uri)
        self.client = MlflowClient()
    
    # ─────────────────────────────────────────────
    # 모델 등록
    # ─────────────────────────────────────────────
    
    def register_model(
        self,
        run_id: str,
        artifact_path: str,
        model_name: str,
        alias: ModelAlias = ModelAlias.CANDIDATE,
        tags: dict = None,
    ) -> RegisteredModelInfo:
        """
        학습/양자화 결과 모델을 Registry에 등록
        
        Args:
            run_id: MLflow run ID
            artifact_path: run 내 모델 artifact 경로 (e.g., "weights/best.pt")
            model_name: 등록할 모델명 (e.g., "donghwa-entec/busan-plant1/welding/ppe")
            alias: 초기 alias (기본: candidate)
            tags: 모델 버전 태그
        """
        model_uri = f"runs:/{run_id}/{artifact_path}"
        
        # 모델 등록 (버전 자동 증가)
        mv = mlflow.register_model(model_uri, model_name)
        
        # 태그 추가
        if tags:
            for k, v in tags.items():
                self.client.set_model_version_tag(model_name, mv.version, k, v)
        
        # Alias 설정
        self.client.set_registered_model_alias(model_name, alias.value, mv.version)
        
        return RegisteredModelInfo(
            name=model_name,
            version=mv.version,
            run_id=run_id,
            aliases=[alias.value],
        )
    
    # ─────────────────────────────────────────────
    # Alias 관리 (승격/롤백)
    # ─────────────────────────────────────────────
    
    def promote_to_champion(self, model_name: str, version: str = None) -> dict:
        """
        candidate → champion 승격
        기존 champion → rollback 이동
        
        Args:
            model_name: 등록된 모델명
            version: 승격할 버전 (None이면 현재 candidate)
        """
        # 승격 대상 버전 결정
        if version is None:
            version = self._get_version_by_alias(model_name, ModelAlias.CANDIDATE)
            if not version:
                raise ValueError(f"No candidate found for {model_name}")
        
        # 현재 champion을 rollback으로
        current_champion = self._get_version_by_alias(model_name, ModelAlias.CHAMPION)
        if current_champion:
            self.client.set_registered_model_alias(
                model_name, ModelAlias.ROLLBACK.value, current_champion
            )
        
        # candidate를 champion으로
        self.client.set_registered_model_alias(
            model_name, ModelAlias.CHAMPION.value, version
        )
        
        # candidate alias 제거
        try:
            self.client.delete_registered_model_alias(model_name, ModelAlias.CANDIDATE.value)
        except:
            pass  # alias가 없을 수 있음
        
        return {
            "promoted_version": version,
            "previous_champion": current_champion,
            "model_name": model_name,
        }
    
    def rollback(self, model_name: str) -> dict:
        """
        champion ← rollback 롤백
        """
        rollback_version = self._get_version_by_alias(model_name, ModelAlias.ROLLBACK)
        if not rollback_version:
            raise ValueError(f"No rollback version for {model_name}")
        
        current_champion = self._get_version_by_alias(model_name, ModelAlias.CHAMPION)
        
        # rollback → champion
        self.client.set_registered_model_alias(
            model_name, ModelAlias.CHAMPION.value, rollback_version
        )
        
        # 기존 champion → candidate (재평가용)
        if current_champion:
            self.client.set_registered_model_alias(
                model_name, ModelAlias.CANDIDATE.value, current_champion
            )
        
        return {
            "rolled_back_to": rollback_version,
            "demoted_version": current_champion,
        }
    
    # ─────────────────────────────────────────────
    # 조회
    # ─────────────────────────────────────────────
    
    def get_champion(self, model_name: str) -> str | None:
        """현재 champion 버전 조회"""
        return self._get_version_by_alias(model_name, ModelAlias.CHAMPION)
    
    def get_model_uri(self, model_name: str, alias: ModelAlias = ModelAlias.CHAMPION) -> str:
        """배포용 모델 URI 반환"""
        return f"models:/{model_name}@{alias.value}"
    
    def list_versions(self, model_name: str) -> list[dict]:
        """모델의 모든 버전 조회"""
        versions = self.client.search_model_versions(f"name='{model_name}'")
        return [
            {
                "version": v.version,
                "run_id": v.run_id,
                "aliases": v.aliases,
                "tags": v.tags,
                "creation_timestamp": v.creation_timestamp,
            }
            for v in versions
        ]
    
    def _get_version_by_alias(self, model_name: str, alias: ModelAlias) -> str | None:
        """alias로 버전 조회"""
        try:
            mv = self.client.get_model_version_by_alias(model_name, alias.value)
            return mv.version
        except:
            return None
Runner에 등록 로직 통합
python
# scenario/runner.py (Model Registry 통합)
import mlflow
from .model_registry import ModelRegistryManager, ModelAlias


class TrainingRunner(BaseRunner):
    def __init__(self, scenario, tracking_uri: str = None):
        super().__init__(scenario, tracking_uri)
        self.registry = ModelRegistryManager(tracking_uri)
    
    def _execute(self):
        s = self.scenario
        
        # ... 학습 로직 ...
        
        # 최종 모델 artifact 저장
        best_weights_path = "weights/best.pt"
        mlflow.log_artifact(trainer.get_best_model_path(), "weights")
        
        # ─────────────────────────────────────────────
        # Model Registry 등록
        # ─────────────────────────────────────────────
        run_id = mlflow.active_run().info.run_id
        model_name = s.get_registered_model_name()  # e.g., "donghwa-entec/busan-plant1/welding/ppe"
        
        model_info = self.registry.register_model(
            run_id=run_id,
            artifact_path=best_weights_path,
            model_name=model_name,
            alias=ModelAlias.CANDIDATE,
            tags={
                "architecture": s.model.architecture,
                "input_size": str(s.model.input_size),
                "task": s.task.type,
                "exp_version": s.version,
            },
        )
        
        mlflow.set_tag("registered_model_name", model_info.name)
        mlflow.set_tag("registered_model_version", model_info.version)
        
        print(f"✅ Model registered: {model_info.name} v{model_info.version} @candidate")


class QuantizationRunner(BaseRunner):
    def __init__(self, scenario, tracking_uri: str = None):
        super().__init__(scenario, tracking_uri)
        self.registry = ModelRegistryManager(tracking_uri)
    
    def _execute(self):
        s = self.scenario
        
        # ... 양자화 로직 ...
        
        # 양자화 모델 artifact 저장
        quantized_artifact_path = f"weights/model.{s.quantization.backend}"
        mlflow.log_artifact(quantized_model_path, "weights")
        
        # ─────────────────────────────────────────────
        # 양자화 모델 별도 등록 (variant suffix 포함)
        # ─────────────────────────────────────────────
        run_id = mlflow.active_run().info.run_id
        
        # e.g., "donghwa-entec/busan-plant1/welding/ppe__openvino_int8"
        model_name = s.get_registered_model_name()  # 자동으로 variant 포함
        
        model_info = self.registry.register_model(
            run_id=run_id,
            artifact_path=quantized_artifact_path,
            model_name=model_name,
            alias=ModelAlias.CANDIDATE,
            tags={
                "source_run_id": s.source_run_id,
                "quant_backend": s.quantization.backend,
                "quant_method": s.quantization.method,
                "precision": s.environment.precision,
            },
        )
        
        print(f"✅ Quantized model registered: {model_info.name} v{model_info.version} @candidate")
GenericRunner에도 통합
python
# pipelines/generic_runner.py (Model Registry 추가)

class GenericRunner:
    def __init__(self, experiment_name: str, exp_type: str, version: str = "v1"):
        self.registry_manager = ModelRegistryManager()
        # ... 기존 코드 ...
    
    def run(self, params: dict = None, script_override: str = None, 
            register_model: bool = True) -> str:
        """실험 실행 및 MLflow 기록"""
        
        # ... 기존 실행 로직 ...
        
        with mlflow.start_run(run_name=run_name) as run:
            # ... 실행 및 artifact 수집 ...
            
            # 모델 등록 (학습/양자화인 경우)
            if register_model and self.exp_type in ["train", "quant"]:
                self._register_model_if_exists(run.info.run_id)
            
            return run.info.run_id
    
    def _register_model_if_exists(self, run_id: str):
        """weights 폴더에서 모델 찾아서 등록"""
        weights_patterns = ["best.pt", "*.pt", "*.onnx", "*.engine", "*.xml"]
        
        for pattern in weights_patterns:
            matches = list(self.entry.path.rglob(f"**/{pattern}"))
            recent = [m for m in matches if self._is_recent(m)]
            
            if recent:
                model_path = recent[0]
                
                # 모델명 생성
                variant = self._detect_variant(model_path)
                model_name = self._build_model_name(variant)
                
                # artifact로 먼저 로깅 (이미 안됐다면)
                artifact_path = f"weights/{model_path.name}"
                
                self.registry_manager.register_model(
                    run_id=run_id,
                    artifact_path=artifact_path,
                    model_name=model_name,
                    alias=ModelAlias.CANDIDATE,
                )
                print(f"✅ Auto-registered: {model_name}")
                break
    
    def _detect_variant(self, model_path: Path) -> str:
        """파일 확장자/이름으로 variant 감지"""
        name = model_path.name.lower()
        suffix = model_path.suffix
        
        if suffix == ".engine":
            return "tensorrt_fp16"  # 또는 int8
        elif suffix == ".xml":
            return "openvino_int8"
        elif suffix == ".onnx":
            return "onnx"
        else:
            return "fp32"
    
    def _build_model_name(self, variant: str) -> str:
        """레지스트리 기반 모델명 생성"""
        base = f"{self.entry.site.customer}/{self.entry.site.region}-{self.entry.site.plant}/{self.entry.site.zone}/{self.entry.task.type}"
        
        if variant != "fp32":
            return f"{base}__{variant}"
        return base
CLI 확장 (승격/롤백)
python
# cli.py에 추가

def main():
    # ... 기존 subparsers ...
    
    # ─────────────────────────────────────────────
    # promote: candidate → champion 승격
    # ─────────────────────────────────────────────
    promote_parser = subparsers.add_parser("promote", help="Promote candidate to champion")
    promote_parser.add_argument("model_name", help="Registered model name")
    promote_parser.add_argument("--version", "-v", help="Specific version (default: current candidate)")
    
    # ─────────────────────────────────────────────
    # rollback: champion ← rollback
    # ─────────────────────────────────────────────
    rollback_parser = subparsers.add_parser("rollback", help="Rollback to previous champion")
    rollback_parser.add_argument("model_name", help="Registered model name")
    
    # ─────────────────────────────────────────────
    # models: 모델 목록/상태 조회
    # ─────────────────────────────────────────────
    models_parser = subparsers.add_parser("models", help="List registered models")
    models_parser.add_argument("--name", help="Filter by model name pattern")
    models_parser.add_argument("--customer", help="Filter by customer")
    
    args = parser.parse_args()
    
    # ... 기존 command 처리 ...
    
    if args.command == "promote":
        manager = ModelRegistryManager()
        result = manager.promote_to_champion(args.model_name, args.version)
        print(f"✅ Promoted v{result['promoted_version']} to champion")
        if result['previous_champion']:
            print(f"   Previous champion v{result['previous_champion']} → rollback")
    
    elif args.command == "rollback":
        manager = ModelRegistryManager()
        result = manager.rollback(args.model_name)
        print(f"✅ Rolled back to v{result['rolled_back_to']}")
    
    elif args.command == "models":
        manager = ModelRegistryManager()
        # 모델 목록 출력 로직
        ...
사용 예시
bash
# 1. 학습 실행 → 자동으로 @candidate 등록
python -m guardianflow.cli run donghwa-entec-ppe --type train --version v3

# 2. 벤치마크 테스트 (candidate 모델로)
python -m guardianflow.cli run donghwa-entec-ppe --type infer --version v3

# 3. 검증 후 승격
python -m guardianflow.cli promote "donghwa-entec/busan-plant1/welding/ppe"

# 4. 문제 발생 시 롤백
python -m guardianflow.cli rollback "donghwa-entec/busan-plant1/welding/ppe"

# 5. 양자화 실행 → 별도 모델로 등록
python -m guardianflow.cli run donghwa-entec-ppe --type quant --version v3
# → donghwa-entec/busan-plant1/welding/ppe__openvino_int8 @candidate

# 6. 양자화 모델 승격
python -m guardianflow.cli promote "donghwa-entec/busan-plant1/welding/ppe__openvino_int8"
```

---

## Model Registry 구조 (최종)
```
MLflow Model Registry
├── donghwa-entec/busan-plant1/welding/ppe
│   ├── v1 (tags: architecture=yolov11s)
│   ├── v2 @rollback
│   └── v3 @champion
│
├── donghwa-entec/busan-plant1/welding/ppe__openvino_int8
│   ├── v1
│   └── v2 @champion
│
├── donghwa-entec/busan-plant1/welding/ppe__tensorrt_fp16
│   └── v1 @candidate
│
└── sampyo/gimhae-plant1/bct/ppe
    └── v1 @champion
실험별 케이스를 더 고려해본다면? 단독모델, 2개 이상 비교, 단일 모델 여러 파라미터 수정, 비양자화 vs 양자화 등

실험별 케이스를 어떻게 더 쪼갤 수 있는지, 쪼개야 하는지

30 Jan
실험 케이스 분류 체계
현재 3가지(train/infer/quant)는 실험 타입. 여기에 실험 목적/설계에 따른 분류 추가 필요.

실험 케이스 매트릭스
실험 목적	모델 수	변수	예시	MLflow 구조
Baseline	1	없음	최초 학습	Single Run
Hyperparameter Sweep	1	N개 파라미터 조합	lr, batch_size 그리드	Parent + Child Runs
Architecture Comparison	N	모델 아키텍처	yolov11n vs s vs m	Parallel Runs (같은 Experiment)
Ablation Study	1	구성요소 on/off	augmentation 효과	Parent + Child Runs
A/B Benchmark	2	없음	v2 vs v3 성능 비교	Comparison Run
Quantization Comparison	1	backend/precision	fp32 vs int8 vs fp16	Parent + Child Runs
Cross-site Generalization	1	테스트 데이터셋	A사이트 모델 → B사이트	Multi-dataset Run
실험 설계 타입 정의
python
# scenario/experiment_design.py
from dataclasses import dataclass, field
from typing import Literal
from enum import Enum
from itertools import product


class ExperimentDesign(str, Enum):
    """실험 설계 타입"""
    SINGLE = "single"                    # 단일 실행
    SWEEP = "sweep"                      # 파라미터 스윕
    COMPARISON = "comparison"            # 모델/버전 비교
    ABLATION = "ablation"                # 구성요소 제거 실험
    QUANTIZATION_COMPARE = "quant_compare"  # 양자화 비교


@dataclass
class SweepConfig:
    """하이퍼파라미터 스윕 설정"""
    parameters: dict[str, list]  # {"lr": [0.001, 0.01], "batch_size": [8, 16, 32]}
    strategy: Literal["grid", "random", "bayesian"] = "grid"
    max_runs: int | None = None  # random/bayesian용
    
    def generate_combinations(self) -> list[dict]:
        """파라미터 조합 생성"""
        if self.strategy == "grid":
            keys = self.parameters.keys()
            values = self.parameters.values()
            return [dict(zip(keys, v)) for v in product(*values)]
        elif self.strategy == "random":
            import random
            combinations = self.generate_combinations_grid()
            random.shuffle(combinations)
            return combinations[:self.max_runs] if self.max_runs else combinations
        # bayesian은 별도 구현 필요 (optuna 연동)
        return []
    
    def generate_combinations_grid(self) -> list[dict]:
        keys = self.parameters.keys()
        values = self.parameters.values()
        return [dict(zip(keys, v)) for v in product(*values)]


@dataclass
class ComparisonConfig:
    """비교 실험 설정"""
    # 모델 비교
    models: list[str] = None           # ["yolov11n", "yolov11s", "yolov11m"]
    
    # 버전 비교
    run_ids: list[str] = None          # 기존 run들 비교
    model_versions: list[str] = None   # registry 버전 비교
    
    # 양자화 비교
    quantization_variants: list[dict] = None  # [{"backend": "openvino", "precision": "int8"}, ...]
    
    # 데이터셋 비교 (일반화 테스트)
    datasets: list[str] = None         # ["site_a", "site_b", "site_c"]


@dataclass
class AblationConfig:
    """Ablation Study 설정"""
    baseline: dict                     # 기본 설정
    components: dict[str, list]        # 제거/변경할 구성요소
    # 예: {"augmentation": [True, False], "pretrained": [True, False]}
확장된 시나리오 스키마
python
# scenario/schema.py (확장)
from .experiment_design import ExperimentDesign, SweepConfig, ComparisonConfig, AblationConfig


@dataclass
class BaseScenario(ABC):
    """모든 시나리오 공통"""
    name: str
    version: str
    experiment_type: Literal["train", "infer", "quant"]
    
    # 실험 설계 (신규)
    design: ExperimentDesign = ExperimentDesign.SINGLE
    sweep_config: SweepConfig | None = None
    comparison_config: ComparisonConfig | None = None
    ablation_config: AblationConfig | None = None
    
    # 기존 필드들...
    site: SiteConfig = None
    task: TaskConfig = None
    model: ModelConfig = None
    environment: EnvironmentConfig = None
    dataset: DatasetConfig = None


@dataclass
class TrainingScenario(BaseScenario):
    experiment_type: str = field(default="train", init=False)
    training: TrainingParams = None
    
    # Sweep 지원
    def with_sweep(self, params: dict[str, list], strategy: str = "grid") -> "TrainingScenario":
        """스윕 설정 추가"""
        self.design = ExperimentDesign.SWEEP
        self.sweep_config = SweepConfig(parameters=params, strategy=strategy)
        return self


@dataclass
class InferenceBenchmarkScenario(BaseScenario):
    experiment_type: str = field(default="infer", init=False)
    benchmark: InferenceBenchmarkParams = None
    source_run_id: str | None = None
    
    # 비교 지원
    def with_comparison(self, **kwargs) -> "InferenceBenchmarkScenario":
        self.design = ExperimentDesign.COMPARISON
        self.comparison_config = ComparisonConfig(**kwargs)
        return self


@dataclass  
class QuantizationScenario(BaseScenario):
    experiment_type: str = field(default="quant", init=False)
    quantization: QuantizationParams = None
    source_run_id: str = None
    
    # 양자화 비교 지원
    def with_quant_comparison(self, variants: list[dict]) -> "QuantizationScenario":
        self.design = ExperimentDesign.QUANTIZATION_COMPARE
        self.comparison_config = ComparisonConfig(quantization_variants=variants)
        return self
실험 설계별 Runner
python
# scenario/runners/sweep_runner.py
import mlflow
from concurrent.futures import ThreadPoolExecutor, as_completed


class SweepRunner:
    """하이퍼파라미터 스윕 실행기"""
    
    def __init__(self, scenario: TrainingScenario, base_runner_cls, tracking_uri: str = None):
        self.scenario = scenario
        self.base_runner_cls = base_runner_cls
        self.tracking_uri = tracking_uri
    
    def run(self, parallel: int = 1) -> dict:
        """스윕 실행"""
        if not self.scenario.sweep_config:
            raise ValueError("No sweep config defined")
        
        combinations = self.scenario.sweep_config.generate_combinations()
        
        mlflow.set_experiment(self.scenario.get_experiment_name())
        
        # Parent run 생성
        with mlflow.start_run(run_name=f"{self.scenario.name}-sweep") as parent_run:
            mlflow.set_tag("experiment.design", "sweep")
            mlflow.set_tag("sweep.strategy", self.scenario.sweep_config.strategy)
            mlflow.set_tag("sweep.total_runs", len(combinations))
            mlflow.log_params({"sweep.parameters": str(self.scenario.sweep_config.parameters)})
            
            results = []
            
            if parallel > 1:
                results = self._run_parallel(combinations, parent_run.info.run_id, parallel)
            else:
                results = self._run_sequential(combinations, parent_run.info.run_id)
            
            # 최적 결과 기록
            best = max(results, key=lambda x: x.get("mAP50", 0))
            mlflow.log_metrics({
                "best.mAP50": best["mAP50"],
                "best.mAP50_95": best.get("mAP50_95", 0),
            })
            mlflow.set_tag("best.run_id", best["run_id"])
            mlflow.set_tag("best.params", str(best["params"]))
            
            return {
                "parent_run_id": parent_run.info.run_id,
                "child_runs": results,
                "best": best,
            }
    
    def _run_sequential(self, combinations: list[dict], parent_run_id: str) -> list[dict]:
        results = []
        for i, params in enumerate(combinations):
            result = self._run_single(params, parent_run_id, i)
            results.append(result)
        return results
    
    def _run_parallel(self, combinations: list[dict], parent_run_id: str, max_workers: int) -> list[dict]:
        results = []
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(self._run_single, params, parent_run_id, i): i
                for i, params in enumerate(combinations)
            }
            for future in as_completed(futures):
                results.append(future.result())
        return results
    
    def _run_single(self, params: dict, parent_run_id: str, index: int) -> dict:
        """단일 조합 실행"""
        # 시나리오 복사 후 파라미터 오버라이드
        child_scenario = self._override_params(params)
        
        with mlflow.start_run(
            run_name=f"{self.scenario.name}-{index:03d}",
            nested=True,
            parent_run_id=parent_run_id,
        ) as child_run:
            mlflow.set_tag("sweep.index", index)
            mlflow.log_params(params)
            
            # 실제 학습 실행
            runner = self.base_runner_cls(child_scenario, self.tracking_uri)
            runner._execute()
            
            # 결과 수집
            metrics = mlflow.get_run(child_run.info.run_id).data.metrics
            
            return {
                "run_id": child_run.info.run_id,
                "params": params,
                "mAP50": metrics.get("det.mAP50", 0),
                "mAP50_95": metrics.get("det.mAP50_95", 0),
            }
    
    def _override_params(self, params: dict) -> TrainingScenario:
        """파라미터 오버라이드된 시나리오 생성"""
        import copy
        new_scenario = copy.deepcopy(self.scenario)
        
        # training params 오버라이드
        for k, v in params.items():
            if hasattr(new_scenario.training, k):
                setattr(new_scenario.training, k, v)
            elif hasattr(new_scenario.model, k):
                setattr(new_scenario.model, k, v)
        
        return new_scenario
python
# scenario/runners/comparison_runner.py
import mlflow
from dataclasses import asdict


class ComparisonRunner:
    """모델/버전 비교 실험 실행기"""
    
    def __init__(self, scenario: InferenceBenchmarkScenario, tracking_uri: str = None):
        self.scenario = scenario
        if tracking_uri:
            mlflow.set_tracking_uri(tracking_uri)
        self.client = mlflow.tracking.MlflowClient()
    
    def run(self) -> dict:
        config = self.scenario.comparison_config
        if not config:
            raise ValueError("No comparison config defined")
        
        mlflow.set_experiment(self.scenario.get_experiment_name())
        
        with mlflow.start_run(run_name=f"{self.scenario.name}-comparison") as parent_run:
            mlflow.set_tag("experiment.design", "comparison")
            
            results = []
            
            # 케이스 1: 아키텍처 비교
            if config.models:
                results = self._compare_architectures(config.models, parent_run.info.run_id)
            
            # 케이스 2: 기존 run 비교
            elif config.run_ids:
                results = self._compare_runs(config.run_ids)
            
            # 케이스 3: 양자화 variant 비교
            elif config.quantization_variants:
                results = self._compare_quantization(config.quantization_variants, parent_run.info.run_id)
            
            # 케이스 4: 데이터셋 일반화 테스트
            elif config.datasets:
                results = self._compare_datasets(config.datasets, parent_run.info.run_id)
            
            # 비교 결과 요약
            self._log_comparison_summary(results)
            
            return {
                "parent_run_id": parent_run.info.run_id,
                "results": results,
            }
    
    def _compare_architectures(self, models: list[str], parent_run_id: str) -> list[dict]:
        """아키텍처별 비교"""
        results = []
        for model_arch in models:
            with mlflow.start_run(
                run_name=f"arch-{model_arch}",
                nested=True,
                parent_run_id=parent_run_id,
            ) as run:
                mlflow.set_tag("comparison.variable", "architecture")
                mlflow.set_tag("comparison.value", model_arch)
                
                # 해당 아키텍처로 벤치마크 실행
                metrics = self._run_benchmark(model_arch)
                mlflow.log_metrics(metrics)
                
                results.append({
                    "run_id": run.info.run_id,
                    "architecture": model_arch,
                    **metrics,
                })
        return results
    
    def _compare_quantization(self, variants: list[dict], parent_run_id: str) -> list[dict]:
        """양자화 variant 비교 (fp32 vs int8 vs fp16 등)"""
        results = []
        
        for variant in variants:
            variant_name = f"{variant.get('backend', 'native')}_{variant.get('precision', 'fp32')}"
            
            with mlflow.start_run(
                run_name=f"quant-{variant_name}",
                nested=True,
                parent_run_id=parent_run_id,
            ) as run:
                mlflow.set_tag("comparison.variable", "quantization")
                mlflow.set_tag("comparison.value", variant_name)
                mlflow.log_params(variant)
                
                # 해당 variant로 벤치마크
                metrics = self._run_benchmark_with_variant(variant)
                mlflow.log_metrics(metrics)
                
                results.append({
                    "run_id": run.info.run_id,
                    "variant": variant_name,
                    **variant,
                    **metrics,
                })
        
        return results
    
    def _compare_datasets(self, datasets: list[str], parent_run_id: str) -> list[dict]:
        """Cross-site 일반화 테스트"""
        results = []
        
        for dataset_name in datasets:
            with mlflow.start_run(
                run_name=f"dataset-{dataset_name}",
                nested=True,
                parent_run_id=parent_run_id,
            ) as run:
                mlflow.set_tag("comparison.variable", "dataset")
                mlflow.set_tag("comparison.value", dataset_name)
                
                metrics = self._run_benchmark_on_dataset(dataset_name)
                mlflow.log_metrics(metrics)
                
                results.append({
                    "run_id": run.info.run_id,
                    "dataset": dataset_name,
                    **metrics,
                })
        
        return results
    
    def _compare_runs(self, run_ids: list[str]) -> list[dict]:
        """기존 run 결과 비교 (새 실행 없이 메트릭만 수집)"""
        results = []
        for run_id in run_ids:
            run = self.client.get_run(run_id)
            results.append({
                "run_id": run_id,
                "run_name": run.info.run_name,
                **run.data.metrics,
                **run.data.params,
            })
        
        # 비교표 artifact로 저장
        import pandas as pd
        df = pd.DataFrame(results)
        df.to_csv("/tmp/comparison.csv", index=False)
        mlflow.log_artifact("/tmp/comparison.csv")
        
        return results
    
    def _log_comparison_summary(self, results: list[dict]):
        """비교 결과 요약 로깅"""
        import pandas as pd
        
        df = pd.DataFrame(results)
        
        # 최고 성능 모델
        if "det.mAP50" in df.columns:
            best_idx = df["det.mAP50"].idxmax()
            best = df.loc[best_idx]
            mlflow.set_tag("comparison.best_run_id", best.get("run_id", ""))
            mlflow.log_metric("comparison.best_mAP50", best.get("det.mAP50", 0))
        
        # 비교표 저장
        df.to_csv("/tmp/comparison_summary.csv", index=False)
        mlflow.log_artifact("/tmp/comparison_summary.csv")
        
        # 차트 생성 (선택적)
        self._generate_comparison_chart(df)
    
    def _generate_comparison_chart(self, df: pd.DataFrame):
        """비교 차트 생성"""
        import matplotlib.pyplot as plt
        
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))
        
        # mAP 비교
        if "det.mAP50" in df.columns:
            labels = df.get("architecture", df.get("variant", df.index))
            axes[0].bar(labels, df["det.mAP50"])
            axes[0].set_title("mAP50 Comparison")
            axes[0].set_ylabel("mAP50")
        
        # Latency 비교
        if "sys.inference_time_ms" in df.columns:
            axes[1].bar(labels, df["sys.inference_time_ms"])
            axes[1].set_title("Inference Time Comparison")
            axes[1].set_ylabel("ms")
        
        plt.tight_layout()
        plt.savefig("/tmp/comparison_chart.png", dpi=150)
        mlflow.log_artifact("/tmp/comparison_chart.png")
        plt.close()
통합 팩토리
python
# scenario/factory.py
from .experiment_design import ExperimentDesign
from .runners.sweep_runner import SweepRunner
from .runners.comparison_runner import ComparisonRunner


def create_runner(scenario: BaseScenario, tracking_uri: str = None):
    """실험 설계에 따른 적절한 Runner 반환"""
    
    # 실험 설계별 분기
    if scenario.design == ExperimentDesign.SWEEP:
        base_runner = _get_base_runner(scenario.experiment_type)
        return SweepRunner(scenario, base_runner, tracking_uri)
    
    elif scenario.design in [ExperimentDesign.COMPARISON, ExperimentDesign.QUANTIZATION_COMPARE]:
        return ComparisonRunner(scenario, tracking_uri)
    
    elif scenario.design == ExperimentDesign.ABLATION:
        base_runner = _get_base_runner(scenario.experiment_type)
        return AblationRunner(scenario, base_runner, tracking_uri)
    
    else:  # SINGLE
        runner_cls = _get_base_runner(scenario.experiment_type)
        return runner_cls(scenario, tracking_uri)


def _get_base_runner(experiment_type: str):
    runners = {
        "train": TrainingRunner,
        "infer": InferenceBenchmarkRunner,
        "quant": QuantizationRunner,
    }
    return runners[experiment_type]
사용 예시
python
# ─────────────────────────────────────────────
# 1. 단일 학습 (기본)
# ─────────────────────────────────────────────
scenario = TrainingScenario(
    name="ppe-baseline",
    version="v1",
    site=SiteConfig(customer="donghwa-entec", region="busan", zone="welding"),
    task=TaskConfig(type="ppe"),
    model=ModelConfig(architecture="yolov11s"),
    training=TrainingParams(epochs=100),
)
runner = create_runner(scenario)
runner.run()


# ─────────────────────────────────────────────
# 2. 하이퍼파라미터 스윕
# ─────────────────────────────────────────────
scenario = TrainingScenario(
    name="ppe-hp-sweep",
    version="v1",
    site=SiteConfig(customer="donghwa-entec", region="busan", zone="welding"),
    task=TaskConfig(type="ppe"),
    model=ModelConfig(architecture="yolov11s"),
    training=TrainingParams(),
).with_sweep(
    params={
        "learning_rate": [0.001, 0.005, 0.01],
        "batch_size": [8, 16, 32],
        "optimizer": ["adam", "adamw", "sgd"],
    },
    strategy="grid",  # 27개 조합
)
runner = create_runner(scenario)
result = runner.run(parallel=4)  # 4개 동시 실행
print(f"Best params: {result['best']['params']}")


# ─────────────────────────────────────────────
# 3. 아키텍처 비교
# ─────────────────────────────────────────────
scenario = InferenceBenchmarkScenario(
    name="ppe-arch-compare",
    version="v1",
    site=SiteConfig(customer="donghwa-entec", region="busan", zone="welding"),
    task=TaskConfig(type="ppe"),
    model=ModelConfig(architecture="yolov11s"),  # base
    benchmark=InferenceBenchmarkParams(),
).with_comparison(
    models=["yolov11n", "yolov11s", "yolov11m", "yolov11l"],
)
runner = create_runner(scenario)
runner.run()


# ─────────────────────────────────────────────
# 4. 양자화 비교 (fp32 vs fp16 vs int8)
# ─────────────────────────────────────────────
scenario = InferenceBenchmarkScenario(
    name="ppe-quant-compare",
    version="v1",
    site=SiteConfig(customer="donghwa-entec", region="busan", zone="welding"),
    task=TaskConfig(type="ppe"),
    model=ModelConfig(architecture="yolov11s"),
    benchmark=InferenceBenchmarkParams(),
).with_comparison(
    quantization_variants=[
        {"backend": "native", "precision": "fp32"},
        {"backend": "native", "precision": "fp16"},
        {"backend": "openvino", "precision": "fp16"},
        {"backend": "openvino", "precision": "int8"},
        {"backend": "tensorrt", "precision": "fp16"},
        {"backend": "tensorrt", "precision": "int8"},
    ],
)
runner = create_runner(scenario)
runner.run()


# ─────────────────────────────────────────────
# 5. Cross-site 일반화 테스트
# ─────────────────────────────────────────────
scenario = InferenceBenchmarkScenario(
    name="ppe-generalization",
    version="v1",
    site=SiteConfig(customer="donghwa-entec", region="busan", zone="welding"),
    task=TaskConfig(type="ppe"),
    model=ModelConfig(architecture="yolov11s", weights_path="models://.../ppe@champion"),
    benchmark=InferenceBenchmarkParams(),
).with_comparison(
    datasets=["donghwa-entec-welding", "donghwa-entec-brazing", "sampyo-gimhae-bct"],
)
runner = create_runner(scenario)
runner.run()


# ─────────────────────────────────────────────
# 6. 기존 Run 비교 (메트릭 수집만)
# ─────────────────────────────────────────────
scenario = InferenceBenchmarkScenario(
    name="historical-compare",
    version="v1",
    site=SiteConfig(customer="donghwa-entec", region="busan", zone="welding"),
    task=TaskConfig(type="ppe"),
    model=ModelConfig(architecture="yolov11s"),
    benchmark=InferenceBenchmarkParams(),
).with_comparison(
    run_ids=["run_abc123", "run_def456", "run_ghi789"],
)
runner = create_runner(scenario)
runner.run()
```

---

## MLflow 구조 (Nested Runs)
```
Experiment: donghwa-entec/busan-plant1/welding
│
├── Run: ppe-hp-sweep (Parent)
│   ├── tags: experiment.design=sweep, sweep.total_runs=27
│   ├── metrics: best.mAP50=0.89
│   │
│   ├── Child Run: ppe-hp-sweep-000 (lr=0.001, bs=8)
│   │   └── metrics: det.mAP50=0.82
│   ├── Child Run: ppe-hp-sweep-001 (lr=0.001, bs=16)
│   │   └── metrics: det.mAP50=0.85
│   └── ... (27개)
│
├── Run: ppe-arch-compare (Parent)
│   ├── tags: experiment.design=comparison
│   │
│   ├── Child Run: arch-yolov11n
│   │   └── metrics: det.mAP50=0.78, sys.inference_time_ms=8.2
│   ├── Child Run: arch-yolov11s
│   │   └── metrics: det.mAP50=0.85, sys.inference_time_ms=12.5
│   └── ...
│
└── Run: ppe-quant-compare (Parent)
    ├── Child Run: quant-native_fp32
    ├── Child Run: quant-openvino_int8
    └── ...
실험 케이스 분류 결론
분류 기준	분리 여부	이유
실험 타입 (train/infer/quant)	✅ 분리	입출력, 파이프라인 완전히 다름
실험 설계 (single/sweep/comparison)	✅ 분리	실행 방식, MLflow 구조 다름
파라미터 변형	❌ 통합	Sweep 내 Child Run으로 처리
모델 변형	❌ 통합	Comparison 내 Child Run으로 처리
양자화 변형	❌ 통합	Comparison 내 Child Run으로 처리
핵심: 실험 타입 × 실험 설계 = 9가지 조합이 가능하나, Runner 상속/조합으로 처리. 과도한 분리는 복잡도만 증가.

수집된 메트릭지표가 로컬 seaweedfs에 저장되고, 대시보드 지표에 따라 고도화하는 법

30 Jan
핵심 구조
실험 실행 → MLflow → SeaweedFS (저장) → 대시보드 (시각화) → 인사이트 → 고도화
아키텍처
┌─────────────────────────────────────────────────────────────────┐
│                        GuardianFlow                              │
├─────────────────────────────────────────────────────────────────┤
│  Experiments    →    MLflow Server    →    SeaweedFS            │
│  (Runner)            (Tracking)            (Artifact Store)      │
│                           │                      │               │
│                           ▼                      ▼               │
│                      PostgreSQL             S3-compatible        │
│                      (metadata)             (artifacts/metrics)  │
└───────────────────────────┬─────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────────────┐
│                      Dashboard Layer                             │
├─────────────────────────────────────────────────────────────────┤
│  Grafana / Streamlit / Custom React                             │
│  - 실험 비교 대시보드                                             │
│  - 모델 성능 트렌드                                               │
│  - 시스템 리소스 모니터링                                          │
│  - 알람 및 자동화 트리거                                          │
└─────────────────────────────────────────────────────────────────┘
1. SeaweedFS + MLflow 연동 설정
yaml
# docker-compose.yml
version: '3.8'

services:
  # ─────────────────────────────────────────────
  # SeaweedFS (S3-compatible Object Storage)
  # ─────────────────────────────────────────────
  seaweedfs-master:
    image: chrislusf/seaweedfs:latest
    command: master -ip=seaweedfs-master -port=9333
    ports:
      - "9333:9333"
    volumes:
      - seaweedfs-master-data:/data

  seaweedfs-volume:
    image: chrislusf/seaweedfs:latest
    command: volume -mserver="seaweedfs-master:9333" -port=8080 -dir=/data
    depends_on:
      - seaweedfs-master
    volumes:
      - seaweedfs-volume-data:/data

  seaweedfs-filer:
    image: chrislusf/seaweedfs:latest
    command: filer -master="seaweedfs-master:9333" -port=8888 -s3 -s3.port=8333
    depends_on:
      - seaweedfs-master
      - seaweedfs-volume
    ports:
      - "8888:8888"   # Filer API
      - "8333:8333"   # S3 API
    volumes:
      - seaweedfs-filer-data:/data

  # ─────────────────────────────────────────────
  # MLflow Server
  # ─────────────────────────────────────────────
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: mlflow
      POSTGRES_USER: mlflow
      POSTGRES_PASSWORD: mlflow123
    volumes:
      - postgres-data:/var/lib/postgresql/data

  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.10.0
    depends_on:
      - postgres
      - seaweedfs-filer
    environment:
      AWS_ACCESS_KEY_ID: ""           # SeaweedFS는 빈 값 허용
      AWS_SECRET_ACCESS_KEY: ""
    command: >
      mlflow server
      --backend-store-uri postgresql://mlflow:mlflow123@postgres:5432/mlflow
      --artifacts-destination s3://mlflow-artifacts/
      --host 0.0.0.0
      --port 5000
    ports:
      - "5000:5000"
    volumes:
      - ./mlflow-config:/config

  # ─────────────────────────────────────────────
  # Grafana (대시보드)
  # ─────────────────────────────────────────────
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin123
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
      - ./grafana/dashboards:/var/lib/grafana/dashboards

volumes:
  seaweedfs-master-data:
  seaweedfs-volume-data:
  seaweedfs-filer-data:
  postgres-data:
  grafana-data:
python
# config/mlflow_config.py
"""MLflow + SeaweedFS 설정"""
import os

MLFLOW_CONFIG = {
    "tracking_uri": os.getenv("MLFLOW_TRACKING_URI", "http://localhost:5000"),
    "artifact_location": os.getenv("MLFLOW_ARTIFACT_ROOT", "s3://mlflow-artifacts/"),
    "s3_endpoint_url": os.getenv("MLFLOW_S3_ENDPOINT_URL", "http://localhost:8333"),
}

# 환경변수 설정 (boto3/mlflow가 SeaweedFS 사용하도록)
os.environ["MLFLOW_S3_ENDPOINT_URL"] = MLFLOW_CONFIG["s3_endpoint_url"]
os.environ["AWS_ACCESS_KEY_ID"] = ""
os.environ["AWS_SECRET_ACCESS_KEY"] = ""
2. 메트릭 수집 및 저장 확장
python
# scenario/metrics_store.py
"""
메트릭을 MLflow + 별도 시계열 DB에 이중 저장
대시보드 쿼리 최적화를 위해 분리 저장
"""
import json
import time
from dataclasses import dataclass, asdict
from typing import Optional
from pathlib import Path

import mlflow
from mlflow.tracking import MlflowClient


@dataclass
class MetricRecord:
    """대시보드용 메트릭 레코드"""
    timestamp: float
    run_id: str
    experiment_name: str
    run_name: str
    
    # 컨텍스트
    customer: str
    site: str
    zone: str
    task: str
    
    # 실험 정보
    experiment_type: str      # train, infer, quant
    experiment_design: str    # single, sweep, comparison
    model_architecture: str
    model_version: str
    
    # Detection 메트릭
    mAP50: Optional[float] = None
    mAP50_95: Optional[float] = None
    precision: Optional[float] = None
    recall: Optional[float] = None
    
    # MOT 메트릭
    mota: Optional[float] = None
    idf1: Optional[float] = None
    hota: Optional[float] = None
    
    # System 메트릭
    inference_time_ms: Optional[float] = None
    fps: Optional[float] = None
    gpu_memory_mb: Optional[float] = None
    gpu_utilization: Optional[float] = None
    model_size_mb: Optional[float] = None
    
    # 양자화 비교
    accuracy_drop: Optional[float] = None
    speedup_ratio: Optional[float] = None
    compression_ratio: Optional[float] = None


class MetricsStore:
    """메트릭 저장소 - MLflow + 로컬 JSON/Parquet"""
    
    def __init__(self, 
                 mlflow_uri: str = None,
                 local_store_path: str = "./metrics_store"):
        self.client = MlflowClient(mlflow_uri)
        self.local_path = Path(local_store_path)
        self.local_path.mkdir(parents=True, exist_ok=True)
    
    def save_metrics(self, record: MetricRecord):
        """메트릭 저장 (MLflow + 로컬)"""
        # 1. MLflow에 저장 (이미 Runner에서 수행)
        # 2. 로컬 JSON Lines로 저장 (대시보드 쿼리용)
        self._append_to_local(record)
    
    def _append_to_local(self, record: MetricRecord):
        """JSON Lines 형식으로 append"""
        date_str = time.strftime("%Y-%m-%d")
        file_path = self.local_path / f"metrics_{date_str}.jsonl"
        
        with open(file_path, "a") as f:
            f.write(json.dumps(asdict(record)) + "\n")
    
    def export_to_parquet(self, start_date: str = None, end_date: str = None):
        """대시보드용 Parquet 변환"""
        import pandas as pd
        
        records = []
        for jsonl_file in self.local_path.glob("metrics_*.jsonl"):
            with open(jsonl_file) as f:
                for line in f:
                    records.append(json.loads(line))
        
        df = pd.DataFrame(records)
        
        # 날짜 필터
        if start_date:
            df = df[df["timestamp"] >= pd.Timestamp(start_date).timestamp()]
        if end_date:
            df = df[df["timestamp"] <= pd.Timestamp(end_date).timestamp()]
        
        # Parquet 저장 (Grafana/BI 도구 연동용)
        output_path = self.local_path / "metrics_aggregated.parquet"
        df.to_parquet(output_path, index=False)
        
        return output_path
    
    def query_metrics(self, 
                      customer: str = None,
                      task: str = None,
                      experiment_type: str = None,
                      start_time: float = None,
                      end_time: float = None) -> list[dict]:
        """메트릭 쿼리"""
        import pandas as pd
        
        records = []
        for jsonl_file in self.local_path.glob("metrics_*.jsonl"):
            with open(jsonl_file) as f:
                for line in f:
                    records.append(json.loads(line))
        
        df = pd.DataFrame(records)
        
        if customer:
            df = df[df["customer"] == customer]
        if task:
            df = df[df["task"] == task]
        if experiment_type:
            df = df[df["experiment_type"] == experiment_type]
        if start_time:
            df = df[df["timestamp"] >= start_time]
        if end_time:
            df = df[df["timestamp"] <= end_time]
        
        return df.to_dict("records")
3. Runner에 메트릭 저장 통합
python
# scenario/runner.py (MetricsStore 통합)
from .metrics_store import MetricsStore, MetricRecord


class BaseRunner(ABC):
    def __init__(self, scenario, tracking_uri: str = None):
        self.scenario = scenario
        self.metrics_store = MetricsStore(mlflow_uri=tracking_uri)
        # ...
    
    def _save_to_dashboard_store(self, run_id: str, metrics: dict):
        """대시보드용 메트릭 저장"""
        s = self.scenario
        
        record = MetricRecord(
            timestamp=time.time(),
            run_id=run_id,
            experiment_name=s.get_experiment_name(),
            run_name=s.get_run_name(),
            
            customer=s.site.customer,
            site=f"{s.site.region}-{s.site.plant}",
            zone=s.site.zone,
            task=s.task.type,
            
            experiment_type=s.experiment_type,
            experiment_design=s.design.value,
            model_architecture=s.model.architecture,
            model_version=s.version,
            
            # Detection
            mAP50=metrics.get("det.mAP50"),
            mAP50_95=metrics.get("det.mAP50_95"),
            precision=metrics.get("det.precision"),
            recall=metrics.get("det.recall"),
            
            # MOT
            mota=metrics.get("mot.mota"),
            idf1=metrics.get("mot.idf1"),
            hota=metrics.get("mot.hota"),
            
            # System
            inference_time_ms=metrics.get("sys.inference_time_ms"),
            fps=metrics.get("sys.fps"),
            gpu_memory_mb=metrics.get("sys.gpu_memory_used_mb"),
            gpu_utilization=metrics.get("sys.gpu_utilization_pct"),
            model_size_mb=metrics.get("sys.model_size_mb"),
        )
        
        self.metrics_store.save_metrics(record)


class TrainingRunner(BaseRunner):
    def _execute(self):
        # ... 학습 로직 ...
        
        # 최종 메트릭
        final_metrics = {
            "det.mAP50": ...,
            "det.mAP50_95": ...,
            "sys.gpu_memory_used_mb": ...,
        }
        
        mlflow.log_metrics(final_metrics)
        
        # 대시보드 저장소에도 저장
        self._save_to_dashboard_store(
            mlflow.active_run().info.run_id,
            final_metrics
        )
4. 대시보드 API 서버
python
# dashboard/api.py
"""대시보드용 REST API"""
from fastapi import FastAPI, Query
from typing import Optional
import pandas as pd

from scenario.metrics_store import MetricsStore

app = FastAPI(title="GuardianFlow Dashboard API")
store = MetricsStore()


@app.get("/api/metrics/summary")
def get_metrics_summary(
    customer: Optional[str] = None,
    days: int = Query(default=30, ge=1, le=365),
):
    """고객별 메트릭 요약"""
    import time
    start_time = time.time() - (days * 86400)
    
    records = store.query_metrics(customer=customer, start_time=start_time)
    df = pd.DataFrame(records)
    
    if df.empty:
        return {"summary": {}}
    
    summary = {
        "total_experiments": len(df),
        "by_type": df.groupby("experiment_type").size().to_dict(),
        "by_task": df.groupby("task").size().to_dict(),
        "avg_mAP50": df["mAP50"].mean(),
        "best_mAP50": df["mAP50"].max(),
        "avg_inference_time_ms": df["inference_time_ms"].mean(),
    }
    
    return {"summary": summary}


@app.get("/api/metrics/trend")
def get_metrics_trend(
    customer: str,
    task: str,
    metric: str = "mAP50",
    days: int = 90,
):
    """메트릭 트렌드 (시계열)"""
    import time
    start_time = time.time() - (days * 86400)
    
    records = store.query_metrics(
        customer=customer, 
        task=task, 
        start_time=start_time
    )
    df = pd.DataFrame(records)
    
    if df.empty:
        return {"trend": []}
    
    df["date"] = pd.to_datetime(df["timestamp"], unit="s").dt.date
    trend = df.groupby("date")[metric].agg(["mean", "max", "min"]).reset_index()
    
    return {
        "trend": trend.to_dict("records"),
        "metric": metric,
    }


@app.get("/api/metrics/comparison")
def get_model_comparison(
    customer: str,
    task: str,
    architectures: str = Query(..., description="Comma-separated architectures"),
):
    """아키텍처별 성능 비교"""
    arch_list = [a.strip() for a in architectures.split(",")]
    
    records = store.query_metrics(customer=customer, task=task)
    df = pd.DataFrame(records)
    df = df[df["model_architecture"].isin(arch_list)]
    
    comparison = df.groupby("model_architecture").agg({
        "mAP50": ["mean", "max", "std"],
        "inference_time_ms": ["mean", "min"],
        "model_size_mb": "first",
    }).round(4)
    
    return {"comparison": comparison.to_dict()}


@app.get("/api/metrics/alerts")
def get_performance_alerts(
    threshold_mAP50: float = 0.7,
    threshold_latency_ms: float = 50,
):
    """성능 저하 알림"""
    import time
    recent_time = time.time() - 86400  # 최근 24시간
    
    records = store.query_metrics(start_time=recent_time)
    df = pd.DataFrame(records)
    
    alerts = []
    
    # mAP 저하 알림
    low_map = df[df["mAP50"] < threshold_mAP50]
    for _, row in low_map.iterrows():
        alerts.append({
            "type": "LOW_ACCURACY",
            "severity": "warning",
            "message": f"{row['customer']}/{row['task']}: mAP50={row['mAP50']:.3f} < {threshold_mAP50}",
            "run_id": row["run_id"],
        })
    
    # 지연시간 알림
    high_latency = df[df["inference_time_ms"] > threshold_latency_ms]
    for _, row in high_latency.iterrows():
        alerts.append({
            "type": "HIGH_LATENCY",
            "severity": "warning",
            "message": f"{row['customer']}/{row['task']}: {row['inference_time_ms']:.1f}ms > {threshold_latency_ms}ms",
            "run_id": row["run_id"],
        })
    
    return {"alerts": alerts}
5. Grafana 대시보드 설정
json
// grafana/dashboards/guardianflow_overview.json
{
  "title": "GuardianFlow MLOps Overview",
  "panels": [
    {
      "title": "mAP50 Trend by Customer",
      "type": "timeseries",
      "datasource": "JSON API",
      "targets": [
        {
          "url": "http://dashboard-api:8000/api/metrics/trend",
          "params": {
            "customer": "${customer}",
            "task": "${task}",
            "metric": "mAP50"
          }
        }
      ]
    },
    {
      "title": "Model Architecture Comparison",
      "type": "barchart",
      "datasource": "JSON API",
      "targets": [
        {
          "url": "http://dashboard-api:8000/api/metrics/comparison",
          "params": {
            "customer": "${customer}",
            "task": "${task}",
            "architectures": "yolov11n,yolov11s,yolov11m,yolov11l"
          }
        }
      ]
    },
    {
      "title": "Inference Latency Distribution",
      "type": "histogram"
    },
    {
      "title": "GPU Utilization",
      "type": "gauge"
    },
    {
      "title": "Performance Alerts",
      "type": "table",
      "datasource": "JSON API",
      "targets": [
        {
          "url": "http://dashboard-api:8000/api/metrics/alerts"
        }
      ]
    }
  ],
  "templating": {
    "list": [
      {
        "name": "customer",
        "type": "query",
        "query": "donghwa-entec,sampyo,duckwon"
      },
      {
        "name": "task",
        "type": "query", 
        "query": "ppe,fall,fire"
      }
    ]
  }
}
6. 고도화 자동화 파이프라인
python
# automation/optimizer.py
"""
대시보드 지표 기반 자동 고도화 트리거
"""
import mlflow
from dataclasses import dataclass
from typing import Callable

from scenario.metrics_store import MetricsStore
from scenario.factory import create_runner
from scenario.schema import TrainingScenario


@dataclass
class OptimizationTrigger:
    """고도화 트리거 조건"""
    name: str
    condition: Callable[[dict], bool]
    action: Callable[[dict], None]
    cooldown_hours: int = 24


class AutoOptimizer:
    """대시보드 지표 기반 자동 고도화"""
    
    def __init__(self, metrics_store: MetricsStore):
        self.store = metrics_store
        self.triggers: list[OptimizationTrigger] = []
        self._last_triggered = {}
    
    def register_trigger(self, trigger: OptimizationTrigger):
        self.triggers.append(trigger)
    
    def check_and_optimize(self):
        """트리거 조건 체크 및 고도화 실행"""
        import time
        
        # 최근 메트릭 수집
        recent_metrics = self._get_recent_metrics()
        
        for trigger in self.triggers:
            # 쿨다운 체크
            last_time = self._last_triggered.get(trigger.name, 0)
            if time.time() - last_time < trigger.cooldown_hours * 3600:
                continue
            
            # 조건 체크
            if trigger.condition(recent_metrics):
                print(f"🔄 Trigger activated: {trigger.name}")
                trigger.action(recent_metrics)
                self._last_triggered[trigger.name] = time.time()
    
    def _get_recent_metrics(self) -> dict:
        """최근 메트릭 집계"""
        import time
        import pandas as pd
        
        records = self.store.query_metrics(
            start_time=time.time() - 86400 * 7  # 최근 7일
        )
        df = pd.DataFrame(records)
        
        return {
            "df": df,
            "avg_mAP50": df["mAP50"].mean() if not df.empty else 0,
            "avg_latency": df["inference_time_ms"].mean() if not df.empty else 0,
            "by_customer": df.groupby("customer")["mAP50"].mean().to_dict() if not df.empty else {},
        }


# ─────────────────────────────────────────────
# 트리거 정의 예시
# ─────────────────────────────────────────────

def trigger_low_accuracy(metrics: dict) -> bool:
    """정확도 저하 감지"""
    return metrics["avg_mAP50"] < 0.75


def action_retrain_with_sweep(metrics: dict):
    """하이퍼파라미터 스윕으로 재학습"""
    df = metrics["df"]
    
    # 가장 성능 낮은 고객/태스크 찾기
    worst = df.groupby(["customer", "task"])["mAP50"].mean().idxmin()
    customer, task = worst
    
    print(f"🎯 Retraining for {customer}/{task}")
    
    # 스윕 시나리오 생성 및 실행
    scenario = TrainingScenario(
        name=f"{task}-auto-retrain",
        version="auto",
        site=SiteConfig(customer=customer, region="auto", zone="auto"),
        task=TaskConfig(type=task),
        model=ModelConfig(architecture="yolov11s"),
        training=TrainingParams(),
    ).with_sweep(
        params={
            "learning_rate": [0.001, 0.005, 0.01],
            "batch_size": [8, 16],
        }
    )
    
    runner = create_runner(scenario)
    runner.run()


def trigger_high_latency(metrics: dict) -> bool:
    """지연시간 초과 감지"""
    return metrics["avg_latency"] > 30  # 30ms 초과


def action_auto_quantize(metrics: dict):
    """자동 양자화 실행"""
    df = metrics["df"]
    
    # 지연시간 높은 모델 찾기
    slow_models = df[df["inference_time_ms"] > 30]
    
    for _, row in slow_models.iterrows():
        print(f"⚡ Auto-quantizing: {row['run_id']}")
        
        scenario = QuantizationScenario(
            name=f"auto-quant-{row['task']}",
            version="auto",
            site=SiteConfig(customer=row["customer"], region="auto", zone=row["zone"]),
            task=TaskConfig(type=row["task"]),
            model=ModelConfig(architecture=row["model_architecture"]),
            environment=EnvironmentConfig(precision="int8"),
            quantization=QuantizationParams(backend="openvino"),
            source_run_id=row["run_id"],
        )
        
        runner = create_runner(scenario)
        runner.run()


# ─────────────────────────────────────────────
# 스케줄러 설정
# ─────────────────────────────────────────────

def setup_optimizer():
    optimizer = AutoOptimizer(MetricsStore())
    
    optimizer.register_trigger(OptimizationTrigger(
        name="low_accuracy_retrain",
        condition=trigger_low_accuracy,
        action=action_retrain_with_sweep,
        cooldown_hours=48,
    ))
    
    optimizer.register_trigger(OptimizationTrigger(
        name="high_latency_quantize",
        condition=trigger_high_latency,
        action=action_auto_quantize,
        cooldown_hours=24,
    ))
    
    return optimizer


# cron 또는 Airflow에서 주기적 실행
if __name__ == "__main__":
    optimizer = setup_optimizer()
    optimizer.check_and_optimize()
7. CLI 확장 (대시보드/고도화)
bash
# 메트릭 조회
python -m guardianflow.cli metrics summary --customer donghwa-entec --days 30
python -m guardianflow.cli metrics trend --customer donghwa-entec --task ppe --metric mAP50

# 알림 확인
python -m guardianflow.cli alerts list

# 자동 고도화 실행
python -m guardianflow.cli optimize check    # 트리거 조건 체크
python -m guardianflow.cli optimize run      # 고도화 실행

# Parquet 내보내기 (BI 도구용)
python -m guardianflow.cli metrics export --format parquet --output ./exports/
```

---

## 대시보드 지표 → 고도화 흐름
```
┌─────────────────────────────────────────────────────────────────┐
│                     Dashboard Metrics                            │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐       │
│  │ mAP50 < 0.75 │    │ Latency > 30 │    │ GPU Util Low │       │
│  │   Trigger    │    │   Trigger    │    │   Trigger    │       │
│  └──────┬───────┘    └──────┬───────┘    └──────┬───────┘       │
│         │                   │                   │                │
│         ▼                   ▼                   ▼                │
│  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐       │
│  │  HP Sweep    │    │  Quantize    │    │  Batch Size  │       │
│  │  Retrain     │    │  (INT8)      │    │  Increase    │       │
│  └──────┬───────┘    └──────┬───────┘    └──────┬───────┘       │
│         │                   │                   │                │
│         └───────────────────┼───────────────────┘                │
│                             │                                    │
│                             ▼                                    │
│                    ┌────────────────┐                            │
│                    │ New Experiment │                            │
│                    │ → MLflow Run   │                            │
│                    │ → Model Reg.   │                            │
│                    └────────────────┘                            │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
요약
구성요소	기술 스택	역할
저장	SeaweedFS (S3) + PostgreSQL	Artifacts + Metadata
추적	MLflow	실험 관리
쿼리	MetricsStore (JSON/Parquet)	대시보드 최적화
시각화	Grafana / FastAPI	대시보드
자동화	AutoOptimizer + Triggers	고도화 파이프라인
현재까지 작성한 코드를 총평. 부족한 부분이나 나아갈 방향을 냉정히 진단

30 Jan
총평: 현재 설계 상태 진단
1. 완성도 평가
영역	완성도	상태
스키마 설계	80%	구조 정의됨, 실제 검증 미완
Runner 구현	40%	인터페이스만 존재, 핵심 로직 미구현
Model Registry	60%	API 정의됨, 에러 핸들링 부족
메트릭 수집	50%	스키마 존재, 실제 수집 코드 불완전
기존 코드 연동	30%	GenericRunner 개념만, 실제 연동 미검증
대시보드	20%	API 스켈레톤만 존재
자동화	10%	개념 설계만
종합: 설계 문서 수준 (PoC 미도달)

2. 핵심 문제점
A. 실제 동작 코드 부재
python
# 현재 상태: 추상화만 존재
class TrainingRunner(BaseRunner):
    def _execute(self):
        # ... 학습 로직 ...  ← 이 부분이 비어있음
        pass

# 필요한 것: 실제 YOLO/Ultralytics 연동
class TrainingRunner(BaseRunner):
    def _execute(self):
        from ultralytics import YOLO
        model = YOLO(self.scenario.model.architecture)
        results = model.train(
            data=self.scenario.dataset.path,
            epochs=self.scenario.training.epochs,
            # ...실제 파라미터 매핑
        )
문제: 설계만 있고 실행 불가능

B. 기존 30+ 실험 디렉토리 연동 미검증
python
# 가정한 구조
~/experiments/donghwa-entec-ppe/
├── train.py          # 표준 인터페이스 구현?
└── ...

# 실제 가능한 상태
~/experiments/donghwa-entec-ppe/
├── yolo_v11_train_1214.py        # 비표준 파일명
├── 학습_최종_v3.py                # 한글 파일명
├── train_backup_old.py           # 중복 파일
└── notebooks/                    # Jupyter 혼재
    └── experiment.ipynb
문제: 실제 디렉토리 구조 파악 없이 설계

C. 에러 핸들링/복구 전략 부재
python
# 현재: Happy path만 고려
def run(self):
    with mlflow.start_run():
        self._execute()
        self.registry.register_model(...)

# 필요: 실패 케이스 처리
def run(self):
    try:
        with mlflow.start_run() as run:
            self._execute()
    except TrainingFailedError:
        mlflow.set_tag("status", "FAILED")
        self._save_partial_results()
    except OutOfMemoryError:
        self._reduce_batch_and_retry()
    finally:
        self._cleanup_temp_files()
```

---

### D. 테스트 전무
```
guardianflow/
├── scenario/
├── pipelines/
└── tests/          ← 존재하지 않음
    ├── test_schema.py
    ├── test_runner.py
    └── fixtures/
E. 설정/환경 관리 미흡
python
# 현재: 하드코딩 산재
mlflow.set_tracking_uri("http://localhost:5000")
store_path = "./metrics_store"

# 필요: 중앙 설정 관리
# config/settings.py
class Settings(BaseSettings):
    mlflow_tracking_uri: str
    seaweedfs_endpoint: str
    metrics_store_path: Path
    
    class Config:
        env_file = ".env"
3. 과도한 추상화 문제
python
# 현재 계층 구조 (너무 깊음)
BaseScenario
  → TrainingScenario
      → with_sweep() → SweepConfig
          → SweepRunner
              → BaseRunner
                  → TrainingRunner
                      → _execute()

# 30개 실험을 즉시 관리하기엔 오버엔지니어링
```

**문제**: MVP 전에 확장성 고려 과다

---

## 4. 권장 우선순위 (재정립)

### Phase 1: 최소 동작 버전 (1-2주)
```
목표: 단일 실험 → MLflow 기록 → 모델 등록

구현 범위:
1. TrainingRunner._execute() 실제 구현 (YOLO 연동)
2. 기존 실험 디렉토리 1개 선정하여 연동 테스트
3. MLflow + SeaweedFS 실제 배포
4. 단일 학습 → 등록 → 조회 E2E 검증
python
# phase1/simple_runner.py
"""Phase 1: 최소 동작 버전"""
import mlflow
from ultralytics import YOLO


def run_training(
    experiment_name: str,
    model_arch: str,
    data_yaml: str,
    epochs: int,
    batch_size: int,
):
    """단순 학습 실행 + MLflow 기록"""
    mlflow.set_experiment(experiment_name)
    
    with mlflow.start_run():
        # 파라미터 기록
        mlflow.log_params({
            "model": model_arch,
            "epochs": epochs,
            "batch_size": batch_size,
        })
        
        # 학습 실행
        model = YOLO(model_arch)
        results = model.train(
            data=data_yaml,
            epochs=epochs,
            batch=batch_size,
        )
        
        # 메트릭 기록
        mlflow.log_metrics({
            "mAP50": results.results_dict["metrics/mAP50(B)"],
            "mAP50-95": results.results_dict["metrics/mAP50-95(B)"],
        })
        
        # 모델 등록
        mlflow.log_artifact(str(model.trainer.best))
        mlflow.register_model(
            f"runs:/{mlflow.active_run().info.run_id}/best.pt",
            experiment_name.replace("/", "-"),
        )


# 즉시 실행 가능
if __name__ == "__main__":
    run_training(
        experiment_name="donghwa-entec/welding/ppe",
        model_arch="yolov11s.pt",
        data_yaml="/path/to/data.yaml",
        epochs=100,
        batch_size=16,
    )
```

---

### Phase 2: 기존 코드 점진적 연동 (2-3주)
```
목표: 기존 30개 디렉토리 중 5개 표준화

작업:
1. 실제 디렉토리 구조 조사 (ls -la 수행)
2. 공통 패턴 추출
3. 최소 래퍼 작성 (GenericRunner 단순화)
4. 레지스트리 YAML 5개만 작성
python
# phase2/wrapper.py
"""Phase 2: 기존 코드 래퍼 (단순화)"""
import subprocess
import mlflow


def wrap_existing_script(
    script_path: str,
    experiment_name: str,
    working_dir: str,
    args: list[str] = None,
):
    """기존 스크립트를 MLflow로 래핑"""
    mlflow.set_experiment(experiment_name)
    
    with mlflow.start_run():
        mlflow.set_tag("script", script_path)
        mlflow.set_tag("wrapped", "true")
        
        # 기존 스크립트 실행
        cmd = ["python", script_path] + (args or [])
        result = subprocess.run(
            cmd,
            cwd=working_dir,
            capture_output=True,
            text=True,
        )
        
        mlflow.log_text(result.stdout, "stdout.txt")
        mlflow.log_text(result.stderr, "stderr.txt")
        
        # 결과물 자동 수집
        for ext in ["*.pt", "*.csv", "*.png"]:
            for f in Path(working_dir).rglob(ext):
                if f.stat().st_mtime > start_time:
                    mlflow.log_artifact(str(f))


# 사용
wrap_existing_script(
    script_path="train_yolov11_v2.py",
    experiment_name="donghwa-entec/welding/ppe",
    working_dir="/home/user/experiments/donghwa-entec-ppe",
    args=["--epochs", "100"],
)
```

---

### Phase 3: 스키마/Runner 고도화 (3-4주)
```
목표: 현재 설계한 스키마 실제 적용

조건: Phase 1, 2 완료 후에만 진행
- 실제 사용 패턴 파악 후 스키마 수정
- 불필요한 추상화 제거
- 테스트 코드 추가
```

---

### Phase 4: 대시보드/자동화 (4주+)
```
목표: 모니터링 및 자동 고도화

조건: Phase 3 안정화 후
- 메트릭 수집 검증 완료
- 충분한 실험 데이터 축적
5. 즉시 해야 할 것
bash
# 1. 실제 디렉토리 구조 파악
ls -la ~/experiments/
find ~/experiments -name "*.py" | head -50
find ~/experiments -name "*.pt" | head -20

# 2. 대표 실험 1개 선정
# - 가장 단순한 구조
# - 최근 사용된 것
# - 문서화된 것

# 3. MLflow 로컬 실행 확인
mlflow ui --port 5000

# 4. Phase 1 코드로 E2E 테스트
python phase1/simple_runner.py
6. 삭제/단순화 권장 항목
현재 설계	권장	이유
ExperimentDesign enum (5가지)	2가지 (single, sweep)	나머지는 실제 필요 시 추가
AblationConfig	삭제	사용 사례 불명확
ComparisonRunner	단순화	기존 run 쿼리로 대체 가능
AutoOptimizer	연기	Phase 4 이후
Grafana 연동	연기	MLflow UI로 충분
복잡한 상속 구조	단순 함수	MVP에서 불필요
7. 결론
현재 상태: 아키텍처 설계 문서 (실행 불가)

핵심 리스크:

실제 기존 코드와 괴리
과도한 추상화로 MVP 지연
검증 없는 설계 누적
권장 방향:

설계 멈추고 Phase 1 코드 작성
단일 E2E 성공 후 확장
기존 30개 디렉토리 실사 후 설계 수정
설계 멈추고 Phase 1 코드 작성

30 Jan
Phase 1: 최소 동작 버전
목표: 단일 학습 → MLflow 기록 → 모델 등록 E2E 동작

디렉토리 구조
guardianflow/
├── config.py           # 환경 설정
├── runner.py           # 학습 실행기
├── registry.py         # 모델 등록
├── metrics.py          # 메트릭 수집
└── cli.py              # CLI 진입점
구현
python
# guardianflow/config.py
"""환경 설정"""
import os
from pathlib import Path


class Config:
    # MLflow
    MLFLOW_TRACKING_URI = os.getenv("MLFLOW_TRACKING_URI", "http://localhost:5000")
    
    # SeaweedFS (S3 호환)
    S3_ENDPOINT_URL = os.getenv("MLFLOW_S3_ENDPOINT_URL", "http://localhost:8333")
    AWS_ACCESS_KEY_ID = os.getenv("AWS_ACCESS_KEY_ID", "")
    AWS_SECRET_ACCESS_KEY = os.getenv("AWS_SECRET_ACCESS_KEY", "")
    
    # 작업 디렉토리
    WORK_DIR = Path(os.getenv("GUARDIANFLOW_WORK_DIR", "/tmp/guardianflow"))
    
    @classmethod
    def setup(cls):
        """환경변수 설정"""
        os.environ["MLFLOW_TRACKING_URI"] = cls.MLFLOW_TRACKING_URI
        os.environ["MLFLOW_S3_ENDPOINT_URL"] = cls.S3_ENDPOINT_URL
        os.environ["AWS_ACCESS_KEY_ID"] = cls.AWS_ACCESS_KEY_ID
        os.environ["AWS_SECRET_ACCESS_KEY"] = cls.AWS_SECRET_ACCESS_KEY
        cls.WORK_DIR.mkdir(parents=True, exist_ok=True)
python
# guardianflow/metrics.py
"""메트릭 수집"""
import time
from dataclasses import dataclass, field
from typing import Optional

try:
    import pynvml
    pynvml.nvmlInit()
    NVML_AVAILABLE = True
except:
    NVML_AVAILABLE = False


@dataclass
class TrainingMetrics:
    """학습 결과 메트릭"""
    # Detection
    mAP50: float = 0.0
    mAP50_95: float = 0.0
    precision: float = 0.0
    recall: float = 0.0
    
    # Loss
    box_loss: float = 0.0
    cls_loss: float = 0.0
    dfl_loss: float = 0.0
    
    def to_dict(self, prefix: str = "") -> dict:
        p = f"{prefix}." if prefix else ""
        return {
            f"{p}mAP50": self.mAP50,
            f"{p}mAP50_95": self.mAP50_95,
            f"{p}precision": self.precision,
            f"{p}recall": self.recall,
            f"{p}box_loss": self.box_loss,
            f"{p}cls_loss": self.cls_loss,
            f"{p}dfl_loss": self.dfl_loss,
        }


@dataclass
class SystemMetrics:
    """시스템 메트릭"""
    gpu_memory_used_mb: float = 0.0
    gpu_utilization_pct: float = 0.0
    inference_time_ms: float = 0.0
    model_size_mb: float = 0.0
    
    def to_dict(self, prefix: str = "sys") -> dict:
        return {
            f"{prefix}.gpu_memory_mb": self.gpu_memory_used_mb,
            f"{prefix}.gpu_util_pct": self.gpu_utilization_pct,
            f"{prefix}.inference_ms": self.inference_time_ms,
            f"{prefix}.model_size_mb": self.model_size_mb,
        }


def get_gpu_metrics(device_id: int = 0) -> dict:
    """GPU 메트릭 수집"""
    if not NVML_AVAILABLE:
        return {}
    
    try:
        handle = pynvml.nvmlDeviceGetHandleByIndex(device_id)
        mem = pynvml.nvmlDeviceGetMemoryInfo(handle)
        util = pynvml.nvmlDeviceGetUtilizationRates(handle)
        
        return {
            "gpu_memory_used_mb": mem.used / 1024**2,
            "gpu_memory_total_mb": mem.total / 1024**2,
            "gpu_utilization_pct": util.gpu,
        }
    except:
        return {}


def get_model_size(model_path: str) -> float:
    """모델 파일 크기 (MB)"""
    from pathlib import Path
    p = Path(model_path)
    if p.exists():
        return p.stat().st_size / 1024**2
    return 0.0
python
# guardianflow/registry.py
"""모델 레지스트리 관리"""
import mlflow
from mlflow.tracking import MlflowClient
from typing import Optional


class ModelRegistry:
    """MLflow Model Registry 관리"""
    
    def __init__(self):
        self.client = MlflowClient()
    
    def register(
        self,
        run_id: str,
        artifact_path: str,
        model_name: str,
        tags: dict = None,
    ) -> dict:
        """모델 등록 및 candidate alias 설정"""
        model_uri = f"runs:/{run_id}/{artifact_path}"
        
        try:
            # 모델 등록
            mv = mlflow.register_model(model_uri, model_name)
            
            # 태그 추가
            if tags:
                for k, v in tags.items():
                    self.client.set_model_version_tag(model_name, mv.version, k, str(v))
            
            # candidate alias 설정
            self.client.set_registered_model_alias(model_name, "candidate", mv.version)
            
            return {
                "name": model_name,
                "version": mv.version,
                "alias": "candidate",
                "uri": model_uri,
            }
        
        except Exception as e:
            print(f"⚠️ Model registration failed: {e}")
            return {"error": str(e)}
    
    def promote(self, model_name: str, version: str = None) -> dict:
        """candidate → champion 승격"""
        try:
            # 승격 대상 결정
            if version is None:
                mv = self.client.get_model_version_by_alias(model_name, "candidate")
                version = mv.version
            
            # 현재 champion을 rollback으로
            try:
                current = self.client.get_model_version_by_alias(model_name, "champion")
                self.client.set_registered_model_alias(model_name, "rollback", current.version)
            except:
                pass  # champion 없음
            
            # 승격
            self.client.set_registered_model_alias(model_name, "champion", version)
            
            # candidate alias 제거
            try:
                self.client.delete_registered_model_alias(model_name, "candidate")
            except:
                pass
            
            return {"promoted": version, "model": model_name}
        
        except Exception as e:
            return {"error": str(e)}
    
    def get_champion_uri(self, model_name: str) -> Optional[str]:
        """champion 모델 URI 반환"""
        try:
            mv = self.client.get_model_version_by_alias(model_name, "champion")
            return f"models:/{model_name}@champion"
        except:
            return None
python
# guardianflow/runner.py
"""학습 실행기"""
import mlflow
from pathlib import Path
from datetime import datetime
from typing import Optional

from .config import Config
from .metrics import TrainingMetrics, SystemMetrics, get_gpu_metrics, get_model_size
from .registry import ModelRegistry


class TrainingRunner:
    """YOLO 학습 실행 + MLflow 기록"""
    
    def __init__(
        self,
        experiment_name: str,
        model_arch: str = "yolov11s.pt",
        data_yaml: str = None,
    ):
        Config.setup()
        
        self.experiment_name = experiment_name
        self.model_arch = model_arch
        self.data_yaml = data_yaml
        self.registry = ModelRegistry()
        
        # MLflow 실험 설정
        mlflow.set_tracking_uri(Config.MLFLOW_TRACKING_URI)
        mlflow.set_experiment(experiment_name)
    
    def run(
        self,
        epochs: int = 100,
        batch_size: int = 16,
        imgsz: int = 640,
        lr: float = 0.01,
        device: str = "0",
        run_name: str = None,
        tags: dict = None,
        register: bool = True,
    ) -> dict:
        """학습 실행"""
        from ultralytics import YOLO
        
        run_name = run_name or f"train-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
        
        with mlflow.start_run(run_name=run_name) as run:
            run_id = run.info.run_id
            print(f"🚀 Starting run: {run_id}")
            
            # ─────────────────────────────────────────
            # 1. 파라미터 기록
            # ─────────────────────────────────────────
            params = {
                "model": self.model_arch,
                "epochs": epochs,
                "batch_size": batch_size,
                "imgsz": imgsz,
                "lr": lr,
                "device": device,
                "data": self.data_yaml,
            }
            mlflow.log_params(params)
            
            # 태그 기록
            if tags:
                mlflow.set_tags(tags)
            
            # ─────────────────────────────────────────
            # 2. 학습 실행
            # ─────────────────────────────────────────
            try:
                model = YOLO(self.model_arch)
                
                results = model.train(
                    data=self.data_yaml,
                    epochs=epochs,
                    batch=batch_size,
                    imgsz=imgsz,
                    lr0=lr,
                    device=device,
                    project=str(Config.WORK_DIR / "runs"),
                    name=run_id,
                    exist_ok=True,
                )
                
                mlflow.set_tag("status", "COMPLETED")
                
            except Exception as e:
                mlflow.set_tag("status", "FAILED")
                mlflow.set_tag("error", str(e))
                print(f"❌ Training failed: {e}")
                return {"run_id": run_id, "status": "FAILED", "error": str(e)}
            
            # ─────────────────────────────────────────
            # 3. 메트릭 기록
            # ─────────────────────────────────────────
            metrics = self._extract_metrics(results)
            mlflow.log_metrics(metrics.to_dict())
            
            # 시스템 메트릭
            sys_metrics = self._get_system_metrics(model.trainer.best)
            mlflow.log_metrics(sys_metrics.to_dict())
            
            # ─────────────────────────────────────────
            # 4. Artifacts 저장
            # ─────────────────────────────────────────
            best_model_path = Path(model.trainer.best)
            last_model_path = Path(model.trainer.last)
            
            if best_model_path.exists():
                mlflow.log_artifact(str(best_model_path), "weights")
            if last_model_path.exists():
                mlflow.log_artifact(str(last_model_path), "weights")
            
            # 학습 결과 파일
            results_dir = Path(model.trainer.save_dir)
            for f in results_dir.glob("*.csv"):
                mlflow.log_artifact(str(f), "results")
            for f in results_dir.glob("*.png"):
                mlflow.log_artifact(str(f), "plots")
            
            # ─────────────────────────────────────────
            # 5. 모델 등록
            # ─────────────────────────────────────────
            model_info = None
            if register and best_model_path.exists():
                model_name = self._get_model_name()
                model_info = self.registry.register(
                    run_id=run_id,
                    artifact_path=f"weights/{best_model_path.name}",
                    model_name=model_name,
                    tags={
                        "architecture": self.model_arch,
                        "mAP50": metrics.mAP50,
                        "epochs": epochs,
                    },
                )
                print(f"✅ Model registered: {model_info}")
            
            return {
                "run_id": run_id,
                "status": "COMPLETED",
                "metrics": metrics.to_dict(),
                "model": model_info,
            }
    
    def _extract_metrics(self, results) -> TrainingMetrics:
        """Ultralytics 결과에서 메트릭 추출"""
        try:
            r = results.results_dict
            return TrainingMetrics(
                mAP50=r.get("metrics/mAP50(B)", 0),
                mAP50_95=r.get("metrics/mAP50-95(B)", 0),
                precision=r.get("metrics/precision(B)", 0),
                recall=r.get("metrics/recall(B)", 0),
                box_loss=r.get("train/box_loss", 0),
                cls_loss=r.get("train/cls_loss", 0),
                dfl_loss=r.get("train/dfl_loss", 0),
            )
        except:
            return TrainingMetrics()
    
    def _get_system_metrics(self, model_path: str) -> SystemMetrics:
        """시스템 메트릭 수집"""
        gpu = get_gpu_metrics()
        return SystemMetrics(
            gpu_memory_used_mb=gpu.get("gpu_memory_used_mb", 0),
            gpu_utilization_pct=gpu.get("gpu_utilization_pct", 0),
            model_size_mb=get_model_size(model_path),
        )
    
    def _get_model_name(self) -> str:
        """실험명 → 모델명 변환"""
        # donghwa-entec/busan-plant1/welding → donghwa-entec-busan-plant1-welding
        return self.experiment_name.replace("/", "-")
python
# guardianflow/cli.py
"""CLI 진입점"""
import argparse
import sys


def cmd_train(args):
    """학습 실행"""
    from .runner import TrainingRunner
    
    runner = TrainingRunner(
        experiment_name=args.experiment,
        model_arch=args.model,
        data_yaml=args.data,
    )
    
    tags = {}
    if args.customer:
        tags["customer"] = args.customer
    if args.task:
        tags["task"] = args.task
    
    result = runner.run(
        epochs=args.epochs,
        batch_size=args.batch,
        imgsz=args.imgsz,
        lr=args.lr,
        device=args.device,
        run_name=args.name,
        tags=tags if tags else None,
        register=not args.no_register,
    )
    
    print(f"\n{'='*50}")
    print(f"Run ID: {result['run_id']}")
    print(f"Status: {result['status']}")
    if result.get("metrics"):
        print(f"mAP50: {result['metrics'].get('mAP50', 'N/A'):.4f}")
    if result.get("model"):
        print(f"Model: {result['model']}")
    print(f"{'='*50}")


def cmd_promote(args):
    """모델 승격"""
    from .registry import ModelRegistry
    
    registry = ModelRegistry()
    result = registry.promote(args.model, args.version)
    
    if "error" in result:
        print(f"❌ Failed: {result['error']}")
    else:
        print(f"✅ Promoted {result['model']} v{result['promoted']} to champion")


def cmd_list(args):
    """실험/모델 목록"""
    import mlflow
    from .config import Config
    
    Config.setup()
    mlflow.set_tracking_uri(Config.MLFLOW_TRACKING_URI)
    
    if args.type == "experiments":
        client = mlflow.tracking.MlflowClient()
        for exp in client.search_experiments():
            print(f"  {exp.experiment_id}: {exp.name}")
    
    elif args.type == "models":
        client = mlflow.tracking.MlflowClient()
        for rm in client.search_registered_models():
            print(f"  {rm.name}")
            for alias in ["champion", "candidate", "rollback"]:
                try:
                    mv = client.get_model_version_by_alias(rm.name, alias)
                    print(f"    @{alias}: v{mv.version}")
                except:
                    pass


def main():
    parser = argparse.ArgumentParser(
        prog="guardianflow",
        description="GuardianFlow MLOps CLI",
    )
    subparsers = parser.add_subparsers(dest="command", required=True)
    
    # ─────────────────────────────────────────
    # train
    # ─────────────────────────────────────────
    train_parser = subparsers.add_parser("train", help="Run training")
    train_parser.add_argument("--experiment", "-e", required=True, help="Experiment name")
    train_parser.add_argument("--data", "-d", required=True, help="Data YAML path")
    train_parser.add_argument("--model", "-m", default="yolov11s.pt", help="Model architecture")
    train_parser.add_argument("--epochs", type=int, default=100)
    train_parser.add_argument("--batch", type=int, default=16)
    train_parser.add_argument("--imgsz", type=int, default=640)
    train_parser.add_argument("--lr", type=float, default=0.01)
    train_parser.add_argument("--device", default="0")
    train_parser.add_argument("--name", help="Run name")
    train_parser.add_argument("--customer", help="Customer tag")
    train_parser.add_argument("--task", help="Task tag (ppe, fall, etc)")
    train_parser.add_argument("--no-register", action="store_true", help="Skip model registration")
    train_parser.set_defaults(func=cmd_train)
    
    # ─────────────────────────────────────────
    # promote
    # ─────────────────────────────────────────
    promote_parser = subparsers.add_parser("promote", help="Promote model to champion")
    promote_parser.add_argument("model", help="Model name")
    promote_parser.add_argument("--version", "-v", help="Specific version")
    promote_parser.set_defaults(func=cmd_promote)
    
    # ─────────────────────────────────────────
    # list
    # ─────────────────────────────────────────
    list_parser = subparsers.add_parser("list", help="List experiments/models")
    list_parser.add_argument("type", choices=["experiments", "models"])
    list_parser.set_defaults(func=cmd_list)
    
    args = parser.parse_args()
    args.func(args)


if __name__ == "__main__":
    main()
python
# guardianflow/__init__.py
"""GuardianFlow MLOps"""
from .config import Config
from .runner import TrainingRunner
from .registry import ModelRegistry

__version__ = "0.1.0"
toml
# pyproject.toml
[project]
name = "guardianflow"
version = "0.1.0"
requires-python = ">=3.10"
dependencies = [
    "mlflow>=2.10",
    "ultralytics>=8.0",
    "pynvml",
]

[project.scripts]
guardianflow = "guardianflow.cli:main"
사용법
bash
# 1. 설치
pip install -e .

# 2. MLflow 서버 실행 (로컬 테스트)
mlflow server --host 0.0.0.0 --port 5000

# 3. 학습 실행
guardianflow train \
    --experiment "donghwa-entec/welding/ppe" \
    --data /path/to/data.yaml \
    --model yolov11s.pt \
    --epochs 100 \
    --batch 16 \
    --customer donghwa-entec \
    --task ppe

# 4. 모델 승격
guardianflow promote donghwa-entec-welding-ppe

# 5. 목록 확인
guardianflow list experiments
guardianflow list models
python
# Python에서 직접 사용
from guardianflow import TrainingRunner

runner = TrainingRunner(
    experiment_name="donghwa-entec/welding/ppe",
    model_arch="yolov11s.pt",
    data_yaml="/path/to/data.yaml",
)

result = runner.run(epochs=100, batch_size=16)
print(result["run_id"])
검증 체크리스트
bash
# 1. MLflow 연결 확인
python -c "import mlflow; print(mlflow.get_tracking_uri())"

# 2. YOLO import 확인
python -c "from ultralytics import YOLO; print('OK')"

# 3. 단일 학습 테스트 (짧은 epoch)
guardianflow train \
    --experiment test \
    --data coco128.yaml \
    --epochs 1 \
    --batch 8

# 4. MLflow UI에서 확인
# http://localhost:5000

# 5. 모델 등록 확인
guardianflow list models
다음 단계 (Phase 1 완료 후)
실제 데이터로 E2E 테스트 (donghwa-entec-ppe 등)
SeaweedFS 연동 테스트
에러 케이스 보완 (OOM, 학습 실패 등)
기존 실험 디렉토리 조사 → Phase 2 설계
최종 설계안의 핵심 컴포넌트와 로직들 구체적으로 정리. 이를 통해 작업계획을 상세히 만들 예정

30 Jan
GuardianFlow 최종 설계 요약
1. 핵심 컴포넌트 구조
guardianflow/
├── core/                    # 핵심 로직
│   ├── config.py           # 환경 설정
│   ├── runner.py           # 실험 실행기
│   ├── registry.py         # 모델 레지스트리
│   └── metrics.py          # 메트릭 정의/수집
│
├── scenario/                # 시나리오 관리 (Phase 3)
│   ├── schema.py           # 시나리오 스키마
│   ├── experiment_design.py # 실험 설계 타입
│   └── factory.py          # Runner 팩토리
│
├── adapters/                # 외부 연동
│   ├── generic_wrapper.py  # 기존 코드 래퍼
│   └── experiment_registry.py # 실험 디렉토리 레지스트리
│
├── dashboard/               # 대시보드 (Phase 4)
│   ├── api.py              # REST API
│   ├── metrics_store.py    # 메트릭 저장소
│   └── automation.py       # 자동화 트리거
│
└── cli.py                   # CLI 진입점
2. 컴포넌트별 상세 명세
2.1 Config (환경 설정)
항목	타입	설명	기본값
MLFLOW_TRACKING_URI	str	MLflow 서버 주소	http://localhost:5000
S3_ENDPOINT_URL	str	SeaweedFS S3 엔드포인트	http://localhost:8333
WORK_DIR	Path	작업 디렉토리	/tmp/guardianflow
EXPERIMENTS_BASE	Path	기존 실험 코드 경로	~/experiments
구현 상태: ✅ 완료 (Phase 1)

2.2 TrainingRunner (학습 실행기)
입력:

파라미터	타입	필수	설명
experiment_name	str	✅	MLflow 실험명 (예: donghwa-entec/welding/ppe)
model_arch	str	✅	모델 아키텍처 (예: yolov11s.pt)
data_yaml	str	✅	데이터셋 설정 파일 경로
epochs	int		학습 에폭
batch_size	int		배치 크기
imgsz	int		입력 이미지 크기
lr	float		학습률
device	str		GPU 디바이스
tags	dict		추가 태그 (customer, task 등)
출력:

python
{
    "run_id": str,           # MLflow run ID
    "status": str,           # COMPLETED | FAILED
    "metrics": {
        "mAP50": float,
        "mAP50_95": float,
        "precision": float,
        "recall": float,
    },
    "model": {               # 등록 정보
        "name": str,
        "version": str,
        "alias": str,
    }
}
```

**내부 로직**:
```
1. MLflow run 시작
2. 파라미터 기록 (log_params)
3. YOLO 학습 실행 (ultralytics)
4. 메트릭 추출 및 기록 (log_metrics)
5. Artifact 저장 (weights, plots, results)
6. Model Registry 등록 (@candidate)
7. Run 종료
구현 상태: ✅ 완료 (Phase 1)

2.3 InferenceRunner (추론 벤치마크)
입력:

파라미터	타입	필수	설명
experiment_name	str	✅	MLflow 실험명
model_path	str	✅	모델 경로 또는 Registry URI
data_yaml	str	✅	테스트 데이터셋
batch_sizes	list[int]		벤치마크할 배치 크기들
warmup_runs	int		워밍업 실행 횟수
benchmark_runs	int		벤치마크 실행 횟수
conf_threshold	float		신뢰도 임계값
use_tracking	bool		MOT 메트릭 수집 여부
출력:

python
{
    "run_id": str,
    "detection_metrics": {
        "mAP50": float,
        "mAP50_95": float,
        "precision": float,
        "recall": float,
    },
    "mot_metrics": {         # use_tracking=True 시
        "mota": float,
        "motp": float,
        "idf1": float,
        "hota": float,
        "id_switches": int,
    },
    "system_metrics": {
        "inference_time_ms": float,
        "inference_time_p95_ms": float,
        "fps": float,
        "gpu_memory_mb": float,
    }
}
구현 상태: ❌ 미구현 (Phase 2)

2.4 QuantizationRunner (양자화)
입력:

파라미터	타입	필수	설명
experiment_name	str	✅	MLflow 실험명
source_run_id	str	✅	원본 학습 run ID
backend	str	✅	openvino | tensorrt | onnx
precision	str	✅	fp16 | int8
calibration_data	str		INT8용 캘리브레이션 데이터
calibration_size	int		캘리브레이션 샘플 수
출력:

python
{
    "run_id": str,
    "original_metrics": {...},
    "quantized_metrics": {...},
    "comparison": {
        "accuracy_drop_mAP50": float,
        "speedup_ratio": float,
        "compression_ratio": float,
    },
    "model": {
        "name": str,         # {base}__openvino_int8
        "version": str,
        "alias": str,
    }
}
```

**구현 상태**: ❌ 미구현 (Phase 2)

---

### 2.5 ModelRegistry (모델 레지스트리)

**메서드**:

| 메서드 | 입력 | 출력 | 설명 |
|--------|------|------|------|
| `register()` | run_id, artifact_path, model_name, tags | ModelInfo | 모델 등록 + @candidate |
| `promote()` | model_name, version? | PromoteResult | candidate → champion |
| `rollback()` | model_name | RollbackResult | champion ← rollback |
| `get_champion_uri()` | model_name | str \| None | champion 모델 URI |
| `list_versions()` | model_name | list[VersionInfo] | 버전 목록 |

**Alias 워크플로우**:
```
Training → @candidate
    ↓ (검증 통과)
Promote → @champion (기존 champion → @rollback)
    ↓ (문제 발생)
Rollback → @champion ← @rollback
```

**모델명 규칙**:
```
# FP32 모델
{customer}/{region}-{plant}/{zone}/{task}
예: donghwa-entec/busan-plant1/welding/ppe

# 양자화 모델
{customer}/{region}-{plant}/{zone}/{task}__{backend}_{precision}
예: donghwa-entec/busan-plant1/welding/ppe__openvino_int8
```

**구현 상태**: ✅ 완료 (Phase 1)

---

### 2.6 Metrics (메트릭 정의)

**Detection Metrics**:
| 메트릭 | 타입 | 소스 | 설명 |
|--------|------|------|------|
| `mAP50` | float | YOLO | AP @ IoU 0.5 |
| `mAP50_95` | float | YOLO | AP @ IoU 0.5:0.95 |
| `precision` | float | YOLO | 정밀도 |
| `recall` | float | YOLO | 재현율 |
| `box_loss` | float | YOLO | 박스 손실 |
| `cls_loss` | float | YOLO | 클래스 손실 |
| `dfl_loss` | float | YOLO | DFL 손실 |
| `per_class_ap` | dict | YOLO | 클래스별 AP |

**MOT Metrics**:
| 메트릭 | 타입 | 소스 | 설명 |
|--------|------|------|------|
| `mota` | float | TrackEval | Multi-Object Tracking Accuracy |
| `motp` | float | TrackEval | Multi-Object Tracking Precision |
| `idf1` | float | TrackEval | ID F1 Score |
| `hota` | float | TrackEval | Higher Order Tracking Accuracy |
| `id_switches` | int | TrackEval | ID 전환 횟수 |
| `frag` | int | TrackEval | Fragmentation |

**System Metrics**:
| 메트릭 | 타입 | 소스 | 설명 |
|--------|------|------|------|
| `inference_time_ms` | float | 측정 | 평균 추론 시간 |
| `inference_time_p95_ms` | float | 측정 | P95 추론 시간 |
| `fps` | float | 측정 | 초당 프레임 |
| `gpu_memory_mb` | float | pynvml | GPU 메모리 사용량 |
| `gpu_utilization_pct` | float | pynvml | GPU 사용률 |
| `model_size_mb` | float | 파일 | 모델 파일 크기 |
| `model_params_m` | float | thop | 파라미터 수 (M) |
| `model_flops_g` | float | thop | 연산량 (G) |

**구현 상태**: ✅ 정의 완료, 수집 로직 부분 구현

---

### 2.7 GenericWrapper (기존 코드 래퍼)

**목적**: 기존 30+ 실험 디렉토리를 수정 없이 MLflow로 래핑

**입력**:
| 파라미터 | 타입 | 필수 | 설명 |
|----------|------|------|------|
| `experiment_name` | str | ✅ | MLflow 실험명 |
| `script_path` | str | ✅ | 실행할 스크립트 경로 |
| `working_dir` | str | ✅ | 작업 디렉토리 |
| `args` | list[str] | | 스크립트 인자 |
| `artifact_patterns` | list[str] | | 수집할 파일 패턴 |

**로직**:
```
1. MLflow run 시작
2. 스크립트 메타정보 태그 기록
3. subprocess로 스크립트 실행
4. stdout/stderr 저장
5. 결과 파일 자동 수집 (*.pt, *.csv, *.png)
6. 모델 파일 발견 시 자동 등록
구현 상태: ❌ 미구현 (Phase 2)

2.8 ExperimentRegistry (실험 디렉토리 레지스트리)
목적: 기존 실험 디렉토리 메타데이터 관리

YAML 구조:

yaml
experiments:
  donghwa-entec-ppe:
    path: ~/experiments/donghwa-entec-ppe
    site:
      customer: donghwa-entec
      region: busan
      plant: plant1
      zone: welding
    task:
      type: ppe
      classes: [headgear, harness, hook]
    entry_points:
      train:
        default: train_yolov11_v2.py
        patterns: ["train*.py"]
        args_template: "--epochs {epochs} --batch {batch_size}"
      infer:
        default: infer_benchmark.py
      quant:
        default: export_openvino.py
```

**메서드**:
| 메서드 | 설명 |
|--------|------|
| `get(name)` | 실험 정보 조회 |
| `list_all()` | 전체 목록 |
| `find_by_customer(customer)` | 고객별 필터 |
| `scan_and_generate(base_path)` | 디렉토리 스캔 → YAML 생성 |

**구현 상태**: ❌ 미구현 (Phase 2)

---

### 2.9 Scenario Schema (시나리오 스키마)

**계층 구조**:
```
BaseScenario
├── TrainingScenario
├── InferenceBenchmarkScenario
└── QuantizationScenario
```

**BaseScenario 필드**:
| 필드 | 타입 | 설명 |
|------|------|------|
| `name` | str | 시나리오명 |
| `version` | str | 버전 (v1, v2.1) |
| `experiment_type` | str | train \| infer \| quant |
| `design` | ExperimentDesign | single \| sweep \| comparison |
| `site` | SiteConfig | 현장 정보 |
| `task` | TaskConfig | 태스크 정보 |
| `model` | ModelConfig | 모델 정보 |
| `environment` | EnvironmentConfig | 환경 정보 |
| `dataset` | DatasetConfig | 데이터셋 정보 |

**SiteConfig**:
| 필드 | 타입 | 예시 |
|------|------|------|
| `customer` | str | donghwa-entec |
| `region` | str | busan |
| `plant` | str | plant1 |
| `zone` | str | welding |

**생성 메서드**:
| 메서드 | 출력 예시 |
|--------|----------|
| `get_experiment_name()` | `donghwa-entec/busan-plant1/welding` |
| `get_run_name()` | `ppe-yolov11s-train-v2` |
| `get_registered_model_name()` | `donghwa-entec/busan-plant1/welding/ppe` |

**구현 상태**: ✅ 설계 완료, 코드화 필요 (Phase 3)

---

### 2.10 Experiment Design (실험 설계)

**타입**:
| 설계 | 설명 | MLflow 구조 |
|------|------|-------------|
| `SINGLE` | 단일 실행 | 1 Run |
| `SWEEP` | HP 탐색 | Parent + N Child Runs |
| `COMPARISON` | 모델/버전 비교 | Parent + N Child Runs |

**SweepConfig**:
| 필드 | 타입 | 설명 |
|------|------|------|
| `parameters` | dict[str, list] | 탐색 파라미터 |
| `strategy` | str | grid \| random |
| `max_runs` | int | 최대 실행 수 |

**ComparisonConfig**:
| 필드 | 타입 | 설명 |
|------|------|------|
| `models` | list[str] | 비교할 아키텍처 |
| `run_ids` | list[str] | 비교할 기존 run |
| `quantization_variants` | list[dict] | 양자화 variant |
| `datasets` | list[str] | 일반화 테스트 데이터셋 |

**구현 상태**: ✅ 설계 완료, 코드화 필요 (Phase 3)

---

### 2.11 MetricsStore (대시보드용 저장소)

**목적**: MLflow 외 별도 저장소로 빠른 대시보드 쿼리 지원

**저장 형식**:
- JSON Lines (일별 파일): `metrics_2025-01-30.jsonl`
- Parquet (집계): `metrics_aggregated.parquet`

**MetricRecord 필드**:
| 필드 | 타입 | 설명 |
|------|------|------|
| `timestamp` | float | 기록 시각 |
| `run_id` | str | MLflow run ID |
| `customer` | str | 고객사 |
| `task` | str | 태스크 |
| `experiment_type` | str | 실험 타입 |
| `mAP50` | float | 정확도 |
| `inference_time_ms` | float | 지연시간 |
| ... | | |

**메서드**:
| 메서드 | 설명 |
|--------|------|
| `save_metrics(record)` | 메트릭 저장 |
| `query_metrics(filters)` | 필터 조회 |
| `export_to_parquet()` | Parquet 변환 |

**구현 상태**: ❌ 미구현 (Phase 4)

---

### 2.12 Dashboard API (대시보드)

**엔드포인트**:
| 경로 | 메서드 | 설명 |
|------|--------|------|
| `/api/metrics/summary` | GET | 고객별 요약 |
| `/api/metrics/trend` | GET | 시계열 트렌드 |
| `/api/metrics/comparison` | GET | 모델 비교 |
| `/api/metrics/alerts` | GET | 성능 알림 |

**구현 상태**: ❌ 미구현 (Phase 4)

---

### 2.13 AutoOptimizer (자동화)

**트리거 조건**:
| 조건 | 임계값 | 액션 |
|------|--------|------|
| 정확도 저하 | mAP50 < 0.75 | HP Sweep 재학습 |
| 지연시간 초과 | latency > 30ms | 자동 양자화 |
| GPU 활용 저조 | util < 50% | 배치 크기 증가 |

**구현 상태**: ❌ 미구현 (Phase 4)

---

## 3. 인프라 구성
```
┌─────────────────────────────────────────────────────────────┐
│                     Docker Compose                          │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────┐ │
│  │  MLflow     │  │  PostgreSQL │  │  SeaweedFS          │ │
│  │  :5000      │  │  :5432      │  │  Master :9333       │ │
│  │             │  │  (metadata) │  │  Volume :8080       │ │
│  │             │  │             │  │  Filer  :8888       │ │
│  │             │  │             │  │  S3     :8333       │ │
│  └─────────────┘  └─────────────┘  └─────────────────────┘ │
│                                                             │
│  ┌─────────────┐  ┌─────────────┐                          │
│  │  Grafana    │  │  Dashboard  │                          │
│  │  :3000      │  │  API :8000  │                          │
│  └─────────────┘  └─────────────┘                          │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## 4. CLI 명령어

| 명령어 | 설명 | Phase |
|--------|------|-------|
| `guardianflow train` | 학습 실행 | 1 |
| `guardianflow promote` | 모델 승격 | 1 |
| `guardianflow list` | 목록 조회 | 1 |
| `guardianflow infer` | 추론 벤치마크 | 2 |
| `guardianflow quant` | 양자화 | 2 |
| `guardianflow wrap` | 기존 코드 래핑 | 2 |
| `guardianflow scan` | 디렉토리 스캔 | 2 |
| `guardianflow sweep` | HP 탐색 | 3 |
| `guardianflow compare` | 모델 비교 | 3 |
| `guardianflow metrics` | 메트릭 조회 | 4 |
| `guardianflow alerts` | 알림 조회 | 4 |
| `guardianflow optimize` | 자동 고도화 | 4 |

---

## 5. Phase별 작업 범위

### Phase 1: 최소 동작 (완료)
- [x] Config
- [x] TrainingRunner (YOLO 연동)
- [x] ModelRegistry (등록/승격)
- [x] Metrics 정의
- [x] CLI (train, promote, list)

### Phase 2: 기존 코드 연동
- [ ] InferenceRunner
- [ ] QuantizationRunner
- [ ] GenericWrapper
- [ ] ExperimentRegistry
- [ ] CLI 확장 (infer, quant, wrap, scan)

### Phase 3: 시나리오/설계 고도화
- [ ] Scenario Schema 코드화
- [ ] SweepRunner
- [ ] ComparisonRunner
- [ ] Factory 패턴 적용
- [ ] CLI 확장 (sweep, compare)

### Phase 4: 대시보드/자동화
- [ ] MetricsStore
- [ ] Dashboard API
- [ ] Grafana 연동
- [ ] AutoOptimizer
- [ ] CLI 확장 (metrics, alerts, optimize)

---

## 6. 데이터 흐름 요약
```
[실험 코드] 
    │
    ▼
[Runner] ─────────────────────────────────────┐
    │                                         │
    ├─ log_params() ──────────────────────────┤
    ├─ log_metrics() ─────────────────────────┤──▶ [MLflow Server]
    ├─ log_artifact() ────────────────────────┤         │
    │                                         │         ▼
    │                                         │    [PostgreSQL] (metadata)
    │                                         │    [SeaweedFS]  (artifacts)
    │                                         │
    └─ register_model() ──────────────────────┘
                │
                ▼
         [Model Registry]
                │
                ├─ @candidate
                ├─ @champion
                └─ @rollback
                        │
                        ▼
                   [배포/서빙]
다음 단계

30 Jan
Phase 2: 기존 코드 연동 + 추론/양자화
작업 목록
ID	작업	우선순위	예상 시간	의존성
2.1	기존 실험 디렉토리 실사	🔴 높음	2h	-
2.2	InferenceRunner 구현	🔴 높음	4h	-
2.3	QuantizationRunner 구현	🔴 높음	4h	-
2.4	ExperimentRegistry 구현	🟡 중간	3h	2.1
2.5	GenericWrapper 구현	🟡 중간	3h	2.4
2.6	CLI 확장	🟢 낮음	2h	2.2-2.5
2.7	E2E 테스트	🔴 높음	2h	전체
2.1 기존 실험 디렉토리 실사
먼저 실행 필요:

bash
# 1. 디렉토리 구조 파악
ls -la ~/experiments/

# 2. 대표 디렉토리 상세 확인 (3-5개)
find ~/experiments/donghwa-entec-ppe -type f -name "*.py" | head -20
find ~/experiments/sampyo-gimhae-bct -type f -name "*.py" | head -20

# 3. 파일 패턴 분석
find ~/experiments -name "*.py" | xargs basename -a | sort | uniq -c | sort -rn | head -20

# 4. 모델 파일 위치
find ~/experiments -name "*.pt" | head -20

# 5. 데이터 설정 파일
find ~/experiments -name "*.yaml" | head -20
결과를 공유하면 실제 구조에 맞게 코드 조정

2.2 InferenceRunner 구현
python
# guardianflow/inference.py
"""추론 벤치마크 실행기"""
import time
import numpy as np
from pathlib import Path
from dataclasses import dataclass, field
from typing import Optional

import mlflow
import torch

from .config import Config
from .metrics import get_gpu_metrics, get_model_size
from .registry import ModelRegistry


@dataclass
class BenchmarkConfig:
    batch_sizes: list[int] = field(default_factory=lambda: [1, 8, 16])
    warmup_runs: int = 10
    benchmark_runs: int = 100
    conf_threshold: float = 0.25
    iou_threshold: float = 0.45
    imgsz: int = 640
    device: str = "0"


@dataclass
class BenchmarkResult:
    batch_size: int
    latency_mean_ms: float
    latency_p50_ms: float
    latency_p95_ms: float
    latency_p99_ms: float
    fps: float
    gpu_memory_mb: float


class InferenceRunner:
    """추론 벤치마크 실행기"""
    
    def __init__(
        self,
        experiment_name: str,
        model_path: str,
        data_yaml: str,
    ):
        Config.setup()
        
        self.experiment_name = experiment_name
        self.model_path = model_path
        self.data_yaml = data_yaml
        
        mlflow.set_tracking_uri(Config.MLFLOW_TRACKING_URI)
        mlflow.set_experiment(experiment_name)
    
    def run(
        self,
        config: BenchmarkConfig = None,
        run_name: str = None,
        tags: dict = None,
        source_run_id: str = None,
    ) -> dict:
        """벤치마크 실행"""
        from ultralytics import YOLO
        
        config = config or BenchmarkConfig()
        run_name = run_name or f"infer-{time.strftime('%Y%m%d-%H%M%S')}"
        
        with mlflow.start_run(run_name=run_name) as run:
            run_id = run.info.run_id
            print(f"🔍 Starting inference benchmark: {run_id}")
            
            # 태그
            mlflow.set_tag("experiment_type", "infer")
            if source_run_id:
                mlflow.set_tag("source_run_id", source_run_id)
            if tags:
                mlflow.set_tags(tags)
            
            # 파라미터
            mlflow.log_params({
                "model_path": self.model_path,
                "data": self.data_yaml,
                "batch_sizes": str(config.batch_sizes),
                "warmup_runs": config.warmup_runs,
                "benchmark_runs": config.benchmark_runs,
                "conf": config.conf_threshold,
                "iou": config.iou_threshold,
                "imgsz": config.imgsz,
            })
            
            # 모델 로드
            model = YOLO(self.model_path)
            
            # Detection 메트릭 (val)
            detection_metrics = self._run_validation(model, config)
            mlflow.log_metrics(detection_metrics)
            
            # 벤치마크 (batch별)
            benchmark_results = []
            for bs in config.batch_sizes:
                result = self._run_benchmark(model, config, bs)
                benchmark_results.append(result)
                
                mlflow.log_metrics({
                    f"latency_bs{bs}_mean_ms": result.latency_mean_ms,
                    f"latency_bs{bs}_p95_ms": result.latency_p95_ms,
                    f"fps_bs{bs}": result.fps,
                    f"gpu_memory_bs{bs}_mb": result.gpu_memory_mb,
                })
            
            # 최적 배치 크기 (FPS 기준)
            best = max(benchmark_results, key=lambda x: x.fps)
            mlflow.log_metrics({
                "best_fps": best.fps,
                "best_batch_size": best.batch_size,
                "best_latency_ms": best.latency_mean_ms,
            })
            
            # 모델 정보
            mlflow.log_metrics({
                "model_size_mb": get_model_size(self.model_path),
            })
            
            return {
                "run_id": run_id,
                "status": "COMPLETED",
                "detection": detection_metrics,
                "benchmark": [vars(r) for r in benchmark_results],
                "best": vars(best),
            }
    
    def _run_validation(self, model, config: BenchmarkConfig) -> dict:
        """검증 데이터셋으로 Detection 메트릭 계산"""
        results = model.val(
            data=self.data_yaml,
            imgsz=config.imgsz,
            conf=config.conf_threshold,
            iou=config.iou_threshold,
            device=config.device,
            verbose=False,
        )
        
        return {
            "mAP50": results.results_dict.get("metrics/mAP50(B)", 0),
            "mAP50_95": results.results_dict.get("metrics/mAP50-95(B)", 0),
            "precision": results.results_dict.get("metrics/precision(B)", 0),
            "recall": results.results_dict.get("metrics/recall(B)", 0),
        }
    
    def _run_benchmark(self, model, config: BenchmarkConfig, batch_size: int) -> BenchmarkResult:
        """특정 배치 크기로 벤치마크"""
        device = f"cuda:{config.device}" if config.device.isdigit() else config.device
        
        # 더미 입력 생성
        dummy_input = torch.zeros(batch_size, 3, config.imgsz, config.imgsz).to(device)
        
        # Warmup
        for _ in range(config.warmup_runs):
            _ = model.predict(dummy_input, verbose=False)
        
        # 벤치마크
        latencies = []
        for _ in range(config.benchmark_runs):
            torch.cuda.synchronize()
            start = time.perf_counter()
            
            _ = model.predict(dummy_input, verbose=False)
            
            torch.cuda.synchronize()
            elapsed = (time.perf_counter() - start) * 1000
            latencies.append(elapsed)
        
        arr = np.array(latencies)
        gpu_metrics = get_gpu_metrics()
        
        return BenchmarkResult(
            batch_size=batch_size,
            latency_mean_ms=float(np.mean(arr)),
            latency_p50_ms=float(np.percentile(arr, 50)),
            latency_p95_ms=float(np.percentile(arr, 95)),
            latency_p99_ms=float(np.percentile(arr, 99)),
            fps=batch_size * 1000.0 / float(np.mean(arr)),
            gpu_memory_mb=gpu_metrics.get("gpu_memory_used_mb", 0),
        )
2.3 QuantizationRunner 구현
python
# guardianflow/quantization.py
"""양자화 실행기"""
import shutil
import time
from pathlib import Path
from dataclasses import dataclass
from typing import Literal

import mlflow
from mlflow.tracking import MlflowClient

from .config import Config
from .metrics import get_model_size
from .registry import ModelRegistry
from .inference import InferenceRunner, BenchmarkConfig


@dataclass
class QuantConfig:
    backend: Literal["openvino", "tensorrt", "onnx"] = "openvino"
    precision: Literal["fp32", "fp16", "int8"] = "int8"
    imgsz: int = 640
    calibration_data: str = None  # INT8용
    calibration_size: int = 100


class QuantizationRunner:
    """양자화 실행기"""
    
    def __init__(
        self,
        experiment_name: str,
        source_run_id: str,
        data_yaml: str = None,
    ):
        Config.setup()
        
        self.experiment_name = experiment_name
        self.source_run_id = source_run_id
        self.data_yaml = data_yaml
        self.client = MlflowClient()
        self.registry = ModelRegistry()
        
        mlflow.set_tracking_uri(Config.MLFLOW_TRACKING_URI)
        mlflow.set_experiment(experiment_name)
    
    def run(
        self,
        config: QuantConfig = None,
        run_name: str = None,
        tags: dict = None,
        register: bool = True,
    ) -> dict:
        """양자화 실행"""
        from ultralytics import YOLO
        
        config = config or QuantConfig()
        run_name = run_name or f"quant-{config.backend}-{config.precision}-{time.strftime('%Y%m%d-%H%M%S')}"
        
        with mlflow.start_run(run_name=run_name) as run:
            run_id = run.info.run_id
            print(f"⚡ Starting quantization: {run_id}")
            
            # 태그
            mlflow.set_tag("experiment_type", "quant")
            mlflow.set_tag("source_run_id", self.source_run_id)
            mlflow.set_tag("quant_backend", config.backend)
            mlflow.set_tag("quant_precision", config.precision)
            if tags:
                mlflow.set_tags(tags)
            
            # 파라미터
            mlflow.log_params({
                "backend": config.backend,
                "precision": config.precision,
                "imgsz": config.imgsz,
                "calibration_size": config.calibration_size,
            })
            
            # ─────────────────────────────────────────
            # 1. 원본 모델 다운로드
            # ─────────────────────────────────────────
            original_path = self._download_source_model()
            mlflow.log_param("source_model", original_path)
            
            original_size = get_model_size(original_path)
            mlflow.log_metric("original_size_mb", original_size)
            
            # ─────────────────────────────────────────
            # 2. 원본 모델 벤치마크
            # ─────────────────────────────────────────
            model = YOLO(original_path)
            original_metrics = self._benchmark_model(model, config, "original")
            
            # ─────────────────────────────────────────
            # 3. 양자화 실행
            # ─────────────────────────────────────────
            quantized_path = self._export_model(model, config)
            
            quantized_size = get_model_size(quantized_path)
            mlflow.log_metric("quantized_size_mb", quantized_size)
            
            # ─────────────────────────────────────────
            # 4. 양자화 모델 벤치마크
            # ─────────────────────────────────────────
            quantized_model = YOLO(quantized_path)
            quantized_metrics = self._benchmark_model(quantized_model, config, "quantized")
            
            # ─────────────────────────────────────────
            # 5. 비교 메트릭
            # ─────────────────────────────────────────
            comparison = {
                "accuracy_drop_mAP50": original_metrics["mAP50"] - quantized_metrics["mAP50"],
                "speedup_ratio": original_metrics["latency_ms"] / max(quantized_metrics["latency_ms"], 0.01),
                "compression_ratio": original_size / max(quantized_size, 0.01),
            }
            mlflow.log_metrics({f"comparison.{k}": v for k, v in comparison.items()})
            
            # ─────────────────────────────────────────
            # 6. Artifact 저장
            # ─────────────────────────────────────────
            mlflow.log_artifact(quantized_path, "weights")
            
            # ─────────────────────────────────────────
            # 7. 모델 등록
            # ─────────────────────────────────────────
            model_info = None
            if register:
                model_name = self._get_model_name(config)
                model_info = self.registry.register(
                    run_id=run_id,
                    artifact_path=f"weights/{Path(quantized_path).name}",
                    model_name=model_name,
                    tags={
                        "backend": config.backend,
                        "precision": config.precision,
                        "source_run_id": self.source_run_id,
                        "mAP50": quantized_metrics["mAP50"],
                        "speedup": comparison["speedup_ratio"],
                    },
                )
                print(f"✅ Quantized model registered: {model_info}")
            
            return {
                "run_id": run_id,
                "status": "COMPLETED",
                "original": original_metrics,
                "quantized": quantized_metrics,
                "comparison": comparison,
                "model": model_info,
            }
    
    def _download_source_model(self) -> str:
        """원본 run에서 모델 다운로드"""
        local_path = mlflow.artifacts.download_artifacts(
            run_id=self.source_run_id,
            artifact_path="weights",
            dst_path=str(Config.WORK_DIR / "downloads"),
        )
        
        # best.pt 찾기
        local_dir = Path(local_path)
        for pt_file in local_dir.glob("*.pt"):
            if "best" in pt_file.name:
                return str(pt_file)
        
        # 없으면 첫 번째 pt
        pt_files = list(local_dir.glob("*.pt"))
        if pt_files:
            return str(pt_files[0])
        
        raise FileNotFoundError(f"No .pt file in {local_path}")
    
    def _export_model(self, model, config: QuantConfig) -> str:
        """모델 내보내기 (양자화)"""
        export_args = {
            "format": config.backend,
            "imgsz": config.imgsz,
            "half": config.precision == "fp16",
            "int8": config.precision == "int8",
        }
        
        # INT8은 calibration 필요
        if config.precision == "int8" and config.calibration_data:
            export_args["data"] = config.calibration_data
        elif config.precision == "int8" and self.data_yaml:
            export_args["data"] = self.data_yaml
        
        exported_path = model.export(**export_args)
        return str(exported_path)
    
    def _benchmark_model(self, model, config: QuantConfig, prefix: str) -> dict:
        """모델 벤치마크 (간소화)"""
        import torch
        import numpy as np
        
        device = "cuda:0"
        dummy = torch.zeros(1, 3, config.imgsz, config.imgsz).to(device)
        
        # Warmup
        for _ in range(10):
            model.predict(dummy, verbose=False)
        
        # Benchmark
        latencies = []
        for _ in range(50):
            torch.cuda.synchronize()
            start = time.perf_counter()
            model.predict(dummy, verbose=False)
            torch.cuda.synchronize()
            latencies.append((time.perf_counter() - start) * 1000)
        
        # Validation
        val_results = model.val(
            data=self.data_yaml,
            imgsz=config.imgsz,
            verbose=False,
        ) if self.data_yaml else None
        
        metrics = {
            "latency_ms": float(np.mean(latencies)),
            "fps": 1000.0 / float(np.mean(latencies)),
        }
        
        if val_results:
            metrics["mAP50"] = val_results.results_dict.get("metrics/mAP50(B)", 0)
            metrics["mAP50_95"] = val_results.results_dict.get("metrics/mAP50-95(B)", 0)
        else:
            metrics["mAP50"] = 0
            metrics["mAP50_95"] = 0
        
        # MLflow에 prefix와 함께 기록
        mlflow.log_metrics({f"{prefix}.{k}": v for k, v in metrics.items()})
        
        return metrics
    
    def _get_model_name(self, config: QuantConfig) -> str:
        """양자화 모델명 생성"""
        base_name = self.experiment_name.replace("/", "-")
        variant = f"{config.backend}_{config.precision}"
        return f"{base_name}__{variant}"
2.4 ExperimentRegistry 구현
python
# guardianflow/experiment_registry.py
"""기존 실험 디렉토리 레지스트리"""
import yaml
from pathlib import Path
from dataclasses import dataclass, field
from typing import Optional


@dataclass
class SiteInfo:
    customer: str
    region: str = "unknown"
    plant: str = "plant1"
    zone: str = "full"


@dataclass
class TaskInfo:
    type: str
    classes: list[str] = field(default_factory=list)


@dataclass
class EntryPoint:
    default: str
    patterns: list[str] = field(default_factory=list)
    args_template: str = ""


@dataclass
class ExperimentEntry:
    name: str
    path: Path
    site: SiteInfo
    task: TaskInfo
    entry_points: dict[str, EntryPoint] = field(default_factory=dict)
    
    def get_experiment_name(self) -> str:
        """MLflow 실험명 생성"""
        return f"{self.site.customer}/{self.site.region}-{self.site.plant}/{self.site.zone}"
    
    def get_script(self, exp_type: str) -> Optional[Path]:
        """실험 타입에 맞는 스크립트 경로"""
        if exp_type not in self.entry_points:
            return None
        
        ep = self.entry_points[exp_type]
        
        # default 우선
        if ep.default:
            script = self.path / ep.default
            if script.exists():
                return script
        
        # 패턴 매칭
        for pattern in ep.patterns:
            matches = list(self.path.glob(pattern))
            if matches:
                return matches[0]
        
        return None


class ExperimentRegistry:
    """실험 디렉토리 레지스트리"""
    
    def __init__(self, registry_path: str = None):
        self.registry_path = Path(registry_path) if registry_path else None
        self._entries: dict[str, ExperimentEntry] = {}
        
        if self.registry_path and self.registry_path.exists():
            self._load()
    
    def _load(self):
        """YAML 로드"""
        with open(self.registry_path) as f:
            data = yaml.safe_load(f)
        
        for name, entry in data.get("experiments", {}).items():
            self._entries[name] = self._parse_entry(name, entry)
    
    def _parse_entry(self, name: str, data: dict) -> ExperimentEntry:
        """딕셔너리 → ExperimentEntry 변환"""
        site = SiteInfo(**data.get("site", {"customer": name}))
        task = TaskInfo(**data.get("task", {"type": "unknown"}))
        
        entry_points = {}
        for ep_name, ep_data in data.get("entry_points", {}).items():
            if isinstance(ep_data, dict):
                entry_points[ep_name] = EntryPoint(
                    default=ep_data.get("default", ""),
                    patterns=ep_data.get("patterns", []),
                    args_template=ep_data.get("args_template", ""),
                )
            else:
                entry_points[ep_name] = EntryPoint(default=str(ep_data))
        
        return ExperimentEntry(
            name=name,
            path=Path(data["path"]).expanduser(),
            site=site,
            task=task,
            entry_points=entry_points,
        )
    
    def get(self, name: str) -> ExperimentEntry:
        """이름으로 조회"""
        if name not in self._entries:
            raise KeyError(f"Experiment not found: {name}")
        return self._entries[name]
    
    def list_all(self) -> list[str]:
        """전체 목록"""
        return list(self._entries.keys())
    
    def find_by_customer(self, customer: str) -> list[str]:
        """고객별 필터"""
        return [
            name for name, entry in self._entries.items()
            if entry.site.customer == customer
        ]
    
    def save(self, output_path: str = None):
        """YAML 저장"""
        output = Path(output_path) if output_path else self.registry_path
        
        data = {"experiments": {}}
        for name, entry in self._entries.items():
            data["experiments"][name] = {
                "path": str(entry.path),
                "site": {
                    "customer": entry.site.customer,
                    "region": entry.site.region,
                    "plant": entry.site.plant,
                    "zone": entry.site.zone,
                },
                "task": {
                    "type": entry.task.type,
                    "classes": entry.task.classes,
                },
                "entry_points": {
                    ep_name: {
                        "default": ep.default,
                        "patterns": ep.patterns,
                        "args_template": ep.args_template,
                    }
                    for ep_name, ep in entry.entry_points.items()
                },
            }
        
        with open(output, "w") as f:
            yaml.dump(data, f, allow_unicode=True, sort_keys=False, default_flow_style=False)
    
    @classmethod
    def scan_directory(cls, base_path: str) -> "ExperimentRegistry":
        """디렉토리 스캔하여 레지스트리 생성"""
        registry = cls()
        base = Path(base_path).expanduser()
        
        for exp_dir in sorted(base.iterdir()):
            if not exp_dir.is_dir():
                continue
            if exp_dir.name.startswith("."):
                continue
            
            name = exp_dir.name
            
            # 이름 파싱 (예: donghwa-entec-ppe → customer, task)
            parts = name.rsplit("-", 1)
            if len(parts) == 2:
                customer, task_type = parts[0], parts[1]
            else:
                customer, task_type = name, "unknown"
            
            # 스크립트 패턴 탐지
            entry_points = {}
            for py_file in exp_dir.glob("*.py"):
                fname = py_file.name.lower()
                if any(k in fname for k in ["train", "학습"]):
                    entry_points.setdefault("train", EntryPoint(default=py_file.name))
                elif any(k in fname for k in ["infer", "eval", "bench", "val"]):
                    entry_points.setdefault("infer", EntryPoint(default=py_file.name))
                elif any(k in fname for k in ["export", "quant", "onnx", "openvino"]):
                    entry_points.setdefault("quant", EntryPoint(default=py_file.name))
            
            registry._entries[name] = ExperimentEntry(
                name=name,
                path=exp_dir,
                site=SiteInfo(customer=customer),
                task=TaskInfo(type=task_type),
                entry_points=entry_points,
            )
        
        return registry
2.5 GenericWrapper 구현
python
# guardianflow/wrapper.py
"""기존 실험 코드 래퍼"""
import subprocess
import sys
import time
from pathlib import Path
from typing import Optional

import mlflow

from .config import Config
from .experiment_registry import ExperimentRegistry, ExperimentEntry
from .registry import ModelRegistry


class GenericWrapper:
    """기존 스크립트를 MLflow로 래핑"""
    
    def __init__(
        self,
        experiment_entry: ExperimentEntry,
        exp_type: str,
    ):
        Config.setup()
        
        self.entry = experiment_entry
        self.exp_type = exp_type
        self.registry = ModelRegistry()
        
        mlflow.set_tracking_uri(Config.MLFLOW_TRACKING_URI)
        mlflow.set_experiment(self.entry.get_experiment_name())
    
    def run(
        self,
        script_override: str = None,
        args: list[str] = None,
        params: dict = None,
        run_name: str = None,
        tags: dict = None,
        register_model: bool = True,
    ) -> dict:
        """스크립트 실행 및 MLflow 기록"""
        
        # 스크립트 결정
        if script_override:
            script_path = self.entry.path / script_override
        else:
            script_path = self.entry.get_script(self.exp_type)
        
        if not script_path or not script_path.exists():
            raise FileNotFoundError(f"Script not found for {self.exp_type}")
        
        run_name = run_name or f"wrap-{self.exp_type}-{time.strftime('%Y%m%d-%H%M%S')}"
        
        with mlflow.start_run(run_name=run_name) as run:
            run_id = run.info.run_id
            start_time = time.time()
            
            print(f"🔧 Wrapping: {script_path}")
            print(f"   Working dir: {self.entry.path}")
            
            # 태그
            mlflow.set_tag("experiment_type", self.exp_type)
            mlflow.set_tag("wrapped", "true")
            mlflow.set_tag("script", script_path.name)
            mlflow.set_tag("source_dir", str(self.entry.path))
            mlflow.set_tag("customer", self.entry.site.customer)
            mlflow.set_tag("task", self.entry.task.type)
            if tags:
                mlflow.set_tags(tags)
            
            # 파라미터
            if params:
                mlflow.log_params(params)
            
            # 실행 인자 구성
            cmd_args = self._build_args(args, params)
            cmd = [sys.executable, str(script_path)] + cmd_args
            
            mlflow.log_param("command", " ".join(cmd))
            
            # 스크립트 실행
            result = subprocess.run(
                cmd,
                cwd=self.entry.path,
                capture_output=True,
                text=True,
            )
            
            # stdout/stderr 저장
            mlflow.log_text(result.stdout, "stdout.txt")
            mlflow.log_text(result.stderr, "stderr.txt")
            mlflow.log_metric("exit_code", result.returncode)
            mlflow.log_metric("duration_sec", time.time() - start_time)
            
            if result.returncode != 0:
                mlflow.set_tag("status", "FAILED")
                print(f"❌ Script failed (exit code: {result.returncode})")
                print(result.stderr[:500])
                return {
                    "run_id": run_id,
                    "status": "FAILED",
                    "exit_code": result.returncode,
                    "error": result.stderr[:500],
                }
            
            mlflow.set_tag("status", "COMPLETED")
            
            # 결과물 수집
            artifacts = self._collect_artifacts(start_time)
            
            # 모델 등록
            model_info = None
            if register_model and self.exp_type in ["train", "quant"]:
                model_info = self._register_model_if_found(run_id, start_time)
            
            return {
                "run_id": run_id,
                "status": "COMPLETED",
                "artifacts": artifacts,
                "model": model_info,
            }
    
    def _build_args(self, args: list[str], params: dict) -> list[str]:
        """실행 인자 구성"""
        result = list(args) if args else []
        
        # entry_point에 args_template이 있으면 사용
        if self.exp_type in self.entry.entry_points and params:
            ep = self.entry.entry_points[self.exp_type]
            if ep.args_template:
                try:
                    template_args = ep.args_template.format(**params)
                    result.extend(template_args.split())
                except KeyError:
                    pass
        
        return result
    
    def _collect_artifacts(self, since_time: float) -> list[str]:
        """최근 생성된 파일 수집"""
        collected = []
        
        patterns = {
            "weights": ["**/*.pt", "**/*.pth", "**/*.onnx", "**/*.engine"],
            "results": ["**/*.csv", "**/*.json", "**/metrics*.txt"],
            "plots": ["**/*.png", "**/*.jpg"],
        }
        
        for category, globs in patterns.items():
            for pattern in globs:
                for f in self.entry.path.glob(pattern):
                    if f.is_file() and f.stat().st_mtime > since_time:
                        mlflow.log_artifact(str(f), category)
                        collected.append(f"{category}/{f.name}")
        
        return collected
    
    def _register_model_if_found(self, run_id: str, since_time: float) -> Optional[dict]:
        """모델 파일 발견 시 등록"""
        for pattern in ["**/best.pt", "**/*.pt"]:
            for f in self.entry.path.glob(pattern):
                if f.is_file() and f.stat().st_mtime > since_time:
                    model_name = f"{self.entry.site.customer}-{self.entry.site.zone}-{self.entry.task.type}"
                    
                    return self.registry.register(
                        run_id=run_id,
                        artifact_path=f"weights/{f.name}",
                        model_name=model_name,
                        tags={
                            "source": "wrapped",
                            "customer": self.entry.site.customer,
                            "task": self.entry.task.type,
                        },
                    )
        
        return None
2.6 CLI 확장
python
# guardianflow/cli.py (Phase 2 확장)

def cmd_infer(args):
    """추론 벤치마크"""
    from .inference import InferenceRunner, BenchmarkConfig
    
    config = BenchmarkConfig(
        batch_sizes=[int(x) for x in args.batch_sizes.split(",")],
        warmup_runs=args.warmup,
        benchmark_runs=args.runs,
        imgsz=args.imgsz,
        device=args.device,
    )
    
    runner = InferenceRunner(
        experiment_name=args.experiment,
        model_path=args.model,
        data_yaml=args.data,
    )
    
    result = runner.run(
        config=config,
        run_name=args.name,
        source_run_id=args.source_run,
    )
    
    print(f"\n{'='*50}")
    print(f"Run ID: {result['run_id']}")
    print(f"Best FPS: {result['best']['fps']:.1f} @ batch {result['best']['batch_size']}")
    print(f"mAP50: {result['detection'].get('mAP50', 'N/A')}")
    print(f"{'='*50}")


def cmd_quant(args):
    """양자화"""
    from .quantization import QuantizationRunner, QuantConfig
    
    config = QuantConfig(
        backend=args.backend,
        precision=args.precision,
        imgsz=args.imgsz,
    )
    
    runner = QuantizationRunner(
        experiment_name=args.experiment,
        source_run_id=args.source_run,
        data_yaml=args.data,
    )
    
    result = runner.run(
        config=config,
        run_name=args.name,
        register=not args.no_register,
    )
    
    print(f"\n{'='*50}")
    print(f"Run ID: {result['run_id']}")
    print(f"Speedup: {result['comparison']['speedup_ratio']:.2f}x")
    print(f"Compression: {result['comparison']['compression_ratio']:.2f}x")
    print(f"Accuracy drop: {result['comparison']['accuracy_drop_mAP50']:.4f}")
    print(f"{'='*50}")


def cmd_wrap(args):
    """기존 코드 래핑"""
    from .experiment_registry import ExperimentRegistry
    from .wrapper import GenericWrapper
    
    registry = ExperimentRegistry(args.registry)
    entry = registry.get(args.experiment)
    
    wrapper = GenericWrapper(entry, args.type)
    
    params = {}
    if args.param:
        for p in args.param:
            k, v = p.split("=", 1)
            params[k] = v
    
    result = wrapper.run(
        script_override=args.script,
        params=params,
        run_name=args.name,
        register_model=not args.no_register,
    )
    
    print(f"\n{'='*50}")
    print(f"Run ID: {result['run_id']}")
    print(f"Status: {result['status']}")
    print(f"Artifacts: {len(result.get('artifacts', []))} files")
    print(f"{'='*50}")


def cmd_scan(args):
    """디렉토리 스캔"""
    from .experiment_registry import ExperimentRegistry
    
    print(f"🔍 Scanning: {args.path}")
    
    registry = ExperimentRegistry.scan_directory(args.path)
    registry.save(args.output)
    
    print(f"✅ Found {len(registry.list_all())} experiments")
    print(f"   Saved to: {args.output}")
    print("\n⚠️  Please review and complete site/task info in YAML")


def main():
    parser = argparse.ArgumentParser(prog="guardianflow")
    subparsers = parser.add_subparsers(dest="command", required=True)
    
    # ... Phase 1 commands (train, promote, list) ...
    
    # ─────────────────────────────────────────
    # infer
    # ─────────────────────────────────────────
    infer_parser = subparsers.add_parser("infer", help="Run inference benchmark")
    infer_parser.add_argument("--experiment", "-e", required=True)
    infer_parser.add_argument("--model", "-m", required=True, help="Model path or registry URI")
    infer_parser.add_argument("--data", "-d", required=True)
    infer_parser.add_argument("--batch-sizes", default="1,8,16")
    infer_parser.add_argument("--warmup", type=int, default=10)
    infer_parser.add_argument("--runs", type=int, default=100)
    infer_parser.add_argument("--imgsz", type=int, default=640)
    infer_parser.add_argument("--device", default="0")
    infer_parser.add_argument("--name")
    infer_parser.add_argument("--source-run")
    infer_parser.set_defaults(func=cmd_infer)
    
    # ─────────────────────────────────────────
    # quant
    # ─────────────────────────────────────────
    quant_parser = subparsers.add_parser("quant", help="Quantize model")
    quant_parser.add_argument("--experiment", "-e", required=True)
    quant_parser.add_argument("--source-run", "-s", required=True)
    quant_parser.add_argument("--data", "-d")
    quant_parser.add_argument("--backend", choices=["openvino", "tensorrt", "onnx"], default="openvino")
    quant_parser.add_argument("--precision", choices=["fp16", "int8"], default="int8")
    quant_parser.add_argument("--imgsz", type=int, default=640)
    quant_parser.add_argument("--name")
    quant_parser.add_argument("--no-register", action="store_true")
    quant_parser.set_defaults(func=cmd_quant)
    
    # ─────────────────────────────────────────
    # wrap
    # ─────────────────────────────────────────
    wrap_parser = subparsers.add_parser("wrap", help="Wrap existing script")
    wrap_parser.add_argument("experiment", help="Experiment name in registry")
    wrap_parser.add_argument("--type", "-t", required=True, choices=["train", "infer", "quant"])
    wrap_parser.add_argument("--registry", "-r", default="registry/experiments.yaml")
    wrap_parser.add_argument("--script", "-s", help="Override script")
    wrap_parser.add_argument("--param", "-p", action="append", help="key=value")
    wrap_parser.add_argument("--name")
    wrap_parser.add_argument("--no-register", action="store_true")
    wrap_parser.set_defaults(func=cmd_wrap)
    
    # ─────────────────────────────────────────
    # scan
    # ─────────────────────────────────────────
    scan_parser = subparsers.add_parser("scan", help="Scan experiment directories")
    scan_parser.add_argument("path", help="Base experiments directory")
    scan_parser.add_argument("--output", "-o", default="registry/experiments.yaml")
    scan_parser.set_defaults(func=cmd_scan)
    
    args = parser.parse_args()
    args.func(args)
사용 예시
bash
# 1. 기존 디렉토리 스캔
guardianflow scan ~/experiments/ -o registry/experiments.yaml

# 2. 레지스트리 수동 보완 후, 래핑 실행
guardianflow wrap donghwa-entec-ppe --type train -p epochs=100 -p batch_size=16

# 3. 직접 추론 벤치마크
guardianflow infer \
    -e "donghwa-entec/welding/ppe" \
    -m runs:/abc123/weights/best.pt \
    -d /path/to/data.yaml \
    --batch-sizes 1,4,8,16

# 4. 양자화
guardianflow quant \
    -e "donghwa-entec/welding/ppe" \
    -s abc123 \
    -d /path/to/data.yaml \
    --backend openvino \
    --precision int8

# 5. 양자화 모델 승격
guardianflow promote donghwa-entec-welding-ppe__openvino_int8
다음 단계
실제 디렉토리 실사 (2.1) → 구조 확인 후 코드 조정
E2E 테스트 (2.7) → 1개 실험으로 전체 파이프라인 검증
Phase 3 → 시나리오 스키마 적용, Sweep/Comparison Runner
1

30 Jan
2.1 기존 실험 디렉토리 실사
아래 명령어들을 실행하여 결과를 공유해주세요.

필수 조사 항목
bash
# 1. 실험 디렉토리 목록 (전체)
ls -la ~/experiments/

# 2. 총 디렉토리 개수
ls -d ~/experiments/*/ 2>/dev/null | wc -l

# 3. 대표 디렉토리 3개 상세 구조
# (가장 자주 사용하는 것으로 선택)
tree ~/experiments/donghwa-entec-ppe -L 2 2>/dev/null || find ~/experiments/donghwa-entec-ppe -maxdepth 2 -type f | head -30

tree ~/experiments/sampyo-gimhae-bct -L 2 2>/dev/null || find ~/experiments/sampyo-gimhae-bct -maxdepth 2 -type f | head -30

# 4. Python 파일 패턴 분석 (전체)
find ~/experiments -name "*.py" -type f | xargs -I {} basename {} | sort | uniq -c | sort -rn | head -30

# 5. 모델 파일 위치
find ~/experiments -name "*.pt" -type f | head -30

# 6. 데이터 설정 파일
find ~/experiments -name "*.yaml" -o -name "*.yml" | head -20

# 7. 대표 학습 스크립트 내용 (1개)
# 가장 최근 사용한 학습 스크립트 경로로 변경
head -50 ~/experiments/donghwa-entec-ppe/train*.py 2>/dev/null || head -50 $(find ~/experiments -name "train*.py" | head -1)
추가 조사 (선택)
bash
# 8. 디렉토리별 파일 개수
for d in ~/experiments/*/; do echo "$(basename $d): $(find $d -type f | wc -l) files"; done

# 9. 최근 수정된 파일 (7일 이내)
find ~/experiments -type f -mtime -7 | head -20

# 10. Jupyter notebook 존재 여부
find ~/experiments -name "*.ipynb" | wc -l

# 11. requirements.txt 또는 환경 파일
find ~/experiments -name "requirements*.txt" -o -name "environment.yml" | head -10
```

---

### 결과 공유 형식
```
# 예시 응답

## 1. 디렉토리 목록
donghwa-entec-ppe/
donghwa-entec-fall/
sampyo-gimhae-bct/
sampyo-sejong-packaging/
...

## 2. 총 개수
35

## 3. 대표 구조 (donghwa-entec-ppe)
├── train_yolov11_v2.py
├── train_yolov11_v3.py
├── infer_benchmark.py
├── data/
│   └── data.yaml
├── weights/
│   ├── best.pt
│   └── last.pt
└── runs/
    └── detect/
        └── train/

## 4. Python 파일 패턴
15 train.py
12 train_yolov11.py
8  infer.py
5  export_openvino.py
...

## 7. 대표 학습 스크립트
from ultralytics import YOLO
model = YOLO('yolov11s.pt')
results = model.train(data='data.yaml', epochs=100, ...)
결과를 공유해주시면 실제 구조에 맞게 코드를 조정하겠습니다.

E2E 테스트 (2.7) → 1개 실험으로 전체 파이프라인 검증

30 Jan
2.7 E2E 테스트 계획
1개 실험으로 전체 파이프라인 검증

테스트 환경 준비
사전 요구사항 체크
bash
# 1. Python 환경 확인
python --version  # 3.10+

# 2. 필수 패키지
pip install mlflow ultralytics pynvml pyyaml

# 3. GPU 확인
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}, Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else None}')"

# 4. MLflow 버전
python -c "import mlflow; print(mlflow.__version__)"

# 5. Ultralytics 버전
python -c "import ultralytics; print(ultralytics.__version__)"
MLflow 서버 실행
bash
# 로컬 테스트용 (SQLite + 로컬 파일)
mlflow server \
    --host 0.0.0.0 \
    --port 5000 \
    --backend-store-uri sqlite:///mlflow.db \
    --default-artifact-root ./mlruns

# 확인
curl http://localhost:5000/health
테스트 데이터 준비
bash
# COCO128 (경량 테스트 데이터셋) 자동 다운로드 확인
python -c "from ultralytics import YOLO; YOLO('yolov8n.pt').val(data='coco128.yaml', imgsz=640)"
또는 기존 데이터셋 경로 확인:

bash
# 실제 데이터셋 경로 (예시)
ls -la /path/to/your/data.yaml
cat /path/to/your/data.yaml
E2E 테스트 스크립트
python
# tests/e2e_test.py
"""
E2E 테스트: train → infer → quant → promote
"""
import os
import sys
import time

# 환경 설정
os.environ["MLFLOW_TRACKING_URI"] = "http://localhost:5000"

import mlflow
from pathlib import Path


def test_environment():
    """환경 체크"""
    print("=" * 60)
    print("1. Environment Check")
    print("=" * 60)
    
    # MLflow 연결
    try:
        mlflow.set_tracking_uri(os.environ["MLFLOW_TRACKING_URI"])
        client = mlflow.tracking.MlflowClient()
        experiments = client.search_experiments()
        print(f"✅ MLflow connected: {len(experiments)} experiments")
    except Exception as e:
        print(f"❌ MLflow connection failed: {e}")
        return False
    
    # CUDA
    import torch
    if torch.cuda.is_available():
        print(f"✅ CUDA available: {torch.cuda.get_device_name(0)}")
    else:
        print("⚠️  CUDA not available, using CPU")
    
    # Ultralytics
    try:
        from ultralytics import YOLO
        print("✅ Ultralytics imported")
    except ImportError as e:
        print(f"❌ Ultralytics import failed: {e}")
        return False
    
    return True


def test_training():
    """학습 테스트 (짧은 epoch)"""
    print("\n" + "=" * 60)
    print("2. Training Test")
    print("=" * 60)
    
    from guardianflow.runner import TrainingRunner
    
    runner = TrainingRunner(
        experiment_name="e2e-test/training",
        model_arch="yolov8n.pt",  # 가장 작은 모델
        data_yaml="coco128.yaml",  # 테스트 데이터셋
    )
    
    result = runner.run(
        epochs=2,           # 최소 epoch
        batch_size=8,
        imgsz=320,          # 작은 이미지
        lr=0.01,
        device="0",
        run_name="e2e-train-test",
        tags={"test": "e2e", "phase": "training"},
        register=True,
    )
    
    print(f"\nTraining Result:")
    print(f"  Run ID: {result['run_id']}")
    print(f"  Status: {result['status']}")
    print(f"  mAP50: {result['metrics'].get('mAP50', 'N/A')}")
    
    if result['status'] != "COMPLETED":
        print(f"❌ Training failed")
        return None
    
    print("✅ Training completed")
    return result['run_id']


def test_inference(source_run_id: str = None):
    """추론 벤치마크 테스트"""
    print("\n" + "=" * 60)
    print("3. Inference Benchmark Test")
    print("=" * 60)
    
    from guardianflow.inference import InferenceRunner, BenchmarkConfig
    
    # 모델 경로 결정
    if source_run_id:
        model_path = f"runs:/{source_run_id}/weights/best.pt"
        # 실제 파일로 다운로드 필요
        local_path = mlflow.artifacts.download_artifacts(
            run_id=source_run_id,
            artifact_path="weights",
        )
        model_path = str(Path(local_path) / "best.pt")
        if not Path(model_path).exists():
            # best.pt 없으면 다른 pt 파일 찾기
            pt_files = list(Path(local_path).glob("*.pt"))
            if pt_files:
                model_path = str(pt_files[0])
    else:
        model_path = "yolov8n.pt"  # 기본 모델
    
    print(f"  Model: {model_path}")
    
    config = BenchmarkConfig(
        batch_sizes=[1, 4],  # 빠른 테스트
        warmup_runs=5,
        benchmark_runs=20,
        imgsz=320,
    )
    
    runner = InferenceRunner(
        experiment_name="e2e-test/inference",
        model_path=model_path,
        data_yaml="coco128.yaml",
    )
    
    result = runner.run(
        config=config,
        run_name="e2e-infer-test",
        source_run_id=source_run_id,
    )
    
    print(f"\nInference Result:")
    print(f"  Run ID: {result['run_id']}")
    print(f"  Best FPS: {result['best']['fps']:.1f}")
    print(f"  Latency: {result['best']['latency_mean_ms']:.2f}ms")
    
    print("✅ Inference benchmark completed")
    return result['run_id']


def test_quantization(source_run_id: str):
    """양자화 테스트"""
    print("\n" + "=" * 60)
    print("4. Quantization Test")
    print("=" * 60)
    
    if not source_run_id:
        print("⚠️  Skipping: No source run ID")
        return None
    
    from guardianflow.quantization import QuantizationRunner, QuantConfig
    
    config = QuantConfig(
        backend="onnx",     # 가장 범용적
        precision="fp16",   # INT8은 calibration 필요
        imgsz=320,
    )
    
    runner = QuantizationRunner(
        experiment_name="e2e-test/quantization",
        source_run_id=source_run_id,
        data_yaml="coco128.yaml",
    )
    
    result = runner.run(
        config=config,
        run_name="e2e-quant-test",
        register=True,
    )
    
    print(f"\nQuantization Result:")
    print(f"  Run ID: {result['run_id']}")
    print(f"  Speedup: {result['comparison']['speedup_ratio']:.2f}x")
    print(f"  Compression: {result['comparison']['compression_ratio']:.2f}x")
    
    print("✅ Quantization completed")
    return result['run_id']


def test_model_registry():
    """모델 레지스트리 테스트"""
    print("\n" + "=" * 60)
    print("5. Model Registry Test")
    print("=" * 60)
    
    from guardianflow.registry import ModelRegistry
    
    registry = ModelRegistry()
    
    # 등록된 모델 목록
    client = mlflow.tracking.MlflowClient()
    models = client.search_registered_models()
    
    print(f"  Registered models: {len(list(models))}")
    
    # e2e-test 모델 찾기
    for rm in client.search_registered_models():
        if "e2e-test" in rm.name:
            print(f"\n  Model: {rm.name}")
            
            # 버전 및 alias 확인
            for alias in ["candidate", "champion", "rollback"]:
                try:
                    mv = client.get_model_version_by_alias(rm.name, alias)
                    print(f"    @{alias}: v{mv.version}")
                except:
                    pass
    
    print("✅ Model registry check completed")


def test_promote():
    """모델 승격 테스트"""
    print("\n" + "=" * 60)
    print("6. Model Promotion Test")
    print("=" * 60)
    
    from guardianflow.registry import ModelRegistry
    
    registry = ModelRegistry()
    
    # e2e-test 모델 찾아서 승격
    client = mlflow.tracking.MlflowClient()
    
    for rm in client.search_registered_models():
        if "e2e-test-training" in rm.name:
            print(f"  Promoting: {rm.name}")
            
            result = registry.promote(rm.name)
            
            if "error" in result:
                print(f"  ⚠️  Promotion failed: {result['error']}")
            else:
                print(f"  ✅ Promoted v{result['promoted']} to champion")
            
            break
    else:
        print("  ⚠️  No e2e-test model found to promote")


def test_cli():
    """CLI 테스트"""
    print("\n" + "=" * 60)
    print("7. CLI Test")
    print("=" * 60)
    
    import subprocess
    
    commands = [
        ["python", "-m", "guardianflow.cli", "list", "experiments"],
        ["python", "-m", "guardianflow.cli", "list", "models"],
    ]
    
    for cmd in commands:
        print(f"\n  $ {' '.join(cmd)}")
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode == 0:
            print(f"  ✅ Success")
            for line in result.stdout.strip().split("\n")[:5]:
                print(f"    {line}")
        else:
            print(f"  ❌ Failed: {result.stderr[:100]}")


def run_e2e_test():
    """전체 E2E 테스트 실행"""
    print("\n" + "#" * 60)
    print("# GuardianFlow E2E Test")
    print("#" * 60)
    
    start_time = time.time()
    
    # 1. 환경 체크
    if not test_environment():
        print("\n❌ Environment check failed. Aborting.")
        return
    
    # 2. 학습
    train_run_id = test_training()
    
    # 3. 추론 벤치마크
    infer_run_id = test_inference(train_run_id)
    
    # 4. 양자화
    quant_run_id = test_quantization(train_run_id)
    
    # 5. 레지스트리 확인
    test_model_registry()
    
    # 6. 승격
    test_promote()
    
    # 7. CLI
    test_cli()
    
    # 요약
    elapsed = time.time() - start_time
    print("\n" + "=" * 60)
    print("E2E Test Summary")
    print("=" * 60)
    print(f"  Total time: {elapsed:.1f}s")
    print(f"  Training run: {train_run_id}")
    print(f"  Inference run: {infer_run_id}")
    print(f"  Quantization run: {quant_run_id}")
    print(f"\n  MLflow UI: http://localhost:5000")
    print("=" * 60)


if __name__ == "__main__":
    run_e2e_test()
실행 방법
bash
# 1. GuardianFlow 패키지 설치 (개발 모드)
cd /path/to/guardianflow
pip install -e .

# 2. MLflow 서버 실행 (별도 터미널)
mlflow server --host 0.0.0.0 --port 5000 --backend-store-uri sqlite:///mlflow.db

# 3. E2E 테스트 실행
python tests/e2e_test.py

# 4. 결과 확인
# 브라우저에서 http://localhost:5000 접속
단계별 수동 테스트 (문제 발생 시)
bash
# 1. 학습만 테스트
guardianflow train \
    -e "e2e-test/manual" \
    -d coco128.yaml \
    -m yolov8n.pt \
    --epochs 2 \
    --batch 8 \
    --imgsz 320

# 2. 추론만 테스트 (위에서 생성된 run_id 사용)
guardianflow infer \
    -e "e2e-test/manual" \
    -m yolov8n.pt \
    -d coco128.yaml \
    --batch-sizes 1,4 \
    --imgsz 320

# 3. 모델 목록 확인
guardianflow list models

# 4. 승격
guardianflow promote e2e-test-manual
```

---

## 예상 출력
```
##############################################################
# GuardianFlow E2E Test
##############################################################

============================================================
1. Environment Check
============================================================
✅ MLflow connected: 3 experiments
✅ CUDA available: NVIDIA GeForce RTX 3090
✅ Ultralytics imported

============================================================
2. Training Test
============================================================
🚀 Starting run: a1b2c3d4e5f6...
Epoch 1/2: 100%|██████████| 8/8 [00:05<00:00]
Epoch 2/2: 100%|██████████| 8/8 [00:04<00:00]

Training Result:
  Run ID: a1b2c3d4e5f6
  Status: COMPLETED
  mAP50: 0.4521
✅ Training completed

============================================================
3. Inference Benchmark Test
============================================================
  Model: /tmp/guardianflow/downloads/weights/best.pt

Inference Result:
  Run ID: b2c3d4e5f6g7
  Best FPS: 245.3
  Latency: 4.08ms
✅ Inference benchmark completed

============================================================
4. Quantization Test
============================================================

Quantization Result:
  Run ID: c3d4e5f6g7h8
  Speedup: 1.85x
  Compression: 2.10x
✅ Quantization completed

============================================================
5. Model Registry Test
============================================================
  Registered models: 2

  Model: e2e-test-training
    @candidate: v1

============================================================
6. Model Promotion Test
============================================================
  Promoting: e2e-test-training
  ✅ Promoted v1 to champion

============================================================
E2E Test Summary
============================================================
  Total time: 45.2s
  Training run: a1b2c3d4e5f6
  Inference run: b2c3d4e5f6g7
  Quantization run: c3d4e5f6g7h8

  MLflow UI: http://localhost:5000
============================================================
테스트 후 확인 사항
항목	확인 방법
Experiment 생성	MLflow UI → Experiments
Run 기록	MLflow UI → Runs (params, metrics, artifacts)
Model 등록	MLflow UI → Models
Alias 설정	Models → Versions → Aliases
Artifacts 저장	Run → Artifacts (weights/, plots/, results/)
준비되면 실행 후 결과나 에러를 공유해주세요.

Phase 3 → 시나리오 스키마 적용, Sweep/Comparison Runner

30 Jan
Phase 3: 시나리오 스키마 + Sweep/Comparison Runner
작업 목록
ID	작업	우선순위	예상 시간	의존성
3.1	Scenario Schema 구현	🔴 높음	3h	-
3.2	SweepRunner 구현	🔴 높음	4h	3.1
3.3	ComparisonRunner 구현	🟡 중간	3h	3.1
3.4	Factory 통합	🟡 중간	2h	3.1-3.3
3.5	CLI 확장	🟢 낮음	2h	3.4
3.6	YAML 기반 시나리오 로드	🟢 낮음	2h	3.1
3.7	E2E 테스트	🔴 높음	2h	전체
3.1 Scenario Schema 구현
python
# guardianflow/scenario/schema.py
"""시나리오 스키마 정의"""
from dataclasses import dataclass, field, asdict
from typing import Literal, Optional
from enum import Enum


# ─────────────────────────────────────────────
# Enums
# ─────────────────────────────────────────────

class ExperimentType(str, Enum):
    TRAIN = "train"
    INFER = "infer"
    QUANT = "quant"


class ExperimentDesign(str, Enum):
    SINGLE = "single"
    SWEEP = "sweep"
    COMPARISON = "comparison"


# ─────────────────────────────────────────────
# Config Components
# ─────────────────────────────────────────────

@dataclass
class SiteConfig:
    """현장 정보 (MLflow Experiment 매핑)"""
    customer: str
    region: str = "default"
    plant: str = "plant1"
    zone: str = "full"
    
    def to_experiment_name(self) -> str:
        return f"{self.customer}/{self.region}-{self.plant}/{self.zone}"
    
    def to_tags(self) -> dict:
        return {
            "site.customer": self.customer,
            "site.region": self.region,
            "site.plant": self.plant,
            "site.zone": self.zone,
        }


@dataclass
class TaskConfig:
    """태스크 정보"""
    type: Literal["ppe", "fall", "fire", "collision", "stuck"]
    classes: list[str] = field(default_factory=list)
    
    def to_tags(self) -> dict:
        return {
            "task.type": self.type,
            "task.classes": ",".join(self.classes) if self.classes else "",
        }


@dataclass
class ModelConfig:
    """모델 정보"""
    architecture: str = "yolov8s.pt"
    input_size: int = 640
    num_classes: int = 10
    weights_path: Optional[str] = None
    
    def to_params(self) -> dict:
        return {
            "model.architecture": self.architecture,
            "model.input_size": self.input_size,
            "model.num_classes": self.num_classes,
        }


@dataclass
class DatasetConfig:
    """데이터셋 정보"""
    path: str
    version: str = "v1"
    split: Literal["train", "val", "test"] = "train"
    
    def to_params(self) -> dict:
        return {
            "data.path": self.path,
            "data.version": self.version,
            "data.split": self.split,
        }


@dataclass
class EnvironmentConfig:
    """환경 정보"""
    device: str = "0"
    precision: Literal["fp32", "fp16", "int8"] = "fp32"
    seed: int = 42
    
    def to_params(self) -> dict:
        return {
            "env.device": self.device,
            "env.precision": self.precision,
            "env.seed": self.seed,
        }


# ─────────────────────────────────────────────
# Training Config
# ─────────────────────────────────────────────

@dataclass
class TrainingConfig:
    """학습 파라미터"""
    epochs: int = 100
    batch_size: int = 16
    learning_rate: float = 0.01
    optimizer: Literal["adam", "adamw", "sgd"] = "adamw"
    scheduler: Optional[str] = "cosine"
    early_stopping: Optional[int] = 10
    
    def to_params(self) -> dict:
        return {
            "train.epochs": self.epochs,
            "train.batch_size": self.batch_size,
            "train.learning_rate": self.learning_rate,
            "train.optimizer": self.optimizer,
            "train.scheduler": self.scheduler or "none",
        }


# ─────────────────────────────────────────────
# Sweep Config
# ─────────────────────────────────────────────

@dataclass
class SweepConfig:
    """하이퍼파라미터 스윕 설정"""
    parameters: dict[str, list] = field(default_factory=dict)
    strategy: Literal["grid", "random"] = "grid"
    max_runs: Optional[int] = None
    
    def generate_combinations(self) -> list[dict]:
        """파라미터 조합 생성"""
        from itertools import product
        
        if not self.parameters:
            return [{}]
        
        keys = list(self.parameters.keys())
        values = list(self.parameters.values())
        
        combinations = [dict(zip(keys, v)) for v in product(*values)]
        
        if self.strategy == "random" and self.max_runs:
            import random
            random.shuffle(combinations)
            return combinations[:self.max_runs]
        
        if self.max_runs:
            return combinations[:self.max_runs]
        
        return combinations
    
    @property
    def total_combinations(self) -> int:
        from functools import reduce
        if not self.parameters:
            return 1
        return reduce(lambda x, y: x * len(y), self.parameters.values(), 1)


# ─────────────────────────────────────────────
# Comparison Config
# ─────────────────────────────────────────────

@dataclass
class ComparisonConfig:
    """비교 실험 설정"""
    # 아키텍처 비교
    architectures: Optional[list[str]] = None
    
    # 기존 run 비교
    run_ids: Optional[list[str]] = None
    
    # 양자화 비교
    quantization_variants: Optional[list[dict]] = None
    
    # 데이터셋 비교 (일반화 테스트)
    datasets: Optional[list[str]] = None
    
    def get_comparison_type(self) -> str:
        if self.architectures:
            return "architecture"
        elif self.run_ids:
            return "runs"
        elif self.quantization_variants:
            return "quantization"
        elif self.datasets:
            return "dataset"
        return "unknown"


# ─────────────────────────────────────────────
# Base Scenario
# ─────────────────────────────────────────────

@dataclass
class BaseScenario:
    """시나리오 기본 클래스"""
    name: str
    version: str = "v1"
    
    site: SiteConfig = None
    task: TaskConfig = None
    model: ModelConfig = None
    dataset: DatasetConfig = None
    environment: EnvironmentConfig = None
    
    design: ExperimentDesign = ExperimentDesign.SINGLE
    
    def __post_init__(self):
        self.site = self.site or SiteConfig(customer="default")
        self.task = self.task or TaskConfig(type="ppe")
        self.model = self.model or ModelConfig()
        self.environment = self.environment or EnvironmentConfig()
    
    @property
    def experiment_type(self) -> str:
        raise NotImplementedError
    
    def get_experiment_name(self) -> str:
        return self.site.to_experiment_name()
    
    def get_run_name(self) -> str:
        return f"{self.task.type}-{self.model.architecture.replace('.pt', '')}-{self.experiment_type}-{self.version}"
    
    def get_model_name(self, variant: str = None) -> str:
        base = f"{self.site.customer}-{self.site.zone}-{self.task.type}"
        if variant:
            return f"{base}__{variant}"
        return base
    
    def to_tags(self) -> dict:
        tags = {
            "scenario.name": self.name,
            "scenario.version": self.version,
            "exp.type": self.experiment_type,
            "exp.design": self.design.value,
        }
        tags.update(self.site.to_tags())
        tags.update(self.task.to_tags())
        return tags
    
    def to_params(self) -> dict:
        params = {}
        params.update(self.model.to_params())
        params.update(self.environment.to_params())
        if self.dataset:
            params.update(self.dataset.to_params())
        return params
    
    def to_dict(self) -> dict:
        return asdict(self)


# ─────────────────────────────────────────────
# Scenario Types
# ─────────────────────────────────────────────

@dataclass
class TrainingScenario(BaseScenario):
    """학습 시나리오"""
    training: TrainingConfig = None
    sweep: Optional[SweepConfig] = None
    
    def __post_init__(self):
        super().__post_init__()
        self.training = self.training or TrainingConfig()
        if self.sweep:
            self.design = ExperimentDesign.SWEEP
    
    @property
    def experiment_type(self) -> str:
        return "train"
    
    def to_params(self) -> dict:
        params = super().to_params()
        params.update(self.training.to_params())
        return params
    
    def with_sweep(self, parameters: dict, strategy: str = "grid", max_runs: int = None) -> "TrainingScenario":
        """스윕 설정 추가"""
        self.sweep = SweepConfig(
            parameters=parameters,
            strategy=strategy,
            max_runs=max_runs,
        )
        self.design = ExperimentDesign.SWEEP
        return self


@dataclass
class InferenceScenario(BaseScenario):
    """추론 벤치마크 시나리오"""
    batch_sizes: list[int] = field(default_factory=lambda: [1, 8, 16])
    warmup_runs: int = 10
    benchmark_runs: int = 100
    
    source_run_id: Optional[str] = None
    comparison: Optional[ComparisonConfig] = None
    
    def __post_init__(self):
        super().__post_init__()
        if self.comparison:
            self.design = ExperimentDesign.COMPARISON
    
    @property
    def experiment_type(self) -> str:
        return "infer"
    
    def to_params(self) -> dict:
        params = super().to_params()
        params.update({
            "bench.batch_sizes": str(self.batch_sizes),
            "bench.warmup_runs": self.warmup_runs,
            "bench.benchmark_runs": self.benchmark_runs,
        })
        return params
    
    def with_comparison(self, **kwargs) -> "InferenceScenario":
        """비교 설정 추가"""
        self.comparison = ComparisonConfig(**kwargs)
        self.design = ExperimentDesign.COMPARISON
        return self


@dataclass
class QuantizationScenario(BaseScenario):
    """양자화 시나리오"""
    backend: Literal["openvino", "tensorrt", "onnx"] = "openvino"
    target_precision: Literal["fp16", "int8"] = "int8"
    calibration_size: int = 100
    
    source_run_id: str = None
    comparison: Optional[ComparisonConfig] = None
    
    def __post_init__(self):
        super().__post_init__()
        if self.comparison:
            self.design = ExperimentDesign.COMPARISON
    
    @property
    def experiment_type(self) -> str:
        return "quant"
    
    def get_model_name(self, variant: str = None) -> str:
        variant = variant or f"{self.backend}_{self.target_precision}"
        return super().get_model_name(variant)
    
    def to_params(self) -> dict:
        params = super().to_params()
        params.update({
            "quant.backend": self.backend,
            "quant.precision": self.target_precision,
            "quant.calibration_size": self.calibration_size,
        })
        return params
    
    def with_comparison(self, variants: list[dict]) -> "QuantizationScenario":
        """양자화 variant 비교"""
        self.comparison = ComparisonConfig(quantization_variants=variants)
        self.design = ExperimentDesign.COMPARISON
        return self
3.2 SweepRunner 구현
python
# guardianflow/scenario/sweep_runner.py
"""하이퍼파라미터 스윕 실행기"""
import copy
import time
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Callable, Optional

import mlflow

from .schema import TrainingScenario, SweepConfig
from ..config import Config


@dataclass
class SweepResult:
    """스윕 결과"""
    parent_run_id: str
    total_runs: int
    completed_runs: int
    failed_runs: int
    best_run_id: str
    best_params: dict
    best_metric: float
    all_results: list[dict]


class SweepRunner:
    """하이퍼파라미터 스윕 실행기"""
    
    def __init__(
        self,
        scenario: TrainingScenario,
        train_fn: Callable[[TrainingScenario], dict],
    ):
        """
        Args:
            scenario: 스윕 설정이 포함된 TrainingScenario
            train_fn: 실제 학습 실행 함수 (scenario -> result dict)
        """
        Config.setup()
        
        if not scenario.sweep:
            raise ValueError("Scenario must have sweep config")
        
        self.scenario = scenario
        self.train_fn = train_fn
        
        mlflow.set_tracking_uri(Config.MLFLOW_TRACKING_URI)
        mlflow.set_experiment(scenario.get_experiment_name())
    
    def run(
        self,
        metric_key: str = "mAP50",
        maximize: bool = True,
        parallel: int = 1,
        fail_fast: bool = False,
    ) -> SweepResult:
        """스윕 실행"""
        
        combinations = self.scenario.sweep.generate_combinations()
        total = len(combinations)
        
        print(f"🔄 Starting sweep: {total} combinations")
        print(f"   Strategy: {self.scenario.sweep.strategy}")
        print(f"   Parameters: {list(self.scenario.sweep.parameters.keys())}")
        
        with mlflow.start_run(run_name=f"{self.scenario.name}-sweep") as parent_run:
            parent_run_id = parent_run.info.run_id
            
            # Parent run 메타데이터
            mlflow.set_tags({
                "exp.design": "sweep",
                "sweep.strategy": self.scenario.sweep.strategy,
                "sweep.total_runs": total,
                "sweep.parameters": str(list(self.scenario.sweep.parameters.keys())),
            })
            mlflow.log_params({
                "sweep.parameter_space": str(self.scenario.sweep.parameters),
            })
            
            # 실행
            if parallel > 1:
                results = self._run_parallel(combinations, parent_run_id, parallel, fail_fast)
            else:
                results = self._run_sequential(combinations, parent_run_id, fail_fast)
            
            # 결과 집계
            completed = [r for r in results if r.get("status") == "COMPLETED"]
            failed = [r for r in results if r.get("status") == "FAILED"]
            
            # 최적 결과
            if completed:
                if maximize:
                    best = max(completed, key=lambda x: x.get(metric_key, float("-inf")))
                else:
                    best = min(completed, key=lambda x: x.get(metric_key, float("inf")))
                
                mlflow.log_metrics({
                    f"best.{metric_key}": best.get(metric_key, 0),
                })
                mlflow.set_tags({
                    "best.run_id": best["run_id"],
                    "best.params": str(best["params"]),
                })
            else:
                best = {"run_id": None, "params": {}, metric_key: 0}
            
            # 요약
            mlflow.log_metrics({
                "sweep.completed_runs": len(completed),
                "sweep.failed_runs": len(failed),
            })
            
            # 결과 테이블 저장
            self._save_results_table(results)
            
            print(f"\n✅ Sweep completed: {len(completed)}/{total} successful")
            print(f"   Best {metric_key}: {best.get(metric_key, 'N/A')}")
            print(f"   Best params: {best.get('params', {})}")
            
            return SweepResult(
                parent_run_id=parent_run_id,
                total_runs=total,
                completed_runs=len(completed),
                failed_runs=len(failed),
                best_run_id=best.get("run_id"),
                best_params=best.get("params", {}),
                best_metric=best.get(metric_key, 0),
                all_results=results,
            )
    
    def _run_sequential(
        self,
        combinations: list[dict],
        parent_run_id: str,
        fail_fast: bool,
    ) -> list[dict]:
        """순차 실행"""
        results = []
        
        for i, params in enumerate(combinations):
            print(f"\n[{i+1}/{len(combinations)}] Params: {params}")
            
            try:
                result = self._run_single(params, parent_run_id, i)
                results.append(result)
                
                if result["status"] == "FAILED" and fail_fast:
                    print("❌ Fail fast enabled, stopping sweep")
                    break
                    
            except Exception as e:
                print(f"❌ Error: {e}")
                results.append({
                    "index": i,
                    "params": params,
                    "status": "FAILED",
                    "error": str(e),
                })
                if fail_fast:
                    break
        
        return results
    
    def _run_parallel(
        self,
        combinations: list[dict],
        parent_run_id: str,
        max_workers: int,
        fail_fast: bool,
    ) -> list[dict]:
        """병렬 실행"""
        results = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(self._run_single, params, parent_run_id, i): i
                for i, params in enumerate(combinations)
            }
            
            for future in as_completed(futures):
                idx = futures[future]
                try:
                    result = future.result()
                    results.append(result)
                    print(f"[{len(results)}/{len(combinations)}] Completed: idx={idx}")
                except Exception as e:
                    results.append({
                        "index": idx,
                        "params": combinations[idx],
                        "status": "FAILED",
                        "error": str(e),
                    })
        
        return results
    
    def _run_single(self, params: dict, parent_run_id: str, index: int) -> dict:
        """단일 조합 실행"""
        # 시나리오 복사 및 파라미터 오버라이드
        child_scenario = self._apply_params(params)
        
        with mlflow.start_run(
            run_name=f"{self.scenario.name}-{index:03d}",
            nested=True,
        ) as child_run:
            mlflow.set_tags({
                "sweep.index": index,
                "sweep.parent_run_id": parent_run_id,
            })
            mlflow.log_params({f"sweep.{k}": v for k, v in params.items()})
            
            # 실제 학습 실행
            result = self.train_fn(child_scenario)
            
            return {
                "run_id": child_run.info.run_id,
                "index": index,
                "params": params,
                "status": result.get("status", "COMPLETED"),
                **result.get("metrics", {}),
            }
    
    def _apply_params(self, params: dict) -> TrainingScenario:
        """파라미터 오버라이드된 시나리오 생성"""
        scenario = copy.deepcopy(self.scenario)
        scenario.design = "single"  # child는 single
        scenario.sweep = None
        
        # 파라미터 매핑
        param_mapping = {
            "learning_rate": ("training", "learning_rate"),
            "lr": ("training", "learning_rate"),
            "batch_size": ("training", "batch_size"),
            "epochs": ("training", "epochs"),
            "optimizer": ("training", "optimizer"),
            "input_size": ("model", "input_size"),
            "imgsz": ("model", "input_size"),
            "architecture": ("model", "architecture"),
        }
        
        for key, value in params.items():
            if key in param_mapping:
                config_name, attr_name = param_mapping[key]
                config_obj = getattr(scenario, config_name)
                if hasattr(config_obj, attr_name):
                    setattr(config_obj, attr_name, value)
        
        return scenario
    
    def _save_results_table(self, results: list[dict]):
        """결과 테이블 CSV 저장"""
        import csv
        import tempfile
        
        if not results:
            return
        
        # 모든 키 수집
        all_keys = set()
        for r in results:
            all_keys.update(r.keys())
        all_keys = sorted(all_keys)
        
        with tempfile.NamedTemporaryFile(mode="w", suffix=".csv", delete=False) as f:
            writer = csv.DictWriter(f, fieldnames=all_keys)
            writer.writeheader()
            for r in results:
                writer.writerow({k: r.get(k, "") for k in all_keys})
            
            mlflow.log_artifact(f.name, "sweep_results")
3.3 ComparisonRunner 구현
python
# guardianflow/scenario/comparison_runner.py
"""비교 실험 실행기"""
import time
from dataclasses import dataclass
from typing import Callable, Optional

import mlflow
from mlflow.tracking import MlflowClient

from .schema import InferenceScenario, QuantizationScenario, ComparisonConfig
from ..config import Config


@dataclass
class ComparisonResult:
    """비교 결과"""
    parent_run_id: str
    comparison_type: str
    results: list[dict]
    best: dict
    summary: dict


class ComparisonRunner:
    """비교 실험 실행기"""
    
    def __init__(self, scenario, infer_fn: Callable = None, quant_fn: Callable = None):
        """
        Args:
            scenario: 비교 설정이 포함된 시나리오
            infer_fn: 추론 실행 함수
            quant_fn: 양자화 실행 함수
        """
        Config.setup()
        
        self.scenario = scenario
        self.infer_fn = infer_fn
        self.quant_fn = quant_fn
        self.client = MlflowClient()
        
        mlflow.set_tracking_uri(Config.MLFLOW_TRACKING_URI)
        mlflow.set_experiment(scenario.get_experiment_name())
    
    def run(self, metric_key: str = "mAP50") -> ComparisonResult:
        """비교 실행"""
        comparison = self.scenario.comparison
        if not comparison:
            raise ValueError("Scenario must have comparison config")
        
        comparison_type = comparison.get_comparison_type()
        
        print(f"📊 Starting comparison: {comparison_type}")
        
        with mlflow.start_run(run_name=f"{self.scenario.name}-comparison") as parent_run:
            parent_run_id = parent_run.info.run_id
            
            mlflow.set_tags({
                "exp.design": "comparison",
                "comparison.type": comparison_type,
            })
            
            # 타입별 실행
            if comparison_type == "architecture":
                results = self._compare_architectures(comparison.architectures, parent_run_id)
            elif comparison_type == "runs":
                results = self._compare_runs(comparison.run_ids)
            elif comparison_type == "quantization":
                results = self._compare_quantization(comparison.quantization_variants, parent_run_id)
            elif comparison_type == "dataset":
                results = self._compare_datasets(comparison.datasets, parent_run_id)
            else:
                raise ValueError(f"Unknown comparison type: {comparison_type}")
            
            # 최적 결과
            valid_results = [r for r in results if r.get(metric_key) is not None]
            if valid_results:
                best = max(valid_results, key=lambda x: x.get(metric_key, 0))
                mlflow.set_tag("comparison.best_run_id", best.get("run_id", ""))
                mlflow.log_metric(f"comparison.best_{metric_key}", best.get(metric_key, 0))
            else:
                best = {}
            
            # 요약 저장
            summary = self._generate_summary(results, metric_key)
            self._save_comparison_artifacts(results, summary)
            
            print(f"\n✅ Comparison completed: {len(results)} variants")
            
            return ComparisonResult(
                parent_run_id=parent_run_id,
                comparison_type=comparison_type,
                results=results,
                best=best,
                summary=summary,
            )
    
    def _compare_architectures(self, architectures: list[str], parent_run_id: str) -> list[dict]:
        """아키텍처 비교"""
        results = []
        
        for arch in architectures:
            print(f"\n  Testing: {arch}")
            
            with mlflow.start_run(
                run_name=f"arch-{arch.replace('.pt', '')}",
                nested=True,
            ) as run:
                mlflow.set_tags({
                    "comparison.variable": "architecture",
                    "comparison.value": arch,
                })
                
                # 시나리오 복사 및 아키텍처 변경
                import copy
                child_scenario = copy.deepcopy(self.scenario)
                child_scenario.model.architecture = arch
                child_scenario.comparison = None
                
                # 추론 실행
                if self.infer_fn:
                    result = self.infer_fn(child_scenario)
                    metrics = result.get("metrics", result.get("detection", {}))
                else:
                    metrics = self._run_inference(child_scenario)
                
                mlflow.log_metrics(metrics)
                
                results.append({
                    "run_id": run.info.run_id,
                    "architecture": arch,
                    **metrics,
                })
        
        return results
    
    def _compare_runs(self, run_ids: list[str]) -> list[dict]:
        """기존 run 비교 (실행 없이 메트릭 수집)"""
        results = []
        
        for run_id in run_ids:
            run = self.client.get_run(run_id)
            
            results.append({
                "run_id": run_id,
                "run_name": run.info.run_name,
                **run.data.metrics,
            })
        
        return results
    
    def _compare_quantization(self, variants: list[dict], parent_run_id: str) -> list[dict]:
        """양자화 variant 비교"""
        results = []
        
        # 원본 (fp32) 먼저
        print(f"\n  Testing: original (fp32)")
        with mlflow.start_run(run_name="quant-original-fp32", nested=True) as run:
            mlflow.set_tags({
                "comparison.variable": "quantization",
                "comparison.value": "original_fp32",
            })
            
            original_metrics = self._run_inference(self.scenario)
            mlflow.log_metrics(original_metrics)
            
            results.append({
                "run_id": run.info.run_id,
                "variant": "original_fp32",
                "backend": "native",
                "precision": "fp32",
                **original_metrics,
            })
        
        # 각 양자화 variant
        for variant in variants:
            variant_name = f"{variant.get('backend', 'onnx')}_{variant.get('precision', 'fp16')}"
            print(f"\n  Testing: {variant_name}")
            
            with mlflow.start_run(run_name=f"quant-{variant_name}", nested=True) as run:
                mlflow.set_tags({
                    "comparison.variable": "quantization",
                    "comparison.value": variant_name,
                })
                mlflow.log_params(variant)
                
                # 양자화 실행
                if self.quant_fn:
                    result = self.quant_fn(self.scenario, variant)
                    metrics = result.get("quantized", {})
                else:
                    metrics = {"mAP50": 0, "inference_time_ms": 0}  # placeholder
                
                mlflow.log_metrics(metrics)
                
                results.append({
                    "run_id": run.info.run_id,
                    "variant": variant_name,
                    **variant,
                    **metrics,
                })
        
        return results
    
    def _compare_datasets(self, datasets: list[str], parent_run_id: str) -> list[dict]:
        """데이터셋별 일반화 테스트"""
        results = []
        
        for dataset_path in datasets:
            dataset_name = dataset_path.split("/")[-1].replace(".yaml", "")
            print(f"\n  Testing on: {dataset_name}")
            
            with mlflow.start_run(run_name=f"dataset-{dataset_name}", nested=True) as run:
                mlflow.set_tags({
                    "comparison.variable": "dataset",
                    "comparison.value": dataset_name,
                })
                
                # 시나리오 복사 및 데이터셋 변경
                import copy
                child_scenario = copy.deepcopy(self.scenario)
                child_scenario.dataset.path = dataset_path
                child_scenario.comparison = None
                
                metrics = self._run_inference(child_scenario)
                mlflow.log_metrics(metrics)
                
                results.append({
                    "run_id": run.info.run_id,
                    "dataset": dataset_name,
                    "dataset_path": dataset_path,
                    **metrics,
                })
        
        return results
    
    def _run_inference(self, scenario) -> dict:
        """기본 추론 실행"""
        from ultralytics import YOLO
        
        model_path = scenario.model.weights_path or scenario.model.architecture
        model = YOLO(model_path)
        
        # Validation
        results = model.val(
            data=scenario.dataset.path if scenario.dataset else "coco128.yaml",
            imgsz=scenario.model.input_size,
            device=scenario.environment.device,
            verbose=False,
        )
        
        # 간단한 latency 측정
        import torch
        import numpy as np
        
        dummy = torch.zeros(1, 3, scenario.model.input_size, scenario.model.input_size)
        if torch.cuda.is_available():
            dummy = dummy.cuda()
        
        latencies = []
        for _ in range(20):
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            start = time.perf_counter()
            model.predict(dummy, verbose=False)
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            latencies.append((time.perf_counter() - start) * 1000)
        
        return {
            "mAP50": results.results_dict.get("metrics/mAP50(B)", 0),
            "mAP50_95": results.results_dict.get("metrics/mAP50-95(B)", 0),
            "precision": results.results_dict.get("metrics/precision(B)", 0),
            "recall": results.results_dict.get("metrics/recall(B)", 0),
            "inference_time_ms": float(np.mean(latencies)),
            "fps": 1000.0 / float(np.mean(latencies)),
        }
    
    def _generate_summary(self, results: list[dict], metric_key: str) -> dict:
        """비교 요약 생성"""
        import numpy as np
        
        metrics = [r.get(metric_key, 0) for r in results if r.get(metric_key) is not None]
        
        if not metrics:
            return {}
        
        return {
            "count": len(results),
            f"{metric_key}_mean": float(np.mean(metrics)),
            f"{metric_key}_std": float(np.std(metrics)),
            f"{metric_key}_min": float(np.min(metrics)),
            f"{metric_key}_max": float(np.max(metrics)),
        }
    
    def _save_comparison_artifacts(self, results: list[dict], summary: dict):
        """비교 결과 저장"""
        import csv
        import json
        import tempfile
        
        # CSV
        if results:
            all_keys = set()
            for r in results:
                all_keys.update(r.keys())
            all_keys = sorted(all_keys)
            
            with tempfile.NamedTemporaryFile(mode="w", suffix=".csv", delete=False) as f:
                writer = csv.DictWriter(f, fieldnames=all_keys)
                writer.writeheader()
                for r in results:
                    writer.writerow({k: r.get(k, "") for k in all_keys})
                mlflow.log_artifact(f.name, "comparison")
        
        # Summary JSON
        with tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False) as f:
            json.dump({"summary": summary, "results": results}, f, indent=2)
            mlflow.log_artifact(f.name, "comparison")
3.4 Factory 통합
python
# guardianflow/scenario/factory.py
"""Runner 팩토리"""
from typing import Union, Callable

from .schema import (
    BaseScenario,
    TrainingScenario,
    InferenceScenario,
    QuantizationScenario,
    ExperimentDesign,
)
from .sweep_runner import SweepRunner
from .comparison_runner import ComparisonRunner


def create_runner(
    scenario: BaseScenario,
    train_fn: Callable = None,
    infer_fn: Callable = None,
    quant_fn: Callable = None,
):
    """시나리오에 맞는 Runner 생성
    
    Args:
        scenario: 실행할 시나리오
        train_fn: 학습 함수 (scenario -> result)
        infer_fn: 추론 함수 (scenario -> result)
        quant_fn: 양자화 함수 (scenario, variant -> result)
    
    Returns:
        적절한 Runner 인스턴스
    """
    
    # 기본 함수 설정
    if train_fn is None:
        train_fn = _default_train_fn
    if infer_fn is None:
        infer_fn = _default_infer_fn
    
    # 설계 타입별 분기
    if scenario.design == ExperimentDesign.SWEEP:
        if not isinstance(scenario, TrainingScenario):
            raise ValueError("Sweep only supports TrainingScenario")
        return SweepRunner(scenario, train_fn)
    
    elif scenario.design == ExperimentDesign.COMPARISON:
        return ComparisonRunner(scenario, infer_fn, quant_fn)
    
    else:  # SINGLE
        return SingleRunner(scenario, train_fn, infer_fn, quant_fn)


class SingleRunner:
    """단일 실험 실행기"""
    
    def __init__(self, scenario, train_fn, infer_fn, quant_fn):
        self.scenario = scenario
        self.train_fn = train_fn
        self.infer_fn = infer_fn
        self.quant_fn = quant_fn
    
    def run(self, **kwargs) -> dict:
        """실험 실행"""
        exp_type = self.scenario.experiment_type
        
        if exp_type == "train":
            return self.train_fn(self.scenario)
        elif exp_type == "infer":
            return self.infer_fn(self.scenario)
        elif exp_type == "quant":
            return self.quant_fn(self.scenario, {})
        else:
            raise ValueError(f"Unknown experiment type: {exp_type}")


def _default_train_fn(scenario: TrainingScenario) -> dict:
    """기본 학습 함수"""
    from ..runner import TrainingRunner
    
    runner = TrainingRunner(
        experiment_name=scenario.get_experiment_name(),
        model_arch=scenario.model.architecture,
        data_yaml=scenario.dataset.path if scenario.dataset else None,
    )
    
    return runner.run(
        epochs=scenario.training.epochs,
        batch_size=scenario.training.batch_size,
        imgsz=scenario.model.input_size,
        lr=scenario.training.learning_rate,
        device=scenario.environment.device,
        run_name=scenario.get_run_name(),
        tags=scenario.to_tags(),
    )


def _default_infer_fn(scenario: InferenceScenario) -> dict:
    """기본 추론 함수"""
    from ..inference import InferenceRunner, BenchmarkConfig
    
    config = BenchmarkConfig(
        batch_sizes=scenario.batch_sizes,
        warmup_runs=scenario.warmup_runs,
        benchmark_runs=scenario.benchmark_runs,
        imgsz=scenario.model.input_size,
        device=scenario.environment.device,
    )
    
    runner = InferenceRunner(
        experiment_name=scenario.get_experiment_name(),
        model_path=scenario.model.weights_path or scenario.model.architecture,
        data_yaml=scenario.dataset.path if scenario.dataset else None,
    )
    
    return runner.run(
        config=config,
        run_name=scenario.get_run_name(),
        source_run_id=scenario.source_run_id,
    )
3.5 CLI 확장
python
# guardianflow/cli.py (Phase 3 추가)

def cmd_sweep(args):
    """HP 스윕 실행"""
    import json
    from .scenario.schema import TrainingScenario, SiteConfig, TaskConfig, ModelConfig, DatasetConfig, TrainingConfig
    from .scenario.factory import create_runner
    
    # 파라미터 파싱
    sweep_params = {}
    for p in args.param:
        key, values = p.split("=")
        sweep_params[key] = json.loads(values)  # "[0.001, 0.01]"
    
    scenario = TrainingScenario(
        name=args.name or "hp-sweep",
        version=args.version,
        site=SiteConfig(customer=args.customer or "default"),
        task=TaskConfig(type=args.task or "ppe"),
        model=ModelConfig(architecture=args.model, input_size=args.imgsz),
        dataset=DatasetConfig(path=args.data),
        training=TrainingConfig(epochs=args.epochs, batch_size=args.batch),
    ).with_sweep(
        parameters=sweep_params,
        strategy=args.strategy,
        max_runs=args.max_runs,
    )
    
    runner = create_runner(scenario)
    result = runner.run(
        metric_key=args.metric,
        maximize=not args.minimize,
        parallel=args.parallel,
    )
    
    print(f"\n{'='*50}")
    print(f"Sweep completed: {result.completed_runs}/{result.total_runs}")
    print(f"Best {args.metric}: {result.best_metric:.4f}")
    print(f"Best params: {result.best_params}")
    print(f"{'='*50}")


def cmd_compare(args):
    """비교 실험"""
    from .scenario.schema import InferenceScenario, SiteConfig, TaskConfig, ModelConfig, DatasetConfig
    from .scenario.factory import create_runner
    
    scenario = InferenceScenario(
        name=args.name or "comparison",
        version=args.version,
        site=SiteConfig(customer=args.customer or "default"),
        task=TaskConfig(type=args.task or "ppe"),
        model=ModelConfig(architecture=args.model, input_size=args.imgsz),
        dataset=DatasetConfig(path=args.data) if args.data else None,
    )
    
    # 비교 타입별 설정
    if args.architectures:
        scenario = scenario.with_comparison(
            architectures=args.architectures.split(",")
        )
    elif args.run_ids:
        scenario = scenario.with_comparison(
            run_ids=args.run_ids.split(",")
        )
    elif args.quant_variants:
        import json
        variants = json.loads(args.quant_variants)
        scenario = scenario.with_comparison(
            quantization_variants=variants
        )
    
    runner = create_runner(scenario)
    result = runner.run(metric_key=args.metric)
    
    print(f"\n{'='*50}")
    print(f"Comparison: {result.comparison_type}")
    print(f"Results: {len(result.results)}")
    if result.best:
        print(f"Best: {result.best.get('run_id', 'N/A')}")
    print(f"{'='*50}")


# main()에 추가
def main():
    # ... 기존 subparsers ...
    
    # ─────────────────────────────────────────
    # sweep
    # ─────────────────────────────────────────
    sweep_parser = subparsers.add_parser("sweep", help="Run HP sweep")
    sweep_parser.add_argument("--name", "-n")
    sweep_parser.add_argument("--version", "-v", default="v1")
    sweep_parser.add_argument("--data", "-d", required=True)
    sweep_parser.add_argument("--model", "-m", default="yolov8s.pt")
    sweep_parser.add_argument("--epochs", type=int, default=50)
    sweep_parser.add_argument("--batch", type=int, default=16)
    sweep_parser.add_argument("--imgsz", type=int, default=640)
    sweep_parser.add_argument("--param", "-p", action="append", required=True,
                              help='key="[val1,val2]" format')
    sweep_parser.add_argument("--strategy", choices=["grid", "random"], default="grid")
    sweep_parser.add_argument("--max-runs", type=int)
    sweep_parser.add_argument("--metric", default="mAP50")
    sweep_parser.add_argument("--minimize", action="store_true")
    sweep_parser.add_argument("--parallel", type=int, default=1)
    sweep_parser.add_argument("--customer")
    sweep_parser.add_argument("--task")
    sweep_parser.set_defaults(func=cmd_sweep)
    
    # ─────────────────────────────────────────
    # compare
    # ─────────────────────────────────────────
    compare_parser = subparsers.add_parser("compare", help="Run comparison")
    compare_parser.add_argument("--name", "-n")
    compare_parser.add_argument("--version", "-v", default="v1")
    compare_parser.add_argument("--data", "-d")
    compare_parser.add_argument("--model", "-m", default="yolov8s.pt")
    compare_parser.add_argument("--imgsz", type=int, default=640)
    compare_parser.add_argument("--metric", default="mAP50")
    compare_parser.add_argument("--customer")
    compare_parser.add_argument("--task")
    # 비교 타입 (상호 배타)
    compare_parser.add_argument("--architectures", help="arch1,arch2,arch3")
    compare_parser.add_argument("--run-ids", help="run1,run2,run3")
    compare_parser.add_argument("--quant-variants", help='JSON: [{"backend":"openvino","precision":"int8"}]')
    compare_parser.set_defaults(func=cmd_compare)
3.6 YAML 기반 시나리오 로드
python
# guardianflow/scenario/loader.py
"""YAML 시나리오 로더"""
import yaml
from pathlib import Path
from typing import Union

from .schema import (
    TrainingScenario,
    InferenceScenario,
    QuantizationScenario,
    SiteConfig,
    TaskConfig,
    ModelConfig,
    DatasetConfig,
    EnvironmentConfig,
    TrainingConfig,
    SweepConfig,
    ComparisonConfig,
)


def load_scenario(path: Union[str, Path]) -> Union[TrainingScenario, InferenceScenario, QuantizationScenario]:
    """YAML 파일에서 시나리오 로드"""
    with open(path) as f:
        data = yaml.safe_load(f)
    
    return parse_scenario(data)


def parse_scenario(data: dict):
    """딕셔너리 → 시나리오 객체 변환"""
    exp_type = data.get("experiment_type", "train")
    
    # 공통 컴포넌트
    site = SiteConfig(**data.get("site", {})) if "site" in data else None
    task = TaskConfig(**data.get("task", {})) if "task" in data else None
    model = ModelConfig(**data.get("model", {})) if "model" in data else None
    dataset = DatasetConfig(**data.get("dataset", {})) if "dataset" in data else None
    environment = EnvironmentConfig(**data.get("environment", {})) if "environment" in data else None
    
    base_kwargs = {
        "name": data.get("name", "unnamed"),
        "version": data.get("version", "v1"),
        "site": site,
        "task": task,
        "model": model,
        "dataset": dataset,
        "environment": environment,
    }
    
    if exp_type == "train":
        training = TrainingConfig(**data.get("training", {})) if "training" in data else None
        sweep = SweepConfig(**data.get("sweep", {})) if "sweep" in data else None
        
        return TrainingScenario(
            **base_kwargs,
            training=training,
            sweep=sweep,
        )
    
    elif exp_type == "infer":
        comparison = ComparisonConfig(**data.get("comparison", {})) if "comparison" in data else None
        
        return InferenceScenario(
            **base_kwargs,
            batch_sizes=data.get("batch_sizes", [1, 8, 16]),
            warmup_runs=data.get("warmup_runs", 10),
            benchmark_runs=data.get("benchmark_runs", 100),
            source_run_id=data.get("source_run_id"),
            comparison=comparison,
        )
    
    elif exp_type == "quant":
        comparison = ComparisonConfig(**data.get("comparison", {})) if "comparison" in data else None
        
        return QuantizationScenario(
            **base_kwargs,
            backend=data.get("backend", "openvino"),
            target_precision=data.get("precision", "int8"),
            calibration_size=data.get("calibration_size", 100),
            source_run_id=data.get("source_run_id"),
            comparison=comparison,
        )
    
    else:
        raise ValueError(f"Unknown experiment type: {exp_type}")


# CLI에 추가
def cmd_run_scenario(args):
    """YAML 시나리오 실행"""
    from .scenario.loader import load_scenario
    from .scenario.factory import create_runner
    
    scenario = load_scenario(args.scenario)
    runner = create_runner(scenario)
    result = runner.run()
    
    print(f"\n✅ Scenario completed")
    print(f"   Run ID: {result.get('run_id', result.get('parent_run_id', 'N/A'))}")
사용 예시
Python API
python
from guardianflow.scenario.schema import *
from guardianflow.scenario.factory import create_runner

# 1. 단일 학습
scenario = TrainingScenario(
    name="ppe-baseline",
    site=SiteConfig(customer="donghwa-entec", region="busan", zone="welding"),
    task=TaskConfig(type="ppe", classes=["helmet", "vest"]),
    model=ModelConfig(architecture="yolov8s.pt", input_size=640),
    dataset=DatasetConfig(path="/data/ppe.yaml"),
    training=TrainingConfig(epochs=100, batch_size=16),
)

runner = create_runner(scenario)
result = runner.run()

# 2. HP 스윕
scenario = TrainingScenario(
    name="ppe-hp-sweep",
    site=SiteConfig(customer="donghwa-entec"),
    model=ModelConfig(architecture="yolov8s.pt"),
    dataset=DatasetConfig(path="/data/ppe.yaml"),
    training=TrainingConfig(epochs=50),
).with_sweep(
    parameters={
        "learning_rate": [0.001, 0.005, 0.01],
        "batch_size": [8, 16, 32],
    },
    strategy="grid",
)

runner = create_runner(scenario)
result = runner.run(metric_key="mAP50", parallel=2)

# 3. 아키텍처 비교
scenario = InferenceScenario(
    name="arch-comparison",
    site=SiteConfig(customer="donghwa-entec"),
    model=ModelConfig(architecture="yolov8s.pt"),
    dataset=DatasetConfig(path="/data/ppe.yaml"),
).with_comparison(
    architectures=["yolov8n.pt", "yolov8s.pt", "yolov8m.pt"]
)

runner = create_runner(scenario)
result = runner.run()
CLI
bash
# HP 스윕
guardianflow sweep \
    -d /data/ppe.yaml \
    -m yolov8s.pt \
    --epochs 50 \
    -p 'learning_rate=[0.001,0.005,0.01]' \
    -p 'batch_size=[8,16,32]' \
    --strategy grid \
    --parallel 2

# 아키텍처 비교
guardianflow compare \
    -d /data/ppe.yaml \
    --architectures yolov8n.pt,yolov8s.pt,yolov8m.pt

# 양자화 비교
guardianflow compare \
    -d /data/ppe.yaml \
    -m yolov8s.pt \
    --quant-variants '[{"backend":"openvino","precision":"fp16"},{"backend":"openvino","precision":"int8"}]'

# YAML 시나리오 실행
guardianflow run-scenario scenarios/ppe_sweep.yaml
YAML 시나리오 예시
yaml
# scenarios/ppe_sweep.yaml
name: ppe-hp-sweep
version: v1
experiment_type: train

site:
  customer: donghwa-entec
  region: busan
  plant: plant1
  zone: welding

task:
  type: ppe
  classes: [helmet, vest, gloves]

model:
  architecture: yolov8s.pt
  input_size: 640

dataset:
  path: /data/ppe.yaml
  version: v2.1

training:
  epochs: 50
  batch_size: 16
  learning_rate: 0.01
  optimizer: adamw

sweep:
  parameters:
    learning_rate: [0.001, 0.005, 0.01]
    batch_size: [8, 16, 32]
  strategy: grid
```

---

## 디렉토리 구조 (Phase 3 완료)
```
guardianflow/
├── __init__.py
├── config.py
├── cli.py
├── runner.py              # Phase 1
├── inference.py           # Phase 2
├── quantization.py        # Phase 2
├── registry.py            # Phase 1
├── metrics.py             # Phase 1
├── wrapper.py             # Phase 2
├── experiment_registry.py # Phase 2
└── scenario/              # Phase 3
    ├── __init__.py
    ├── schema.py          # 시나리오 스키마
    ├── sweep_runner.py    # 스윕 실행기
    ├── comparison_runner.py # 비교 실행기
    ├── factory.py         # Runner 팩토리
    └── loader.py          # YAML 로더
다음 단계
Phase 3 완료 후:

Phase 4: 대시보드 + 자동화 (MetricsStore, Dashboard API, AutoOptimizer)





Claude는 AI이며 실수할 수 있습니다. 응답을 다시 한번 확인해 주세요.
